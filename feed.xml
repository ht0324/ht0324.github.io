<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-07T04:37:01+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Murmuring Woman</title><link href="https://ht0324.github.io/blog/2025/murmuring/" rel="alternate" type="text/html" title="The Murmuring Woman"/><published>2025-05-10T11:00:00+00:00</published><updated>2025-05-10T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/murmuring</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/murmuring/"><![CDATA[<p>I had a peculiar experience today that I wanted to share. I was at a cafe with my girlfriend, planning our vacation. Nearby, there was a woman, maybe in her mid-40s, wearing a face mask. She was constantly murmuring to herself – not loudly, I couldn’t make out the words, but it was non-stop. She looked really agitated, getting up, sitting down slightly differently, pacing out of the room and then returning to the same seat. This went on and on. As we wrapped up our trip planning, I looked up, and she was just gone. That quick.</p> <p>Our vacation plans came together well, but the image of that woman kind of lingered in my head. When I saw her, it really reminded me of large language models. I had to admit that my brain is so stuffed with AI these days – for better or worse.</p> <p>Hear me out.</p> <h3 id="from-self-talk-to-chain-of-thought">From Self-Talk to Chain-of-Thought</h3> <p>The woman’s constant self-talking, that murmuring, felt exactly like what chain-of-thought reasoning models are currently doing. It’s a very simple, almost too easy analogy, but if you start to put weight on it, it feels really, really profound. I don’t know why this happens, but I find that anthropomorphizing large language models sometimes helps me see what capabilities they might need or what data we should give them to make them more capable. These kinds of analogies make it easier for me to see things.</p> <p>There’s a sort of stack here, a progression:</p> <ol> <li><strong>Traditional LLMs:</strong> These models don’t really “think” in a step-by-step way. They just generate, often verbatim, without much pause – a kind of knee-jerk reaction. This is like System 1 thinking.</li> <li><strong>Reasoning Models (Chain-of-Thought):</strong> When these came along, they blew the older models out of the water. This introduced a new scaling paradigm: test-time compute. Introspection, or thinking step-by-step, is much better than a knee-jerk response for many tasks. This is System 2, and it’s really good for improving capabilities. <a href="https://www.youtube.com/watch?v=eaAonE58sLU">Noam Brown’s work</a> really pioneered this area.</li> </ol> <h3 id="the-limits-of-introspection-and-the-need-for-tools">The Limits of Introspection and the Need for Tools</h3> <p>Now, what current models are doing is moving towards becoming agents. And here’s where the analogy with the woman (and I want to be clear, I don’t know her or what she was going through, but she really looked like she was having a tough time – this is just an observation for the sake of analogy) becomes even more interesting.</p> <p>Constant introspection, just talking to oneself, only gets you so far. And that’s exactly the limit I see with first-generation reasoning models, like some of the DeepSeek models or OpenAI’s o1. They can think, they can “talk to themselves” on and on, but they can’t verify their own thoughts quite reliably.</p> <p>Compare this to how people generally operate. When “normal” people think, they can self-verify using external tools or interactions. They might talk something through with someone else for verification, or rely on external aids like their iPhone, a book, or a quick search. This analogy is such a simple thing, right? And that’s exactly what models like Anthropic’s Claude 3.7 Sonnet and OpenAI’s o3 are doing now. They excel at interacting with the real world via an external pipeline, a bridge we call “tools.”</p> <h3 id="the-fine-line-of-anthropomorphism">The Fine Line of Anthropomorphism</h3> <p>When you anthropomorphize a large language model this way, the need for tools and external interaction becomes very obvious. But there’s a very key caveat: the internal modeling of an LLM is very different from human cognition. I’m anthropomorphizing for the sake of seeing what LLMs might <em>benefit from</em>, what might make them more capable. It’s a very fine line to walk.</p> <h3 id="arent-we-all-just-next-word-predictors">Aren’t We All Just Next-Word Predictors?</h3> <p>As this kind of anthropomorphization continues – and it’s very easy to do because language models are so persuasive and lifelike – it reminded me of something <a href="https://youtube.com/watch?v=XgCHZ1G93iA&amp;t=438">Scott Aaronson said</a> a year ago. When LLMs first emerged and there were naysayers arguing “it’s just next-word prediction, just statistical modeling,” he’d retort (and I’m paraphrasing) “But what about you? Aren’t <em>you</em> just a next-word predictor? What about your mom?”</p> <p>It really kind of cracked me up at the time. If I’d said that to some of my close friends who looked down upon LLMs, they would have fumed, they’d be outraged! But when ChatGPT came out, I intuitively, wholeheartedly agreed with Aaronson’s point. My mind hasn’t changed on that.</p> <p>I think as models get more capable, Aaronson’s quip – “aren’t we just the next-word predictor?” – will basically become true in a functional sense. Recently, LLMs just passed the turing test, but society has moved on like nothing ever happened. Sooner or later, for every verifiable task, model capabilities will likely exceed human capabilities. And still, when that happens, they will <em>still</em> be, at their core, next-word predictors. Superhuman next-word predictors, better than us at any given task.</p> <p>Then what would we become?</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[A Parable for LLM Thinking]]></summary></entry><entry><title type="html">Dropout - Review</title><link href="https://ht0324.github.io/blog/2025/dropout/" rel="alternate" type="text/html" title="Dropout - Review"/><published>2025-04-28T10:00:00+00:00</published><updated>2025-04-28T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/dropout</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/dropout/"><![CDATA[<p>Dropout is one of those techniques in deep learning that feels ubiquitous, yet revisiting the original 2014 paper, “<a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>” by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov, reveals layers of insight beyond just “randomly turn off neurons.” It’s a paper grounded in clever intuition and addresses a fundamental challenge in training powerful models.</p> <h3 id="the-evolutionary-analogy-mix-ability-over-brittle-co-adaptation">The Evolutionary Analogy: Mix-ability over Brittle Co-adaptation</h3> <p>One of the most striking parts of the paper is the motivation drawn from evolutionary biology, specifically the role of sexual reproduction.</p> <blockquote> <p>“One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes.”</p> </blockquote> <p>The paper contrasts this with asexual reproduction, where a well-adapted set of genes might be perfectly optimized for a specific environment but could be brittle if conditions change. Sexual reproduction, by constantly shuffling genes, forces individual genes to be effective in collaboration with a <em>random</em> set of other genes. This “mix-ability” fosters robustness.</p> <p>This analogy maps beautifully onto neural networks. A standard network might develop complex “co-adaptations” between hidden units, perfectly fitting the training data but failing on unseen examples. Dropout, by randomly removing units during training, acts like gene shuffling. It forces each unit to be useful on its own or in conjunction with various randomly chosen subsets of other units. This prevents the network from relying on fragile partnerships that only exist in the training data, promoting robustness. As the paper humorously adds, ten small conspiracies might be more robust than one large one requiring everyone to play their part perfectly.</p> <h3 id="the-real-goal-generalizing-to-the-test-set">The Real Goal: Generalizing to the Test Set</h3> <p>This ties into a crucial point, echoing sentiments sometimes expressed by researchers like Ilya Sutskever: the ultimate objective isn’t just fitting the training data, but generalizing to the <strong>test set</strong>. The paper highlights this early on:</p> <blockquote> <p>“With limited training data, however, many of these complicated relationships will be the result of sampling noise, so they will exist in the training set but not in real test data even if it is drawn from the same distribution. This leads to overfitting…”</p> </blockquote> <p>Dropout directly attacks this problem. Overfitting often involves learning spurious correlations – patterns that exist purely by chance in the training sample. Standard networks, especially high-capacity ones, have the “luxury” of using their parameters to memorize this noise to minimize training loss.</p> <h3 id="dropouts-incentive-learning-robust-features-not-noise">Dropout’s Incentive: Learning Robust Features, Not Noise</h3> <p>Dropout changes the incentive structure during training. By constantly disrupting pathways (randomly dropping units), it makes it significantly harder for the network to rely on specific, complex interactions between neurons that might only capture spurious correlations. The “reward” (gradient signal) for learning these fragile patterns becomes inconsistent.</p> <p>Conversely, strong, prominent features that reflect the true underlying data structure are likely detectable through multiple, more robust pathways or redundant representations. These features “survive” the dropout process more reliably, receiving more consistent positive reinforcement. Dropout, therefore, incentivizes the network to invest its capacity in learning features that are resilient to this random disruption – precisely the features most likely to generalize.</p> <h3 id="the-mechanism-approximating-an-exponential-ensemble">The Mechanism: Approximating an Exponential Ensemble</h3> <p>The core mechanism is elegant:</p> <ol> <li><strong>During Training:</strong> For each training case (or minibatch), randomly “thin” the network by dropping units (setting their output to zero) with a certain probability <code class="language-plaintext highlighter-rouge">1-p</code>. This means training an exponentially large ensemble of networks (potentially 2^N for N units) that all share weights.</li> <li><strong>At Test Time:</strong> Explicitly averaging the predictions of all possible thinned networks is intractable. Instead, use the single, full network but scale down the outgoing weights of units by the retention probability <code class="language-plaintext highlighter-rouge">p</code>. This simple scaling provides a good approximation of the average prediction of the ensemble.</li> </ol> <p>This allows training what is effectively a huge ensemble but performing inference efficiently with a single model.</p> <h3 id="synergy-with-max-norm-regularization">Synergy with Max-Norm Regularization</h3> <p>The paper notes that dropout often works best with high learning rates and momentum. However, this can risk weights growing uncontrollably. They found a specific technique particularly helpful: <strong>Max-Norm Regularization</strong>.</p> <blockquote> <p>“…constraining the norm of the incoming weight vector at each hidden unit to be upper bounded by a fixed constant c. In other words, if w represents the vector of weights incident on any hidden unit, the neural network was optimized under the constraint IIwII₂ ≤ c.”</p> </blockquote> <p>This acts as a crucial stabilizer. By capping the magnitude (L2 norm) of incoming weights to each neuron, it prevents weights from exploding, allowing the use of aggressive learning rates needed to overcome the noise introduced by dropout, without sacrificing stability.</p> <h3 id="sparsity-as-a-side-effect">Sparsity as a Side-Effect</h3> <p>Interestingly, the paper demonstrates (Figures 7 &amp; 8) that dropout often leads to sparser activations in hidden units, even without explicit sparsity-inducing penalties. Neurons learn to be more selective, potentially making the learned representations more interpretable or efficient.</p> <h3 id="final-thoughts-foundational-simplicity">Final Thoughts: Foundational Simplicity</h3> <p>Dropout is a testament to the power of simple, well-motivated ideas. It provides a computationally feasible way to prevent overfitting by mimicking evolutionary pressure towards robustness and discouraging the memorization of spurious, training-set-specific correlations. While not a silver bullet (factors like training time and interaction with Batch Normalization mean it’s not <em>always</em> the best choice), its impact on deep learning has been immense. Reading the original paper clarifies <em>why</em> it works so well – it’s not just random noise; it’s structured noise that fundamentally changes the network’s learning incentives towards generalization.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Revisiting the foundational 2014 paper on Dropout]]></summary></entry><entry><title type="html">Rethinking Sequence-to-Sequence - Review</title><link href="https://ht0324.github.io/blog/2025/attention/" rel="alternate" type="text/html" title="Rethinking Sequence-to-Sequence - Review"/><published>2025-04-26T09:00:00+00:00</published><updated>2025-04-26T09:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/attention</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/attention/"><![CDATA[<p>Reading foundational papers often provides a clearer perspective on how current ideas evolved. Recently, I went through the 2015 ICLR paper “<a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>” by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Knowing the authors, especially Cho and Bengio, signals its importance. It tackles a core problem in early sequence-to-sequence models for machine translation.</p> <p>The main issue they identified was the “bottleneck” inherent in the standard RNN Encoder-Decoder framework popular at the time (like in Cho et al., 2014a or Sutskever et al., 2014). These models tried to compress the entire meaning of a source sentence, regardless of its length, into a single fixed-length vector. As the paper noted, this makes it difficult to handle long sentences well – performance tended to drop off significantly as sentences got longer.</p> <p>Their proposed solution was to allow the decoder to look back at the source sentence and selectively focus on relevant parts when generating each target word. This avoids forcing all information through one fixed vector.</p> <h3 id="key-concepts">Key Concepts</h3> <p>Here’s a breakdown of the core ideas discussed:</p> <ul> <li><strong>The Problem: Fixed-Length Vector Bottleneck:</strong> Standard encoder-decoders map an input sequence <code class="language-plaintext highlighter-rouge">x = (x_1, ..., x_{T_x})</code> to a fixed context vector <code class="language-plaintext highlighter-rouge">c</code>. The decoder then generates the output <code class="language-plaintext highlighter-rouge">y = (y_1, ..., y_{T_y})</code> based solely on <code class="language-plaintext highlighter-rouge">c</code> and previously generated words. This compression limits the model’s capacity, especially for long inputs.</li> <li><strong>The Solution: Alignment Mechanism (Decoder Focus):</strong> Instead of one <code class="language-plaintext highlighter-rouge">c</code>, the proposed model computes a <em>distinct</em> context vector <code class="language-plaintext highlighter-rouge">c_i</code> for each target word <code class="language-plaintext highlighter-rouge">y_i</code>. This <code class="language-plaintext highlighter-rouge">c_i</code> is a weighted sum of <em>annotations</em> <code class="language-plaintext highlighter-rouge">(h_1, ..., h_{T_x})</code> from the encoder. Each <code class="language-plaintext highlighter-rouge">h_j</code> corresponds to a source word <code class="language-plaintext highlighter-rouge">x_j</code> (or rather, the hidden state around it).</li> <li><strong>How it Works: Alignment Model &amp; Context Vector:</strong> <ul> <li>The weight <code class="language-plaintext highlighter-rouge">a_{ij}</code> for each annotation <code class="language-plaintext highlighter-rouge">h_j</code> when generating <code class="language-plaintext highlighter-rouge">y_i</code> depends on how well the input around position <code class="language-plaintext highlighter-rouge">j</code> aligns with the output at position <code class="language-plaintext highlighter-rouge">i</code>.</li> <li>These weights are calculated using an “alignment model” <code class="language-plaintext highlighter-rouge">a</code>, which takes the previous decoder hidden state <code class="language-plaintext highlighter-rouge">s_{i-1}</code> and the encoder annotation <code class="language-plaintext highlighter-rouge">h_j</code> as input to produce a score <code class="language-plaintext highlighter-rouge">e_{ij}</code>.</li> <li><code class="language-plaintext highlighter-rouge">e_{ij} = a(s_{i-1}, h_j)</code></li> <li>The weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> are obtained by normalizing these scores with a softmax: <code class="language-plaintext highlighter-rouge">a_{ij} = exp(e_{ij}) / Σ_k exp(e_{ik})</code>.</li> <li>The context vector <code class="language-plaintext highlighter-rouge">c_i</code> is then the weighted sum: <code class="language-plaintext highlighter-rouge">c_i = Σ_j a_{ij} h_j</code>.</li> <li>Crucially, the alignment model <code class="language-plaintext highlighter-rouge">a</code> (parameterized as a small feedforward network) is trained <em>jointly</em> with the rest of the system.</li> </ul> </li> <li><strong>Soft vs. Hard Alignment:</strong> The paper uses the term “soft alignment.” This contrasts with “hard alignment,” which would involve making a deterministic choice of which single source word aligns with the target word. Soft alignment uses a weighted average over <em>all</em> source annotations. This makes the mechanism differentiable and allows the model to learn alignments implicitly through backpropagation. It also naturally handles situations where a target word might depend on multiple source words, or vice-versa.</li> <li><strong>The Encoder: Bidirectional RNN (BiRNN):</strong> To ensure the annotation <code class="language-plaintext highlighter-rouge">h_j</code> captures context from both before and after the source word <code class="language-plaintext highlighter-rouge">x_j</code>, they used a BiRNN. This consists of a forward RNN processing the sequence from <code class="language-plaintext highlighter-rouge">x_1</code> to <code class="language-plaintext highlighter-rouge">x_{T_x}</code> and a backward RNN processing it from <code class="language-plaintext highlighter-rouge">x_{T_x}</code> to <code class="language-plaintext highlighter-rouge">x_1</code>. The annotation <code class="language-plaintext highlighter-rouge">h_j</code> is the concatenation of the forward hidden state <code class="language-plaintext highlighter-rouge">\vec{h}_j</code> and the backward hidden state <code class="language-plaintext highlighter-rouge">\cev{h}_j</code>. While BiRNNs weren’t new, their use here makes sense for creating richer annotations.</li> </ul> <h3 id="key-takeaways">Key Takeaways</h3> <p>Reflecting on the paper, several points stand out:</p> <ul> <li><strong>Performance Improvement (Especially on Long Sentences):</strong> The results (Figure 2, Table 1) clearly show the benefit. The standard RNNencdec model’s performance drops sharply with sentence length, while the proposed RNNsearch model remains much more robust. The BLEU scores confirm a significant improvement, bringing NMT closer to traditional phrase-based systems of the time.</li> <li><strong>Interpretability via Alignment:</strong> The alignment weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> can be visualized (Figure 3). This provides insight into what parts of the source sentence the model focuses on when generating a specific target word. The visualizations showed mostly monotonic alignments (as expected between English and French) but also the ability to handle local reordering (like adjective-noun flips) correctly. This interpretability is a nice side effect compared to trying to understand a monolithic RNN.</li> <li><strong>Handling Reordering and Length Differences:</strong> The soft alignment naturally deals with source and target phrases having different lengths or requiring non-trivial mappings, without needing explicit mechanisms like NULL tokens used in traditional SMT.</li> <li><strong>Evolutionary Link to Transformers:</strong> Reading this <em>after</em> knowing about Transformers makes the connection incredibly clear. The core mechanism – scoring source annotations based on the current decoder state, using softmax for weights, and computing a weighted sum – is essentially the attention mechanism. It feels like a direct precursor; the Transformer built upon this by removing recurrence and adding multi-head attention, positional encodings, etc. It’s like seeing an earlier stage in the “evolution” of sequence models.</li> </ul> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This paper feels like a pivotal step in NMT. It directly addressed a clear limitation (the fixed-length vector bottleneck) with an elegant solution: allowing the model to learn where to focus in the source sequence. The “soft alignment” mechanism introduced is, in essence, the attention mechanism that became central to later architectures like the Transformer.</p> <p>Looking back now, the ideas seem quite intuitive, but implementing this effectively and showing its benefits in 2014/2015 was a significant contribution. It’s a well-written paper that clearly explains the problem, the proposed solution, and provides compelling evidence. Reading it helps appreciate the progression of ideas leading to the models we use today.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Looking back at the 2015 paper that introduced an attention-like mechanism to NMT.]]></summary></entry><entry><title type="html">Knowledge Distillation - Review</title><link href="https://ht0324.github.io/blog/2025/distillation/" rel="alternate" type="text/html" title="Knowledge Distillation - Review"/><published>2025-04-22T14:00:00+00:00</published><updated>2025-04-22T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/distillation</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/distillation/"><![CDATA[<p>I’ve known about the concept of knowledge distillation for a while – the core idea is simple yet profound: soft labels (the full probability distribution from a model) contain much richer information about class relationships than hard labels alone. I first encountered it in a lecture by Geoffrey Hinton (<a href="https://www.youtube.com/watch?v=rGgGOccMEiY">like this one discussing paths to intelligence</a>) and decided to read the original 2015 paper, “<a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a>,” co-authored with Oriol Vinyals and Jeff Dean. It’s quite short, but packed with insight.</p> <h3 id="the-insect-analogy-training-vs-deployment">The Insect Analogy: Training vs. Deployment</h3> <p>What really struck me immediately was the opening analogy:</p> <blockquote> <p>“Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction.”</p> </blockquote> <p>I haven’t seen many ML papers start with such a strong, refreshing biological analogy, and it’s spot on! I hadn’t really thought about insect life stages this way before. The larva is all about consumption and growth – slow-moving, maybe not complex, but incredibly efficient at extracting resources (like a large training model absorbing information from data). The adult form is optimized for different tasks – lightweight, fast, mobile, focused on specific functions like reproduction (like an efficient deployment model needing low latency and computational cost).</p> <p>The analogy fits perfectly with the challenge in machine learning:</p> <ul> <li><strong>Training:</strong> We often use huge, “cumbersome” models (or ensembles) that take lots of computation and time but are great at extracting every bit of signal from large datasets.</li> <li><strong>Deployment:</strong> We need models that are fast, efficient, and have low latency for real-world use.</li> </ul> <p>Distillation, then, is like the <strong>metamorphosis</strong> – transforming the knowledge captured by the cumbersome larva/training model into the efficient adult/deployment model.</p> <h3 id="knowledge-beyond-weights">Knowledge Beyond Weights</h3> <p>The paper rightly points out a potential “conceptual block”:</p> <blockquote> <p>“…we tend to identify the knowledge in a trained model with the learned parameter values.”</p> </blockquote> <p>This makes it hard to think about transferring knowledge without just copying weights. Prior work like Rich Caruana’s model compression focused on matching the outputs <em>before</em> the final softmax (the logits). Hinton et al.’s approach refines this by using the <em>probabilities</em> from the softmax, arguing that this captures the learned distribution more meaningfully.</p> <h3 id="the-value-of-wrong-answers">The Value of “Wrong” Answers</h3> <p>A key insight is how the large, cumbersome model generalizes. It’s not just about getting the right answer.</p> <blockquote> <p>“…a side-effect of the learning is that the trained model assigns probabilities to all of the incorrect answers… The relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize.”</p> </blockquote> <p>The example they give is perfect: an image of a BMW might have a tiny probability of being mistaken for a garbage truck, but that probability, however small, is likely <em>much</em> higher than it being mistaken for a carrot. This network of similarities and differences between classes is crucial knowledge learned by the teacher model. Hard labels (just “BMW”) throw this information away. Soft labels (the full probability distribution) preserve it.</p> <p>This aligns with the real objective: we don’t just want models to perform well on training data, we want them to <em>generalize</em> well to new data. Soft targets directly transfer the <em>generalization behavior</em> of the teacher model to the student.</p> <h3 id="the-mechanism-temperature-scaling">The Mechanism: Temperature Scaling</h3> <p>So how do we effectively use these soft labels? If the teacher model is very confident (assigns probability ~1.0 to the correct class), the probabilities for incorrect classes are tiny. Even if their <em>ratios</em> contain information, they have almost no impact on the cross-entropy loss during student training.</p> <p>The solution is to “raise the temperature” <code class="language-plaintext highlighter-rouge">T</code> of the softmax function:</p> <p><code class="language-plaintext highlighter-rouge">q_i = exp(z_i / T) / Σ_j exp(z_j / T)</code></p> <p>where <code class="language-plaintext highlighter-rouge">z_i</code> are the logits. Normally <code class="language-plaintext highlighter-rouge">T=1</code>. Using a higher <code class="language-plaintext highlighter-rouge">T &gt; 1</code> “softens” the probability distribution, increasing the probabilities of incorrect classes and allowing them to contribute more to the loss function. The student model is trained to match this softened distribution, using the same high temperature <code class="language-plaintext highlighter-rouge">T</code>. (After training, the student uses <code class="language-plaintext highlighter-rouge">T=1</code> for inference).</p> <p>This temperature scaling is the core mechanism. The paper notes that in the high-temperature limit, this method becomes equivalent to matching the logits (Caruana’s approach), but at intermediate temperatures, it focuses more on matching the more probable incorrect classes, potentially ignoring noise from very negative logits.</p> <h3 id="training-the-student">Training the Student</h3> <p>The best results often come from combining two objectives:</p> <ol> <li>Matching the soft targets from the teacher (using cross-entropy with high temperature <code class="language-plaintext highlighter-rouge">T</code>).</li> <li>Matching the true hard labels (using cross-entropy with <code class="language-plaintext highlighter-rouge">T=1</code>).</li> </ol> <p>They found a weighted average works well, often with a lower weight on the hard target loss. As they say: “Typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful.”</p> <h3 id="proof-of-generalization-the-mnist-experiment">Proof of Generalization: The MNIST Experiment</h3> <p>A compelling experiment highlights the power of this approach. They trained a student model on MNIST, but <em>omitted all examples of the digit ‘3’</em> from the transfer set. From the student’s perspective, ‘3’ was a “mythical digit” it had never directly seen.</p> <p>Despite this, the distilled model performed surprisingly well on classifying ‘3’s at test time (with a bias adjustment). It had learned about ‘3’ indirectly, through the soft targets for other digits – for example, by learning which ‘8’s looked a bit like a ‘3’ according to the teacher model. This is powerful evidence that soft targets transfer genuine generalization capabilities, not just labels.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>This paper is a classic example of elegance and insight. The core claim is simple but powerful:</p> <blockquote> <p>“…a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.”</p> </blockquote> <p>Knowledge distillation provides a practical way to harness this information, bridging the gap between powerful-but-cumbersome training models and efficient deployment models. While short, the paper’s impact is huge, reflected in its numerous citations. It’s a testament to clear thinking and finding simple solutions to important problems – a touch of genius.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the elegant 2015 paper by Hinton, Vinyals, and Dean on knowledge distillation.]]></summary></entry><entry><title type="html">Neural Probabilistic Language Model - Review</title><link href="https://ht0324.github.io/blog/2025/neural-language/" rel="alternate" type="text/html" title="Neural Probabilistic Language Model - Review"/><published>2025-04-20T15:00:00+00:00</published><updated>2025-04-20T15:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/neural-language</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/neural-language/"><![CDATA[<p>I recently dove into Yoshua Bengio et al.’s 2003 paper, “<a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>”. Reading such an old paper – really foundational work from over two decades ago – is fascinating. What struck me most wasn’t just the specific model (which is simple by today’s standards), but the clarity with which Bengio laid out the core problems and principles of language modeling, principles that are still incredibly relevant. I got a real respect for his vision; it feels like this paper set the trajectory for much of what followed.</p> <h3 id="the-problem-the-curse-of-dimensionality">The Problem: The Curse of Dimensionality</h3> <p>Bengio starts by framing the fundamental challenge: the <strong>curse of dimensionality</strong>. As he puts it,</p> <blockquote> <p>“…a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.”</p> </blockquote> <p>This is because the number of possible sentences is combinatorially vast, essentially infinite – like <a href="https://medium.com/@FdForThought/a-short-story-in-hell-24b02ff4d812">the Library of Babel</a>. Any specific sentence has almost zero probability of occurring randomly.</p> <p>The “curse” goes deeper than just the sheer number of sequences. As the number of dimensions (e.g., the length of the sequence, or the number of features considered) increases:</p> <ol> <li><strong>Space Expands Exponentially:</strong> The volume of the space grows incredibly fast, making the available data extremely sparse.</li> <li><strong>Distance Intuition Breaks:</strong> In high dimensions, points tend to become equidistant from each other, and most of the volume is concentrated far from the center, near the “surface” of the high-dimensional space. Our low-dimensional intuitions about proximity and density fail.</li> <li><strong>Spurious Correlations:</strong> With so many dimensions, it becomes easy to find apparent patterns in data that are just noise.</li> </ol> <p>This is a core challenge for many real-world problems, especially with rich sensory data spanning many dimensions. How do you find the signal in such a vast, sparse space without getting lost?</p> <h3 id="the-solution-fighting-fire-with-fire">The Solution: Fighting Fire with Fire</h3> <p>Bengio and his colleagues proposed a way to fight this curse:</p> <blockquote> <p>“…learning a distributed representation for words…”</p> </blockquote> <p>Essentially, they proposed learning dense, low-dimensional feature vectors (embeddings) for each word in the vocabulary. This feels like fighting fire with fire: while the <em>vocabulary</em> space is huge and discrete, the learned <em>feature</em> space is much smaller (e.g., 30-100 dimensions in their experiments vs. 17k+ words) but continuous. Because it’s a dense, continuous space, even a relatively low-dimensional one has a huge capacity to represent complex relationships. They are mapping the discrete, high-dimensional vocabulary into a structured, continuous latent space.</p> <h3 id="the-magic-how-generalization-happens">The Magic: How Generalization Happens</h3> <p>So how does this help? The paper explains:</p> <blockquote> <p>“Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.”</p> </blockquote> <p>This, for me, is the crux of it. The model learns which words play similar roles (semantically, syntactically) and places them close together in the embedding space. Because the probability function operates smoothly over this continuous space, seeing “The cat sat on the mat” helps the model assign a higher probability to the <em>unseen</em> sentence “A dog rested on the rug,” because the corresponding words have similar learned representations. It’s this mapping from discrete symbols to a meaningful continuous space that allows generalization beyond simply memorizing n-grams. This is fundamentally how current LLMs achieve their (still limited, but powerful) generalization capabilities.</p> <h3 id="learning-end-to-end">Learning End-to-End</h3> <p>A key part of their proposal was point 3:</p> <blockquote> <p>“learn simultaneously the word feature vectors and the parameters of that probability function.”</p> </blockquote> <p>They recognized that the embeddings and the prediction mechanism need to learn <em>from each other</em>. You can’t just fix one and train the other; they have to be optimized together, end-to-end, for the embeddings to become useful for the prediction task and vice-versa.</p> <h3 id="a-historical-aside-parallel-processing-with-cpus">A Historical Aside: Parallel Processing with CPUs</h3> <p>What also caught my eye was the extensive discussion on parallelizing the training process. Remember, this was 2003 – widespread GPU computing for ML wasn’t a thing yet. They detail their efforts using <strong>parameter-parallel processing</strong> across multiple CPUs (up to 64 Athlon processors in their cluster!). They discuss asynchronous updates and communication overhead (MPI). It feels like they were laying the conceptual groundwork for the kind of massive parallelization (now mostly on GPUs/TPUs) that is essential for training today’s large models.</p> <h3 id="lasting-impact">Lasting Impact</h3> <p>While the specific MLP architecture used in the paper is rudimentary now, the core ideas – tackling the curse of dimensionality with learned distributed representations, enabling generalization through semantic similarity in embedding space, and the need for end-to-end training – remain absolutely central to modern NLP and deep learning. Reading this paper felt like seeing a lighthouse beam cutting through the dark, clearly illuminating the path forward for the entire field. It truly helped define the paradigm we’re still working within.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Thoughts on the foundational 2003 paper]]></summary></entry><entry><title type="html">Revisiting the 2014 Sequence-to-Sequence Paper</title><link href="https://ht0324.github.io/blog/2025/lstm/" rel="alternate" type="text/html" title="Revisiting the 2014 Sequence-to-Sequence Paper"/><published>2025-04-20T11:00:00+00:00</published><updated>2025-04-20T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/lstm</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/lstm/"><![CDATA[<p>I recently went back to read the 2014 paper “<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>” by Sutskever, Vinyals, and Le. Since it’s such a seminal paper – practically ancient by today’s ML standards – I thought it would be interesting to look back. While the method itself (LSTM encoder-decoder) is relatively simple compared to modern architectures, focusing on their thinking process back when deep learning was still in its infancy was insightful. I found some things they mentioned almost casually that felt non-trivial to me now.</p> <p>Here are some of my main takeaways:</p> <h3 id="refreshing-views-on-dnn-power">Refreshing Views on DNN Power</h3> <p>The paper starts by framing Deep Neural Networks (DNNs) in ways I hadn’t explicitly considered before. They state:</p> <blockquote> <p>“DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps.”</p> </blockquote> <p>This sentence, while true and maybe obvious in retrospect, struck me. Of course, we know matrix multiplications are parallelizable and run well on GPUs, but thinking about it from the perspective of <em>individual neurons</em> performing computations in parallel felt like a refreshing angle on <em>why</em> NNs are inherently suited for this hardware.</p> <p>They also highlight:</p> <blockquote> <p>“…their ability to sort N N-bit numbers using only 2 hidden layers of quadratic size…”</p> </blockquote> <p>Again, a specific example of computational power packed into a relatively simple network that I hadn’t really internalized.</p> <h3 id="the-core-problem-and-the-lstm-solution">The Core Problem and the LSTM Solution</h3> <p>The authors clearly state the limitation they were tackling:</p> <blockquote> <p>“Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.”</p> </blockquote> <p>This was a major hurdle for many important tasks like machine translation or question answering, where sequence lengths vary. This is where the Long Short-Term Memory (LSTM) network comes in.</p> <p>For me, the biggest contribution of this paper is that they took the LSTM and <em>successfully trained an encoder-decoder architecture at scale</em> on a difficult task (English-to-French translation). They showed that LSTMs weren’t just a theoretical curiosity; they were <em>practical</em> for real-world, large-scale NLP problems. This was really the first paper that demonstrated LSTMs could <em>work</em> in this way, paving the path for much future research.</p> <h3 id="the-input-reversal-trick">The Input Reversal Trick</h3> <p>One specific technical detail that stood out was their trick of reversing the input sentence:</p> <blockquote> <p>“…reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.”</p> </blockquote> <p>My first reaction was: Okay, reversing the input brings the <em>first</em> source word closer to the <em>first</em> target word, which makes sense for translation since the beginning often sets the context. But what about the <em>last</em> words of the source sentence? Don’t they get pushed really far away from the end of the target sentence?</p> <p>That’s a valid point, but I think it highlights a clever trade-off. Getting the beginning of the translation right is often very important; it lays the groundwork. By reversing the input, they made it much easier for the optimization process (SGD) to “establish communication” between the early parts of the source and target sequences. The performance gains they reported (perplexity dropping from 5.8 to 4.7, BLEU jumping from 25.9 to 30.6) suggest this was a very effective trade-off, making the model learn much better, even if it seems counter-intuitive for the tail end of the sequences.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>Reading this paper was a great reminder of how far the field has come, but also of the clear thinking that set the foundations. I really admire Ilya Sutskever’s way of thinking – when I listen to him on podcasts, he often speaks with such clarity and wisdom. Looking at this early work reinforces that impression. Maybe I should make a point of reading through more of his papers.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Personal takeaways from the seminal 2014 paper]]></summary></entry><entry><title type="html">Agency Over Retrieval?</title><link href="https://ht0324.github.io/blog/2025/o3-o4mini2/" rel="alternate" type="text/html" title="Agency Over Retrieval?"/><published>2025-04-18T10:00:00+00:00</published><updated>2025-04-18T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/o3-o4mini2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/o3-o4mini2/"><![CDATA[<p>I’ve been thinking more about OpenAI’s o3 and o4 mini models since my <a href="https://ht0324.github.io/blog/2025/o3-o4mini-product-path/">last post</a>, and using them led to some additional insights. Specifically, I looked into the <a href="https://x.com/elder_plinius/status/1912567149991776417">system prompt for o3/o4 mini</a> and noticed something interesting: it strongly encourages the model to surf the web whenever a query is even slightly vague or uncertain. Essentially, web search is almost the default behavior.</p> <p>Initially, I was puzzled by this choice. Why prompt a <em>reasoning and agentic</em> model to search the web so readily? These aren’t primarily web search models like Perplexity AI. Why not rely more on their internal knowledge and reasoning first?</p> <p>But as I thought deeper about it, everything kind of clicked.</p> <h3 id="beyond-simple-hallucination-mitigation">Beyond Simple Hallucination Mitigation</h3> <p>Of course, one part of the answer relates to model hallucination. Getting large language models to be consistently reliable and avoid making things up is still a really hard problem. While improvements in data and algorithms have reduced blatant hallucination, ensuring reliability over 99% of the time requires robust grounding. Accessing relevant, up-to-date information is obviously important for that.</p> <p>However, I don’t think that’s the full story here. I think OpenAI is doing something more fundamental by leveraging the <em>agentic</em> capabilities of these models.</p> <h3 id="letting-the-model-choose-its-context">Letting the Model Choose Its Context</h3> <p>Think about traditional approaches like Retrieval-Augmented Generation (RAG). In those systems, a separate retrieval mechanism analyzes the user’s query, finds potentially relevant source documents, and then stuffs that information into the language model’s context window. The retrieval system decides what the main LLM sees.</p> <p>What OpenAI seems to be doing with o3/o4 mini is different. They are offloading the task of finding relevant information <em>to the main model itself</em>. Instead of an external system <em>pushing</em> context, the agentic model is encouraged to <em>pull</em> the context it decides it needs by actively searching the web.</p> <p>Let me break it down with an analogy. Imagine you need to solve a complex problem.</p> <ul> <li><strong>Option A (RAG-like):</strong> Someone else (a separate system) looks at your problem, finds some books or articles they think are relevant, and hands them to you. You then try to solve the problem using only those materials.</li> <li><strong>Option B (o3/o4 mini-like):</strong> You look at the problem, and <em>you</em> decide to proactively search your bookshelf, scour the internet, gather information, and based on what you find, iteratively search for more information until <em>you</em> feel you have what you need.</li> </ul> <p>Option B gives you autonomy. You actively choose what information you consume. This feels like a much more effective way to tackle complex or nuanced problems, right?</p> <p>The key difference is agency. The RAG system (Option A) isn’t necessarily as smart or capable as the main LLM it’s feeding context to. Why let a potentially less sophisticated system pre-filter the information? Why not let the powerful base model decide what information is most relevant or needed for its own reasoning process?</p> <p>This principle of giving the model agency to select its own relevant context seems to be more general than just text retrieval via web search. It applies across modalities. Look at OpenAI’s recent post on <a href="https://openai.com/index/thinking-with-images/">“Thinking with Images”</a>. They demonstrate how o3/o4 mini can use tools to manipulate images <em>during</em> their chain of thought. For instance, if text in an image is upside down or hard to read, the model can use tools to zoom in or rotate that specific part of the image to better understand it. If an image is complex, it can zoom into the most relevant section. This is effectively visual information retrieval, driven by the model’s <em>active ability</em> to choose which visual information to focus on using tools, mirroring how it uses web search to retrieve textual information.</p> <p>Traditional RAG can sometimes feel like it just dumps context verbatim into the window, regardless of whether the model actually deems it insightful or sufficient. Giving the model the agency to search – whether the web for text or pixels within an image – means it can dynamically gather precisely what it needs, when it needs it. It’s a recursive, almost meta-cognitive approach – the model decides how to inform itself.</p> <h3 id="consolidation-and-the-bitter-lesson-again">Consolidation and the Bitter Lesson Again?</h3> <p>This feels like another step in the consolidation trend we’ve seen in AI. Previously, NLP was fragmented into many specific tasks (sentiment analysis, named entity recognition, etc.). With the rise of powerful transformers, many of these specialized tasks converged into the capabilities of large, general models.</p> <p>Now, agency might be enabling further consolidation. Tasks like information retrieval (textual or visual) and hallucination mitigation, previously handled by separate scaffolding or techniques like RAG, might increasingly become integrated into the model’s core agentic reasoning loop. As models become more general and capable agents, they can take on more of these sub-tasks themselves.</p> <p>In a way, it feels like the <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">Bitter Lesson</a> playing out once more. Instead of relying heavily on human-designed scaffolding and rule-based systems (like fixed retrieval strategies), perhaps it’s more effective to give the scaled-up model the agency and tools (like web search or image manipulation) and let <em>it</em> learn the best methods for gathering and utilizing information to solve the task at hand. Don’t inhibit the model with rigid external structures; let its own capabilities grow.</p> <p>It’s a simple shift – prompting the model to search when unsure, or allowing it to manipulate input images – but the underlying principle of empowering the model’s own agency to manage its information needs feels like a profound one.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Thinking about why o3/o4 mini are prompted to search the web by default.]]></summary></entry><entry><title type="html">Thoughts on o3, o4 mini</title><link href="https://ht0324.github.io/blog/2025/o3-o4mini/" rel="alternate" type="text/html" title="Thoughts on o3, o4 mini"/><published>2025-04-17T17:00:00+00:00</published><updated>2025-04-17T17:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/o3-o4mini</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/o3-o4mini/"><![CDATA[<p>So, here are my recent thoughts on the release of OpenAI’s <a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3 and o4 mini</a>. In conclusion, it’s a bit of a mixed bag. It’s easy to get caught up in the hype, and there are definitely notable things present. While generally, I think the release and the resulting models are fantastic, there are subtle nuances that need to be addressed.</p> <p>For a quick recap for those who haven’t caught up: OpenAI released <strong>o3</strong> and <strong>o4 mini</strong>, new variants of their reasoning models specifically trained to use tools and be agentic. When you see the demos and people’s use cases, it really is fantastic. It has more of a “person feel.” I’ve used it myself, and compared to previous models that primarily did research by fetching and analyzing web info, <strong>o3</strong> and <strong>o4 mini</strong> feel much more agentic. Unlike previous models which ca use function calling but rather in a separate isolated manner, they seem to actively parse information, act based on it, and use various tools such as command‑line interface. In that sense, they are really capable models.</p> <h3 id="context-and-comparison">Context and Comparison</h3> <p>But for me, after the initial impression settled, it felt like OpenAI basically released their version of Anthropic’s <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which is really good at agentic tasks. Because of its agentic capabilities, 3.7 Sonnet became a go‑to enterprise solution, especially for agent‑coding IDEs. While OpenAI’s new models arrived two or three months later and are arguably better, the fundamental paradigm hasn’t drastically changed.</p> <p>Another thing I really noted was within the ChatGPT interface itself. When serving <strong>o3</strong> and <strong>o4 mini</strong>, OpenAI enabled function calling and tool use <em>by default</em>. This means the models can readily use capabilities like data analysis, the coding environment, web search, and others that OpenAI has integrated into ChatGPT. All this combined gives the models a vast array of tools they didn’t have access to so easily before, and it does have a combinatorial effect on model capabilities.</p> <p>Previously, if I wanted to research a topic while studying, I’d usually paste my content into the chat and query the model. Now, since the model can search the web itself – and it feels less like simple RAG and more like it’s actually fitting the fetched info into its context – I don’t have to provide as much relevant context manually. It feels much more reliable in that sense, reducing the need for me to spoon‑feed context. I think the capability of the chat interface itself has drastically changed.</p> <h3 id="openai-as-a-product-company">OpenAI as a Product Company</h3> <p>This led me to another thought: the overall trajectory of OpenAI as a company.</p> <p>When I saw how OpenAI neatly packaged their models and tools onto a platter and served it within the chat interface, it really clicked for me: as Sam Altman had <a href="https://www.youtube.com/live/5MWT_doo68k?t=653">said</a>, OpenAI is now officially a product company—though in hindsight, given where their revenue comes from, they always were.</p> <p><a href="https://medium.com/@furqankhaan/how-openai-and-anthropic-are-cashing-in-on-ai-a-look-at-their-revenue-models-d9d9ae79dd28">If you compare their revenues to Anthropic’s</a>, OpenAI is the market leader, partly due to the network effect of being the first mover. But most of OpenAI’s revenue comes from user subscriptions with a smaller fraction from API usage. Anthropic is the polar opposite—most revenue comes from API usage, though even their API revenue lags behind OpenAI’s API revenue. This suggests subscriptions are vastly more popular than API access, at least for OpenAI.</p> <p>Then it makes sense for OpenAI to prioritize products because intelligence is becoming cheap, almost too cheap to meter. Model API costs are racing to the bottom, leaving very few margins, especially with competitors like Google, Anthropic, xAI, and others. Subscription is a godsend for cheap cash.</p> <p>So, what do I mean by OpenAI acting as a product company? As I mentioned, <strong>o3/o4 mini</strong> feel like a better version of <strong>Claude 3.7 Sonnet</strong>. There’s a qualitative jump, I’m not denying that, but there’s also a masterful way OpenAI executed the <em>delivery</em>.</p> <p>You see, when <strong>Claude 3.7 Sonnet</strong> launched, Anthropic just launched an enterprise solution, not a product. And make no mistake, <strong>Claude 3.7 Sonnet</strong> is a <em>really</em> capable, mind-blowingly agentic model, written more about <a href="https://ht0324.github.io/blog/2025/Claude-Code/">here</a>. They also generously released the Model Context Protocol <a href="https://www.anthropic.com/news/model-context-protocol">(MCP)</a>, which I’ve also <a href="https://ht0324.github.io/blog/2025/vibe-coding/">written about and presented on</a>. However, when it came to their consumer-facing product, Anthropic essentially just slapped this incredibly smart model into a basic chat interface <em>without</em> providing the tools necessary to showcase its agentic power. The end user interacting with Claude AI wouldn’t even realize how smart and agentic the underlying model truly is.</p> <p>Looking back, especially after seeing OpenAI’s <strong>o3/o4 mini</strong> launch, I can see how Anthropic could have stolen OpenAI’s thunder. If they had built the necessary scaffolding and provided adequate tools for Claude to use directly within the chat interface – allowing it to surf the web, execute code all agentically – the user experience would have been dramatically different. Claude <em>can</em> do these things via MCP, but the defalut interface doesn’t allow it. This forces users like me to manually scaffold MCP and handcraft custom environments just to tap into the model’s full potential, which is far from an ideal experience. With <strong>o3</strong>, OpenAI made it frictionless; it just <em>works</em>.</p> <h3 id="mcp-and-financial-realities">MCP and Financial Realities</h3> <p>In that sense, I was really surprised when OpenAI <a href="https://x.com/sama/status/1904957253456941061">announced</a> they would also support MCP on their models. Initially, I was skeptical they’d adopt it as a standard. First, Anthropic developed it. Second, it seemed counter to OpenAI’s strategy. As you can see, they were prepping <strong>o3/o4 mini</strong> as a <em>product</em>. They serve it via API too, but that doesn’t feel like their main priority. Their strategy seems to be building their own scaffolding and tool integration seamlessly into the model and selling it as a product. MCP, being an open standard, directly counteracts that by leveraging the open‑source community.</p> <p>However, I think this kind of standardization is inevitable, so OpenAI likely just followed suit. For MCP proliferation, the open‑weights/source community and Anthropic seem like the main benefactors, not OpenAI. But personally, I think this outcome—broader adoption of open standards—is desirable, even if it wasn’t OpenAI’s first preference.</p> <p>I believe that to achieve financial independence, OpenAI will aggressively work towards building itself as a product company. I understand the criticisms about OpenAI deviating from its non‑profit roots. But as I’ve <a href="https://ht0324.github.io/blog/2025/OpenAI-for-profit/">covered before</a>, the reality seems simple: they need money. They need financial independence to do what they set out to do. Because of scaling laws and everything else, capital is absolutely necessary to scale up compute and continue research. I think they feel they have no choice but to pursue this path. I know Sam Altman can sound manipulative, but I think it might be true in a sense that they didn’t fully anticipate this financial necessity early on, and now they feel forced into this position.</p> <h3 id="the-walled-garden-strategy">The Walled Garden Strategy</h3> <p>Given this path towards being a product company, OpenAI’s recent moves start to make a lot more sense. Take their <a href="https://x.com/OpenAI/status/1910378768172212636">enhanced memory feature</a>, for example. This isn’t just about convenience; it’s a clear play for user retention, a step towards building a walled garden. They want users deeply integrated into <em>their</em> ecosystem.</p> <p>Looking ahead, imagine if OpenAI develops a truly frontier, genius-level model. What if they <em>only</em> offer it through their ChatGPT interface, not the API? Since subscriptions are the real cash cow compared to the low-margin API race, this seems entirely plausible, especially if AGI-level capabilities emerge. A <a href="https://darioamodei.com/machines-of-loving-grace">country of geniuses in a datacenter</a>, only accessible via chatgpt.com. They could make a shit ton of money this way.</p> <p>Combine that potential model superiority with features like memory that build up personalized context over time, and the friction for users to switch to a competitor becomes immense. Your interaction history, your personalized AI – it all stays within OpenAI’s walls. Essentially, the memory capability becomes a strategic tool. It makes the platform stickier and much harder to leave. In a way, OpenAI is making our accumulated data a reason to stay, almost holding it hostage to deter us from jumping ship to competitors. Food for thought as these platforms evolve.</p> <h3 id="conclusion">Conclusion</h3> <p>So, in conclusion, yes, <strong>o3</strong> and <strong>o4 mini</strong> are qualitatively good models. But I think these kinds of performance improvements were somewhat expected given the trajectory of previous models and the industry. What I think is non‑trivial, and what not enough people seem to be paying attention to, is <em>how</em> OpenAI implemented this. The seamless product integration, the default tool usage in the chat interface, and what this signals about their shift towards being a product‑first company—that’s the bigger story here.</p> <p>Moreover, although OpenAI’s revenue mix already revealed its product focus, this release cements it. ChatGPT’s first‑mover advantage has generated a powerful network effect: the more users it attracts, the more feedback and data it gathers, which funds better models, which in turn attract even more users. It’s the classic aggregator flywheel on steroids. Even though Google currently dazzles with pure‑model wins like Gemini 2.5 Pro, and Anthropic keeps pushing Claude 3.7 Sonnet, neither has managed to match the friction‑free, fully‑integrated product OpenAI now offers. Unless OpenAI makes a catastrophic misstep, I believe that flywheel will only accelerate.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Reflections on the new o3 and o4 mini models]]></summary></entry><entry><title type="html">DeepSeek GRM - Review</title><link href="https://ht0324.github.io/blog/2025/grm/" rel="alternate" type="text/html" title="DeepSeek GRM - Review"/><published>2025-04-12T21:00:00+00:00</published><updated>2025-04-12T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/grm</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/grm/"><![CDATA[<p>This time, I’m looking at the paper <a href="https://arxiv.org/abs/2504.02495">“Inference-Time Scaling for Generalist Reward Modeling”</a> by Liu et al. from DeepSeek-AI. Given DeepSeek’s recent work, I went into this with some anticipation, though my initial feeling was that it might be less of a fundamentally new algorithm and more a detailed concretization of concepts like RLAIF and <a href="https://arxiv.org/abs/2212.08073">Constitutional AI (CAI)</a>. Still, the paper frames the problem very effectively and the results look promising.</p> <p>The core challenge addressed is obtaining accurate reward signals for LLMs in general domains, where tasks aren’t easily verifiable like math problems. While methods like RLAIF help scale beyond human feedback, the paper points out issues with existing reward models (RMs): inflexibility to input types, accuracy limitations, and poor scaling with inference-time compute. This work proposes a way to improve generalist RMs, specifically focusing on making them better when given more thinking time (inference compute).</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Pointwise Generative Reward Modeling (GRM)</strong><br/> The authors adopt a GRM approach. Instead of just outputting a scalar score or a pairwise preference, the RM generates textual output (critiques based on principles) and assigns individual (pointwise) scores to each response (e.g., 1-10). This architecture provides flexibility in handling single, paired, or multiple responses using the same format.</p> <p><strong>Self-Principled Critique Tuning (SPCT)</strong><br/> This is the proposed training methodology. Unlike CAI which uses a fixed, human-defined constitution, SPCT aims to train the GRM to <em>dynamically generate</em> relevant principles and critiques based on the specific input query and responses.</p> <p><strong>SPCT Stage 1: Rejective Fine-Tuning (RFT)</strong><br/> A “cold start” phase to get the GRM generating principles and critiques in the correct format. It uses existing RM datasets and samples trajectories (principle + critique + score). Trajectories are rejected if the predicted reward is incorrect (doesn’t match ground truth preference) or if the task is “too easy” (all sampled trajectories for a given input are correct).</p> <p><strong>SPCT Stage 2: Rule-Based Reinforcement Learning</strong><br/> An online RL phase (using a GRPO setup) to further improve the GRM. The key here is that the RL optimizes the <em>reward model itself</em>. The reward signal for this RL process is based on simple accuracy rules – does the GRM’s generated critique and score correctly identify the best response according to the ground truth preference label from the dataset? A KL penalty is used to maintain stability.</p> <p><strong>Inference-Time Scaling via Sampling &amp; Voting</strong><br/> To leverage more compute at inference, the paper proposes sampling <code class="language-plaintext highlighter-rouge">k</code> times from the trained GRM for the same input. Each sample yields a potentially different set of principles, critiques, and scores. The final score for a response is obtained by summing the scores across all <code class="language-plaintext highlighter-rouge">k</code> samples (“Voting”). This expands the effective reward range, allowing for finer granularity.</p> <p><strong>Meta Reward Model (Meta RM)</strong><br/> As an enhancement to voting, a separate, smaller RM is trained to evaluate the quality of the principles and critiques generated by the main GRM in each of the <code class="language-plaintext highlighter-rouge">k</code> samples. During inference, this Meta RM filters the <code class="language-plaintext highlighter-rouge">k</code> samples down to the top <code class="language-plaintext highlighter-rouge">k_meta</code> based on critique quality, and voting is performed only on these higher-quality samples.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Problem Framing: Generalist Rewards &amp; Scaling</strong><br/> The paper does a good job setting up the motivation. Getting good rewards for complex, open-ended tasks is hard. While CAI introduced principles, this work focuses on making the principle application dynamic and scaling the RM’s quality with compute. The four challenges identified (flexibility, accuracy, inference scaling, learning scalable behaviors) provide a clear target.</p> <p><strong>Dynamic Principles vs. Fixed Constitution</strong><br/> The shift from CAI’s static constitution to SPCT’s dynamically generated principles felt like a notable difference. The idea is that the RM learns to adapt its evaluation criteria (the principles) to the specific context, rather than relying on a predefined, potentially inflexible set. This seems like a natural evolution.</p> <p><strong>Improving the Reward Model, Not Just the Policy</strong><br/> A point that required careful distinction during discussion was the target of the RL. While methods like GRPO use RL to improve the <em>policy</em> LLM based on a reward signal, the rule-based RL in SPCT is used to improve the <em>reward model itself</em> – making it better at generating principles and critiques that align with ground truth preferences. It’s a bit “meta” – training the judge to be a better judge.</p> <p><strong>The “Rejecting Too Easy” Strategy</strong><br/> The RFT stage’s approach of discarding trajectories where the GRM was correct every time was interesting. There are a few angles: Is it just removing uninformative data where the model already performs perfectly? Or is it actively forcing the model to focus on harder examples where it might struggle, thereby promoting more robust learning? Or perhaps it simplifies downstream ranking if all examples aren’t trivially correct? It seems like a pragmatic way to focus the training signal.</p> <p><strong>How Inference Scaling Works (Summing, Not Averaging)</strong><br/> The voting mechanism (Eq. 6) initially seemed odd – why sum scores instead of averaging? But, the thing to remember is that the goal is <em>ranking</em> responses. Summing preserves the relative ranking just as averaging would, but directly reflects the expanded granularity achieved by sampling multiple “perspectives” (principles). Since the absolute value isn’t used directly in a loss function later (unlike scalar value learning), maintaining a fixed 1-10 scale via averaging isn’t strictly necessary.</p> <p><strong>Performance: Scaling Matters</strong><br/> The results (Tables 2, 3, 6, Figure 4) show that the SPCT-trained DeepSeek-GRM performs well, beating baselines and competing with strong models. More importantly, the inference-time scaling demonstrably works. Using more samples (<code class="language-plaintext highlighter-rouge">k=32</code>) or the Meta RM (<code class="language-plaintext highlighter-rouge">k=8</code> or <code class="language-plaintext highlighter-rouge">k=16</code>) allows the 27B GRM to match or exceed the performance of much larger models (like a 671B RFT model) that use less inference compute. This supports the core premise that investing compute at inference time can be highly effective if the model is trained appropriately (via SPCT).</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This paper presents Self-Principled Critique Tuning (SPCT) as a method to train Pointwise Generative Reward Models (GRMs) that generate dynamic principles and critiques, enabling effective inference-time scaling. By training the RM itself using a combination of rejective fine-tuning and rule-based online RL, the authors create a system (DeepSeek-GRM) that improves its reward signal quality when given more compute via sampling.</p> <p>While building on ideas from RLAIF and CAI, the dynamic principle generation and the explicit focus on training the RM for inference-time scalability (including the Meta RM) feel like distinct contributions. The empirical results strongly suggest that scaling inference compute via sampling, especially when guided by a meta-judge, can be a very effective way to boost reward quality, potentially rivaling the gains from simply scaling model size during training. It makes me curious about how these improved reward models will be used to train DeepSeek’s next generation of policy models.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the DeepSeek paper on Inference-Time Scaling for Generalist Reward Modeling]]></summary></entry><entry><title type="html">System 1, System 2</title><link href="https://ht0324.github.io/blog/2025/sys1/" rel="alternate" type="text/html" title="System 1, System 2"/><published>2025-04-12T21:00:00+00:00</published><updated>2025-04-12T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/sys1</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/sys1/"><![CDATA[<p>Our thinking operates on two levels. System 1 acts quickly, intuitively – efficient, yet prone to error. System 2 engages slowly, deliberately – requiring effort for greater accuracy. Daniel Kahneman detailed this duality in <em>Thinking, Fast and Slow</em>, describing these complementary modes of mind.</p> <p><a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi">T-AGI</a>, Artificial General Intelligence measured against time. A system reaches T-AGI if it surpasses human performance on a task within a set duration T. Perhaps today’s large language models already approach a sub-hour T-AGI for certain cognitive work – rapid research, coding assistance, factual recall.</p> <p>Do these models already outperform our own System 1 thinking? For instant recall, like obscure facts or historical details, they often respond with superior speed and accuracy. While techniques like Chain-of-Thought aim for deeper reasoning, their fundamental strength seems rooted in this rapid, pattern-matching mode – System 1 scaled.</p> <p>Perhaps we should embrace this strength. Let the model be the powerful System 1 engine. Then, pair it with a distinct, perhaps more structured, System 2. Retrieval Augmented Generation (RAG) hints at this, combining parametric models (the LLM’s weights) with non-parametric knowledge (retrieved data) – an analogy for instinct coupled with explicit information.</p> <p>The true potential may lie in strengthening that second system, grounding it with deep context. Consider an AI accessing <em>your</em> personal knowledge – your memories, experiences, learned values, your unique nuance. What is a human stripped of memory, of personal history? An echo without a source, adrift. Our past is the lens through which we understand the present.</p> <p>An AI infused with such personal context could become a true cognitive partner. It could navigate the complexities of the world alongside us, offering insights tuned not just to general data, but to our individual lives. Perhaps, as Yuval Noah Harari posited, such systems could eventually learn to make better decisions on our behalf. The result: intelligence deeply personalized, genuinely valuable.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[Thoughts on the value of memory]]></summary></entry></feed>