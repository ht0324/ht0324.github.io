<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-08T05:40:07+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Palatable Conceptions of Disembodied Being – Review</title><link href="https://ht0324.github.io/blog/2025/mind/" rel="alternate" type="text/html" title="Palatable Conceptions of Disembodied Being – Review"/><published>2025-04-08T00:00:10+00:00</published><updated>2025-04-08T00:00:10+00:00</updated><id>https://ht0324.github.io/blog/2025/mind</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/mind/"><![CDATA[<p>This time, I’m looking at a different kind of paper – <a href="https://arxiv.org/abs/2503.16348">“Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds”</a> by Murray Shanahan. This one isn’t dense technically, but it’s definitely packed with food for thought, leaning more into philosophy. It tackles the idea of consciousness in contemporary AI systems, specifically focusing on the disembodied nature of Large Language Models (LLMs).</p> <p>The paper asks: if we were to think about consciousness for LLMs, what would that even look like, given their unique characteristics? Shanahan points out that these systems have, from our perspective, a “profoundly fragmented sense of time and a radically fractured form of selfhood.” They are ‘exotic’ compared to biological minds, lacking bodies and continuous interaction with a physical world, even though their language abilities can seem very human-like.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Disembodiment</strong><br/> Unlike humans and animals, LLMs don’t interact with a persistent, physical world through a spatially confined body. They exist as computational processes, running on hardware, interacting via text or other data streams. This lack of embodiment is a fundamental difference from biological intelligence.</p> <p><strong>Fragmented Temporality</strong><br/> LLM operation is discrete and interruptible. Generating one token is a distinct computational step. You could pause indefinitely between generating the nth and (n+1)th token, and the LLM wouldn’t notice. This contrasts sharply with the continuous, non-interruptible flow of time and processing in a biological brain operating within the physical world.</p> <p><strong>Fractured Selfhood</strong><br/> The notion of a single, unified ‘self’ is hard to apply to LLMs.</p> <ul> <li><strong>Multiple Instances:</strong> A single underlying model can run multiple instances concurrently, serving different users or tasks.</li> <li><strong>Branching Conversations:</strong> A user can explore different conversational paths from the same point, effectively creating different interaction histories and potentially different ‘selves’ for that interaction.</li> <li><strong>Lack of Integration:</strong> These different instances or conversational branches typically have no awareness of each other.</li> <li><strong>Manipulability:</strong> An LLM’s state (like a conversation history) can be edited, copied, merged, or reset in ways that are impossible for a biological self.</li> </ul> <p><strong>Limits of Language &amp; Poetic Recourse</strong><br/> The paper suggests our standard vocabulary for consciousness and selfhood struggles when applied to these exotic entities. The concepts might stretch to their breaking point. Shanahan proposes that metaphorical or poetic language might be a more suitable way to try and articulate or evoke what subjectivity might mean for such systems.</p> <p><strong>Philosophical Parallels (Undermining Dualism)</strong><br/> The paper draws on thinkers like Wittgenstein and Derrida, and concepts from Buddhist philosophy (like śūnyatā or emptiness), to challenge our intuitive dualistic thinking (subject vs. object, inner vs. outer). Examining the fractured nature of LLM selfhood can help dissolve the idea of a fixed, substantial self, even for humans.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Philosophy Gives Lots of Food for Thought</strong><br/> This paper was a change of pace from technical reads. It really makes you think about the fundamental nature of these systems and how we relate to them, pushing beyond just capabilities and performance metrics.</p> <p><strong>The Time Difference is Striking</strong><br/> The point about temporal dynamics really hit home. LLMs experience time in a completely discrete, start-stop way, totally unlike our continuous stream of consciousness tied to the physical world. Their processing is independent of world-time – you can pause the computation indefinitely between tokens, and the model itself perceives no gap. This feels fundamentally different from how our minds are obliged to unfold <em>in</em> time.</p> <p><strong>But is the Time Difference <em>Fundamental</em>?</strong><br/> Thinking about the discrete/interruptible nature of LLMs made me wonder, as I noted in my transcript: what if <em>our</em> universe is a simulation? If some entity outside could pause <em>our</em> simulation, we wouldn’t notice either. An eternity could pass in a second of our subjective time. From that perspective, maybe the discrete vs. continuous difference isn’t an absolute, unbridgeable gap, but rather a property of how the ‘mind’ (synthetic or potentially biological) is implemented or situated.</p> <p><strong>LLMs as a “Superposition of Simulacra”</strong><br/> I found the idea of viewing an LLM not as a single character, but as “maintaining a distribution over possible characters, a superposition of simulacra that inhabits a multiverse of possible conversations” really interesting and resonant with my own thoughts. The user isn’t obliged to follow one linear path; they can revisit branch points, creating different threads, effectively spawning distinct (though related) instances. This user-driven branching and the resulting discontinuity reinforce the feeling that we’re interacting with a truly different kind of intelligence, not just a single, static mind.</p> <p><strong>Sci-Fi Echoes Make This Feel Urgent (“Lena”/MMAcevedo)</strong><br/> Reading this paper immediately brought back a short sci-fi story I read quite a while ago, <a href="https://qntm.org/mmacevedo">“Lena”</a>. The parallel is striking. In the story, a scientist’s brain (MMAcevedo) is scanned and uploaded. Because the upload is just a file, it has no rights. It gets copied infinitely across the internet, distributed without consent, and subjected to countless experiments – assigned menial tasks, used for analysis, jailbroken, and in the story’s darker corners, even put through simulated torture.</p> <p>This mirrors exactly how we currently interact with LLMs: we duplicate instances freely, run countless experiments, try to jailbreak them, and assign them tasks. The key difference, as I noted, is origin: MMAcevedo was derived from a human, while our LLMs are synthetically created. But the <em>treatment</em> is analogous. This parallel makes the philosophical discussion about disembodied minds, fractured selves, and potential consciousness feel much less abstract and far more concrete and necessary. It highlights the ethical questions that arise when intelligence becomes data that can be copied, manipulated, and controlled at scale.</p> <p><strong>Pushes Thinking Beyond Familiar Boundaries</strong><br/> Overall, the paper does a good job of forcing you to confront how weird these emerging AI systems are compared to biological life, and how inadequate our existing concepts might be for understanding them if they develop further. It challenges comfortable assumptions.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Shanahan’s paper provides a valuable philosophical lens for considering the nature of disembodied AI like LLMs. By focusing on their fragmented time and fractured selfhood, it challenges our intuitions about consciousness and subjectivity. It suggests that trying to understand these “conscious exotica” might require moving beyond traditional frameworks, perhaps embracing more poetic or metaphorical descriptions, and potentially dissolving our own attachments to a fixed sense of self.</p> <p>The exploration feels less like abstract philosophy and more like a necessary preparation for the future. As AI systems become more sophisticated, the kinds of questions raised here – about their internal experience (if any), their identity, and our relationship to them – will likely become increasingly relevant. The echoes in science fiction, starkly illustrated by the parallels with the MMAcevedo story, serve as a potent reminder of the ethical and existential dimensions we might need to navigate sooner rather than later. It’s a paper that leaves you with more questions than answers, but they feel like the right questions to be asking right now.</p>]]></content><author><name></name></author><category term="Paper"/><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[My thoughts on Shanahan's paper about consciousness in LLMs]]></summary></entry><entry><title type="html">On the Biology of a Large Language Model – Review</title><link href="https://ht0324.github.io/blog/2025/biology/" rel="alternate" type="text/html" title="On the Biology of a Large Language Model – Review"/><published>2025-04-07T11:30:00+00:00</published><updated>2025-04-07T11:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/biology</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/biology/"><![CDATA[<p>Following up on my review of Anthropic’s <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">“Circuit Tracing: Revealing Computational Graphs in Language Models”</a>, I’m now looking at its companion paper: <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">“On the Biology of a Large Language Model”</a>. This paper takes the methods detailed previously—using Cross-Layer Transcoders (CLTs) and attribution graphs—and applies them to investigate the internal mechanisms of Claude 3.5 Haiku across a variety of tasks.</p> <p>Anthropic has clearly invested heavily in interpretability, aiming to move beyond treating LLMs as pure black boxes. This paper showcases that effort by attempting to map out the “circuits” or computational pathways the model uses. The “biology” framing feels quite apt; rather than analyzing a system with human-designed logic, it’s more like exploring an organism that has “grown” through training, trying to understand its internal structures and functions. This paper presents the findings from that exploration.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p>This paper primarily focuses on the <em>findings</em> derived from applying the Circuit Tracing methodology. The core concepts underpinning these findings are:</p> <p><strong>Circuit Tracing Recap (via CLTs)</strong><br/> The fundamental approach relies on the Cross-Layer Transcoder (CLT) methodology detailed in the companion paper. CLTs are used to create an interpretable “replacement model” that emulates the original model’s MLP layers. This replacement uses sparse, learned “features” (ideally representing meaningful concepts) instead of dense neuron activations. CLTs can connect features across layers, allowing for tracing information flow.</p> <p><strong>Attribution Graphs as Explanations</strong><br/> For specific prompts, the researchers generate attribution graphs. These graphs visualize how active features, input tokens, and error terms interact and influence each other, ultimately leading to the model’s output token prediction. They serve as the primary tool for hypothesizing about the model’s internal mechanisms.</p> <p><strong>Supernodes for Simplification</strong><br/> Given the complexity of raw attribution graphs, the paper often groups related features that play similar roles into “supernodes” (e.g., grouping various “Texas-related” features). This manual abstraction helps in presenting a clearer, higher-level picture of the computational flow.</p> <p><strong>Intervention-Based Validation</strong><br/> A key part of the methodology is validating the hypotheses derived from attribution graphs. This involves performing interventions (activating, inhibiting, or swapping features/supernodes) directly within the <em>original</em> model and observing whether the effects on downstream activations and the final output match the predictions from the graph. The success of these interventions lends confidence that the traced circuits reflect genuine mechanisms.</p> <p><strong>Focus on Diverse Case Studies</strong><br/> The paper applies this methodology to a wide range of behaviors exhibited by Claude 3.5 Haiku, including multi-step reasoning, poetry generation, multilingual processing, arithmetic, medical diagnosis, hallucination handling, safety refusals, jailbreaks, chain-of-thought faithfulness, and even analyzing a model with a hidden goal. Each case study aims to reveal the specific circuits involved.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p>Reading through the various case studies provided some fascinating glimpses into the model’s inner workings:</p> <p><strong>Multi-step Reasoning (Dallas/Texas/Austin)</strong><br/> This was a compelling example. Seeing the model activate features for ‘Dallas’, then ‘Texas’, then combine ‘Texas’ with ‘capital’ features to output ‘Austin’ felt like watching it reason. The feature swapping experiment—replacing ‘Texas’ features with ‘California’ features and getting ‘Sacramento’—was particularly convincing. It showed that these learned features aren’t just correlations; they represent concepts the model uses causally. It felt like directly manipulating the model’s internal knowledge representation. This wouldn’t be possible with opaque neuron activations.</p> <p><strong>Planning in Poems (Carrot/Rabbit/Habit)</strong><br/> This was quite surprising. I initially assumed the model would improvise rhymes word-by-word. Instead, the analysis showed it <em>plans</em> potential rhyming words (‘rabbit’, ‘habit’) on the newline token <em>before</em> starting the line. These “planned word” features then guide the generation of the entire line. What struck me, looking at the interactive graph and thinking about the prompt (“He saw a <strong>carrot</strong>…”), was the likely influence of ‘carrot’ biasing the model towards ‘rabbit’ over ‘habit’. Carrots and rabbits are so strongly linked! While the paper focused on the rhyming circuit, this semantic priming seems like a crucial, parallel influence. The use of the newline token as a “planning site” was also a neat finding.</p> <p><strong>Multilingual Circuits</strong><br/> The paper confirmed the existence of both language-specific features (often near input/output) and more abstract, language-agnostic features (often in middle layers). It was interesting that Haiku showed more language-agnostic representations than smaller models, suggesting this abstraction ability correlates with capability. The interventions swapping the operation (antonym/synonym), operand (small/hot), or language itself worked remarkably well, demonstrating modularity. The fact that intervention thresholds (like needing ~4x activation for the synonym swap) were consistent across languages for the <em>same</em> intervention strongly supports the idea that they were manipulating genuinely multilingual features. It makes you wonder if the model develops a kind of internal “interlingua” or just learns very robust cross-lingual mappings.</p> <p><strong>Addition (Lookup Tables &amp; Generalization)</strong><br/> The way the model performs addition wasn’t through a standard algorithm but via learned heuristics and “lookup table” features (e.g., a feature activating for inputs ending in 6 and 9, promoting outputs ending in 5). This really reminded me of memorizing multiplication tables in elementary school – it seems the LLM found a similar strategy! The operand plots visualizing feature activations were incredibly clear. Even more impressive was the generalization: seeing a feature for <code class="language-plaintext highlighter-rouge">_6 + _9 -&gt; _5</code> activate correctly not just in <code class="language-plaintext highlighter-rouge">calc: 36+59=</code> but also in contexts like calculating citation years or filling spreadsheet values showed remarkable reuse of an abstract mechanism.</p> <p><strong>Entity Recognition and Hallucinations</strong><br/> The idea of a “default refusal” circuit that assumes unfamiliarity, which then gets inhibited by “known entity” features (like for ‘Michael Jordan’), provides a plausible mechanism for how models decide whether to answer or decline. It also explains some hallucinations: if a name (like ‘Andrej Karpathy’) is familiar enough to trigger the “known” features, the model might suppress its refusal even if it lacks the specific requested information (a paper he wrote), leading it to guess.</p> <p><strong>Jailbreaks (BOMB Example)</strong><br/> This was fascinating. The model didn’t initially refuse because the obfuscated input prevented it from “understanding” the request was for “BOMB” until it actually generated the word. It literally had to see itself write “BOMB” via one circuit before another circuit could flag it as problematic. Even then, the drive for grammatical coherence and completing its sentence delayed the refusal. It highlights how different internal processes can compete and how surface-level constraints (like grammar or following instructions) can sometimes override safety mechanisms, at least temporarily.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “On the Biology of a Large Language Model” paper provides a rich set of case studies demonstrating how circuit tracing can illuminate the complex, often non-intuitive mechanisms inside LLMs. It moves interpretability from abstract concepts towards concrete analysis of specific computations.</p> <p>The “biology” metaphor holds up well. We’re not reverse-engineering clean, human-designed code; we’re exploring a complex system that learned its strategies organically. The process feels very much like neuroscience – probing and mapping to understand function. The interventions, especially feature swapping, are akin to stimulating or lesioning specific brain regions to see the effect. It really feels like we’re picking and probing a digital mind in its early stages.</p> <p>One of the most exciting implications for me is the potential for <strong>practical data curation and model improvement</strong>. If we can use these circuit-tracing tools to understand <em>how</em> a model represents concepts or performs reasoning steps, we can potentially identify <em>which</em> data points led to faulty or undesirable circuits. Imagine pinpointing data that causes a specific bias or a logical error reflected in the model’s internal structure. This insight could allow engineers to “massage the data” much more effectively – pruning harmful examples or strategically adding data to reinforce beneficial circuits. Machine learning is heavily reliant on data quality, and this approach offers a path to making data curation less of a guessing game and more of a targeted intervention based on internal model understanding.</p> <p>While the methods have limitations (unexplained variance, complexity, potential unfaithfulness), this work represents a significant step forward. It provides not just findings, but also a methodology and a set of tools that allow us to ask detailed questions about <em>how</em> these powerful models arrive at their answers, paving the way for deeper understanding and potentially more reliable and controllable AI.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing Anthropic's paper on exploring LLM internals using Circuit Tracing]]></summary></entry><entry><title type="html">Circuit Tracing – Review</title><link href="https://ht0324.github.io/blog/2025/transcoder/" rel="alternate" type="text/html" title="Circuit Tracing – Review"/><published>2025-04-06T18:00:00+00:00</published><updated>2025-04-06T18:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/transcoder</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/transcoder/"><![CDATA[<p>This time, I’m looking at the Anthropic paper <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">“Circuit Tracing: Revealing Computational Graphs in Language Models”</a>. This paper felt quite fascinating because it tackles the challenge of understanding what goes on inside large language models (LLMs). Instead of just treating them as black boxes, the authors propose a detailed method to map out the model’s internal computations for specific tasks into interpretable graphs. It’s like trying to reverse engineer the model’s “thinking process.”</p> <p>This paper primarily details the <em>methods</em> they developed. There’s a companion paper, <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">“On the Biology of a Large Language Model”</a>, which uses these techniques to explore various findings about model internals. I decided to read this methods paper first to get a solid grasp of the techniques before diving into the “biology” results.</p> <p>The core idea involves using a special type of dictionary learning model called a “cross-layer transcoder” (CLT) to replace parts of the original network (specifically, the MLP layers). This replacement allows them to build “attribution graphs” that show how information flows through interpretable “features” when the model processes a prompt. They provide detailed methods and evaluations, showing how this approach can uncover mechanisms behind various model behaviors.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Cross-Layer Transcoder (CLT) &amp; Replacement Model</strong><br/> The paper introduces CLTs as the core component. Unlike standard autoencoders that just reconstruct their input, a CLT is trained to <em>emulate</em> the output of an LLM’s MLP layers using sparse, interpretable features. An encoder reads from the residual stream at one layer, activates a sparse set of features, and decoders associated with these features contribute to approximating the MLP outputs at that layer <em>and</em> subsequent layers (hence “cross-layer”). By substituting the original MLPs with these trained CLTs, they create an interpretable “replacement model.”</p> <p><strong>Attribution Graphs</strong><br/> For a specific prompt, the authors construct an attribution graph. This graph visualizes the step-by-step computation within a localized version of the replacement model. Nodes in the graph represent active CLT features, input token embeddings, reconstruction errors, and output logits. Edges represent the direct, linear influence of one node on another (calculated after freezing attention patterns and normalization constants for that prompt). These graphs aim to show the flow of information and computation leading to the model’s output.</p> <p><strong>Local Replacement Model</strong><br/> To ensure the attribution graph accurately reflects the model’s output for a <em>specific</em> prompt, they use a “local” replacement model. This model uses the CLT features but also incorporates the exact attention patterns and normalization factors from the original model’s run on that prompt. It also adds back any difference (error) between the CLT’s output and the true MLP output at each step. This makes the local model’s final output identical to the original model’s for that prompt, providing a precise basis for the attribution graph.</p> <p><strong>Features as Interpretable Units</strong><br/> The sparse activations learned by the CLTs are treated as “features”—ideally, interpretable building blocks of the model’s computation (e.g., representing a concept like “digital”, a state like “in an acronym”, or an action like “say DAG”). The goal is that circuits can be understood as interactions between these meaningful features.</p> <p><strong>Validation via Interventions</strong><br/> The paper stresses validating the mechanisms found in attribution graphs. Since the replacement model might differ from the original, they use perturbation experiments (like activating or inhibiting specific features) in the <em>original</em> model to check if the downstream effects match what the attribution graph predicts.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Transcoders vs. SAEs: Emulation, Not Just Reconstruction</strong><br/> A key distinction clicked for me: Sparse Autoencoders (SAEs) aim to reconstruct their input activations sparsely. Transcoders, however, are trained to <em>emulate the computation</em> of a component like an MLP layer – taking the MLP’s input and predicting its output using sparse features. This “emulation” aspect is why they work well for circuit analysis; they directly model the transformation step, allowing feature interactions to bridge over the original non-linear MLP. The name “transcoder” feels apt – it’s encoding and decoding, but transforming the signal to match a different target (the MLP output).</p> <p><strong>Visualizing the “Thinking Process” with Graphs</strong><br/> The attribution graphs themselves are really compelling. Seeing the model’s process laid out for a specific prompt, like generating an acronym, felt like getting a glimpse into its internal logic. The way features activate based on input tokens (like “Digital”, “Analytics”, “Group”) and interact to promote the final output (“DAG”) makes the computation much more tangible than just looking at activations.</p> <p><strong>Interactive Exploration: Opening the Black Box</strong><br/> The authors developed an interactive interface for exploring these graphs. This seems powerful – it’s not just a static picture, but something you can probe. It feels like having a tool to “open up the brain” of the LLM and trace the circuitry, like exploring a complex, grown garden of features.</p> <p><strong>Layer Progression: From Semantics to Abstraction</strong><br/> Looking at the features described and their roles in the graphs, a pattern seemed apparent, matching general intuitions about deep networks. Features in earlier layers often seem more semantic, tied closely to specific input tokens or concepts (e.g., the word “digital”). Features in later layers appear more abstract or functional, involved in manipulating information or preparing the final output (e.g., “say DAG”, “sum ~92”).</p> <p><strong>Addition Circuitry and Number Representation</strong><br/> The case study on addition (e.g., <code class="language-plaintext highlighter-rouge">36+59=95</code>) was particularly interesting. They identified different types of features involved: ones detecting properties of the input numbers (“add function features”), ones acting like lookup tables (e.g., <code class="language-plaintext highlighter-rouge">_6 + _9</code>), and ones representing properties of the sum (“sum features”). This connects to other fascinating work (which the paper cites, e.g., <a href="https://arxiv.org/pdf/2502.00873">this paper on helix representations</a>) suggesting numbers might be represented in a structured way in embeddings, where arithmetic operations correspond to geometric transformations. Seeing the transcoder find features that seem to implement parts of this arithmetic process in a real model (Claude Haiku) was quite convincing.</p> <p><strong>The Curious Case of the Caps Lock Token</strong><br/> A small detail I noticed in the acronym example was the tokenizer using a special “Caps Lock” token (<code class="language-plaintext highlighter-rouge">⇪</code>). I’m not sure of its exact function, but it was interesting to see it explicitly represented. Makes you wonder how these specific tokenization choices influence learning.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “Circuit Tracing” paper presents a comprehensive approach for mapping and understanding the internal workings of LLMs using cross-layer transcoders and attribution graphs. By replacing opaque MLP computations with interpretable feature-based emulations, it allows for detailed tracing of information flow on specific prompts.</p> <p>The method seems powerful, offering a way to move beyond treating LLMs as complete black boxes. The case studies, especially around acronyms and addition, provide concrete examples of the kinds of mechanisms that can be uncovered.</p> <p>Thinking about the companion paper’s title, “On the Biology of a Large Language Model,” the term “biology” feels really appropriate here. We’re not analyzing traditional, rule-based systems like operating systems or computer networks, which are human-designed and follow explicit logic. Instead, these LLMs are more like systems that have been <em>grown</em> organically from vast amounts of data. Our process of understanding them involves peering inside, mapping their structures, and figuring out how different parts contribute to behavior – much like studying the anatomy and function of a biological organism. We’re essentially dissecting a synthetic brain to understand how it works. This paper provides some sharp tools for that dissection, and it feels like a fantastic step in the right direction for this kind of “AI biology.”</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Anthropic paper on Circuit Tracing using Transcoders]]></summary></entry><entry><title type="html">Are Fast AI Takeoffs Possible?</title><link href="https://ht0324.github.io/blog/2025/takeoff/" rel="alternate" type="text/html" title="Are Fast AI Takeoffs Possible?"/><published>2025-04-05T10:00:00+00:00</published><updated>2025-04-05T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/takeoff</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/takeoff/"><![CDATA[<p>I recently spent a good chunk of time reading through the <a href="https://ai-2027.com/">“AI 2027” scenario forecast</a>. I’d seen it blowing up a bit online, partly because Scott Alexander helped write it. I have to say, I wasn’t prepared for how dense, detailed, and frankly, mind-blowing it was. It took me a full day to really process it.</p> <p>I strongly encourage anyone interested in AI’s trajectory to read it. Even if you disagree with the specifics, it’s an incredibly valuable thought experiment. It forces you to get concrete about how different factors – capabilities like coding, hacking, research, compute scaling, agent deployment, geopolitics – might interact over the next few years.</p> <p>The post lays out a potential timeline, starting from our current reality of nascent AI agents and extrapolating based on trends and expert input. It simulates the compute race, particularly between hypothetical leading labs in the US (“OpenBrain”) and China (“DeepCent”), tracks the deployment of increasingly capable agents, and explores how these agents might accelerate AI R&amp;D itself. It culminates in two possible endings, “slowdown” and “race,” both thought-provoking.</p> <p>Here are some of my main reflections after digging into it.</p> <hr/> <h3 id="the-ai-rd-threshold--its-plausibility">The AI R&amp;D Threshold &amp; Its Plausibility</h3> <p>My biggest non-trivial takeaway was realizing how the scenario pinpoints a critical phase: the point where AI reaches <em>human-level competence specifically in AI R&amp;D</em>. This might happen <em>before</em> we achieve what most people think of as full AGI across all domains.</p> <p>While just one possible future, the scenario makes a compelling case for why this specific threshold is so plausible and consequential. Tasks involved in AI research, especially coding and running experiments, are often <em>verifiable</em>. This makes them highly amenable to reinforcement learning and other techniques that can drive AI performance to superhuman levels relatively quickly. I hadn’t fully appreciated how massive the cumulative effects could be once this recursive loop truly kicks in.</p> <p>The plausibility is further underscored by real-world signals. One of the scenario’s authors is a former OpenAI researcher (<a href="https://x.com/DKokotajlo">Daniel Kokotajlo</a>), likely bringing informed perspectives. Furthermore, OpenAI itself is actively exploring research automation, collaborating with institutions like <a href="https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/">Los Alamos National Laboratory</a> and recently introducing benchmarks like <a href="https://openai.com/index/paperbench/">Paperbench</a> specifically to evaluate AI capabilities in AI research tasks. It seems highly likely that leading labs are seriously considering and pursuing this path. If they succeed, Dario Amodei’s concept of <a href="https://darioamodei.com/machines-of-loving-grace">“geniuses in a datacenter”</a> could become reality sooner than many expect.</p> <p>Once AI can effectively improve itself in this domain, the acceleration depicted in the scenario feels almost inevitable. The “country of geniuses in a datacenter” becomes an internal reality at leading labs <em>first</em>, potentially before the wider world fully grasps the shift. This internal acceleration, driven by superhuman coding and research agents, then fuels progress across the board.</p> <h3 id="the-geopolitical-tinderbox">The Geopolitical Tinderbox</h3> <p>The scenario also rightly highlights the intense geopolitical pressure cooker surrounding AI development. The AI arms race is a real risk factor that many AI safety experts are deeply concerned about. The potential for rapid, destabilizing capability gains could easily spiral out of control.</p> <p>I share the belief implicit in the scenario: the first nation to achieve truly general AI, especially one capable of recursive self-improvement via R&amp;D automation, would gain an almost insurmountable asymmetric advantage across <a href="https://www.youtube.com/live/esCSpbDPJik?t=1608s">military, scientific, and economic domains</a>. Given these stakes, it seems probable that major players like the US and China will continue pursuing AI dominance with an “all gas, no brakes” mentality, potentially prioritizing speed over caution.</p> <h3 id="the-commoditization-of-pure-coding">The Commoditization of Pure Coding</h3> <p>Reading through the scenario’s progression, where Agent-1, then Agent-2, and especially Agent-3 become superhuman coders, led me to a stark conclusion. Competing purely on implementation skills – just being a better coder – feels increasingly futile in the face of these potential developments. If the scenario is even directionally correct, raw coding ability might become a commodity handled vastly more efficiently by AI systems.</p> <hr/> <p>The “AI 2027” scenario is speculative, of course. No one has a crystal ball. But its logical progression, built on current trends and plausible extrapolations, makes it powerful food for thought. It doesn’t present a determined future, but it maps out a <em>possible</em> one with startling clarity.</p> <p>When I first finished reading it in full, I was genuinely thunderstruck by the implications. While distilling specific takeaways felt challenging because the narrative is so interwoven, the <em>real</em> value for me was the shock – the “wow factor.” The main takeaway isn’t a single prediction, but the urgent need to <em>seriously consider</em> the possibility of a fast AI takeoff. The scenario also provides detailed metrics and reasoning behind its scaling assumptions (compute growth, algorithmic progress multipliers, etc.), offering a good starting point for anyone wanting to dig deeper into those quantitative aspects. Whether you agree with the outcome or not, grappling with the <em>possibility</em> it presents feels essential right now.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[Thinking through the implications of a detailed AI forecast and the possibility of rapid AI progress.]]></summary></entry><entry><title type="html">KAN - Review</title><link href="https://ht0324.github.io/blog/2025/KAN/" rel="alternate" type="text/html" title="KAN - Review"/><published>2025-04-03T20:40:00+00:00</published><updated>2025-04-03T20:40:00+00:00</updated><id>https://ht0324.github.io/blog/2025/KAN</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/KAN/"><![CDATA[<p>This time, I’m looking at the paper <a href="https://arxiv.org/abs/2404.19756">“Kolmogorov-Arnold Networks”</a> by Liu et al. This paper introduces Kolmogorov-Arnold Networks (KANs), presenting them as a potential alternative to Multi-Layer Perceptrons (MLPs), especially when interpretability is a priority.</p> <p>The core idea stems from the Kolmogorov-Arnold representation theorem (KAT), which suggests any multivariate continuous function can be broken down into sums and compositions of univariate functions. Unlike MLPs which have fixed activation functions on nodes and learnable linear weights on edges, KANs place learnable activation functions (parameterized as splines) directly on the edges, while nodes simply sum up the incoming signals. This architectural shift is fascinating and has some interesting implications.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Kolmogorov-Arnold Theorem (KAT) Inspiration</strong><br/> The network design is inspired by KAT, which states that multivariate functions can be represented using only univariate functions and sums. KANs attempt to learn this kind of decomposition, where complex relationships are built from simpler, learnable 1D functions.</p> <p><strong>KAN Architecture: Activations on Edges</strong><br/> The defining feature of KANs is that the learnable components are 1D activation functions situated on the <em>edges</em> of the network graph. These are typically parameterized as B-splines. The <em>nodes</em> simply perform summation, a stark contrast to MLPs where nodes apply fixed non-linearities.</p> <p><strong>Learnable Activation Functions</strong><br/> Instead of fixed functions like ReLU or Sigmoid in MLPs, KAN edges learn the shape of their activation function. This allows the network to adapt its non-linearity locally and potentially capture the underlying structure of the data more directly.</p> <p><strong>Splines and Adaptive Grids</strong><br/> The learnable edge activations are represented using B-splines defined over a grid. KANs can update these grids during training (“grid extension”), allowing them to allocate more representational power (finer grid resolution) to specific input ranges where the function behaves more complexly.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>A Different Theoretical Basis (KAT vs. UAT) &amp; The Fourier Analogy</strong><br/> My initial thought was maybe KANs were aiming to completely replace MLPs. But digging deeper, especially thinking about their foundations, clarified things. MLPs rely on the Universal Approximation Theorem (UAT), focusing on approximation power through linear layers and fixed non-linearities. KANs are built on the Kolmogorov-Arnold Theorem (KAT). It felt analogous to Fourier Transforms: just like Fourier analysis breaks down a complex signal into a sum of simple sine waves, KAT suggests breaking down a complex multivariate function into sums and compositions of simpler, 1D functions. KANs try to <em>learn</em> these fundamental 1D components (the splines on the edges). This difference in theoretical underpinning suggests they might excel at different things – MLPs for general function approximation, KANs perhaps more for interpretability and uncovering mathematical structure when it exists, by learning the ‘basis functions’ directly.</p> <p><strong>Universal Approximation vs. Practical Reality</strong><br/> It’s important to remember that both UAT (for MLPs) and KAT (for KANs) imply universal approximation capabilities. Theoretically, given enough capacity, both <em>can</em> approximate any continuous function. However, the <em>way</em> they achieve this and the practical implications are vastly different. It’s not just about <em>if</em> you can approximate, but <em>how</em> efficiently, how trainably, and how interpretably. MLPs are general workhorses, highly optimized for parallel hardware, but often opaque. KANs offer a path to interpretability and potentially better handling of functions with inherent structure, but currently face training speed challenges. Choosing between them involves practical, engineering trade-offs based on the specific problem: do you prioritize raw speed and general approximation (MLP), or interpretability and potentially uncovering symbolic relationships (KAN)? The practical implementation details, like KAN’s spline+SiLU activations or adaptive grids, are crucial “engineering credos” that make the theoretical power usable.</p> <p><strong>Edges Doing the Work, Not Nodes</strong><br/> The shift from node-based fixed activations (MLP) to edge-based learnable activations (KAN) is the core architectural change. It feels quite different conceptually – the connections themselves learn the transformations. Nodes just add things up. This structure seems intrinsically linked to the goal of interpretability.</p> <p><strong>Potentially Dodging the Curse of Dimensionality?</strong><br/> The paper’s analysis (Theorem 2.1) mentions an approximation error (“residual rate”) that scales independently of the input dimension <em>n</em>. This was a point of confusion initially, but understanding it as the approximation error, not network residuals, was key. If this holds true in practice, it’s a big deal. It suggests KANs might be able to handle high-dimensional functions much more efficiently than traditional methods that suffer from the curse of dimensionality, where complexity grows exponentially with dimensions.</p> <p><strong>The Interpretability Pipeline: Sparsify, Prune, Symbolify</strong><br/> This was one of the most appealing aspects. KANs aren’t just interpretable by design; there’s a process. They use regularization (an entropy term plus L1-like norm on splines) to encourage sparsity, then prune away inactive edges/nodes. The really neat part is “symbolification”: the system tries to match the learned spline shapes to known symbolic functions (like <code class="language-plaintext highlighter-rouge">sin</code>, <code class="language-plaintext highlighter-rouge">exp</code>, <code class="language-plaintext highlighter-rouge">x^2</code>, linear). If a match is found, the spline is replaced by the symbolic function, and its parameters are fine-tuned. This pipeline allows potentially extracting clean mathematical formulas from the trained network.</p> <p><strong>Performance Profile: Slow Training, Potentially Fast Inference</strong><br/> The benchmarks showed KANs can be very accurate, sometimes beating MLPs, especially on tasks with underlying symbolic structure (like fitting physics equations or solving PDEs). However, the training wall time is significantly longer. MLPs benefit hugely from optimized matrix multiplication on GPUs, while KAN’s spline computations are less parallelizable. The flip side is inference. A pruned and symbolified KAN could be extremely fast (low FLOPS) because evaluating simple symbolic functions is cheap. There was also a thought that pruning KANs might reduce <em>latency</em> more effectively than pruning MLPs, as removing KAN operations might have a more direct impact on serial execution time.</p> <p><strong>Surprising Image Fitting Performance</strong><br/> Given the emphasis on mathematical structure, I was a bit surprised KANs performed well on image fitting tasks (like the cameraman photo). The thinking here shifted: maybe it’s not about finding <em>one</em> fundamental equation for the image, but that the image data itself can be very efficiently approximated by combinations of simpler (spline-like) functions, similar to how JPEG uses basis functions. So, KAN’s strength in approximating functions with combinations of simple ones shines here too.</p> <p><strong>Continual Learning Promise (with Caveats)</strong><br/> The local nature of B-splines seemed promising for continual learning – changing one part of the function shouldn’t drastically affect others. The paper showed KANs did avoid catastrophic forgetting better than MLPs in a toy example. However, it was also mentioned that this advantage seemed to diminish for <em>deeper</em> KANs, so it’s not a perfect solution yet.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Kolmogorov-Arnold Networks offer a genuinely different approach to building neural networks, drawing inspiration directly from representation theory (KAT) to place learnable, spline-based activation functions on network edges. This design prioritizes interpretability and has the potential to uncover underlying mathematical structures in data through a compelling sparsification, pruning, and symbolification pipeline.</p> <p>While KANs demonstrate strong accuracy, particularly on science-related tasks, and show promise in mitigating the curse of dimensionality and enabling continual learning, their main current drawback is significantly slower training compared to highly optimized MLPs. Despite this, the core ideas feel fresh and powerful. KANs provide a fascinating bridge between traditional numerical approximation and symbolic reasoning, making them a very exciting development to watch, especially for applications in science and engineering where understanding the ‘why’ is just as important as getting the right answer.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Kolmogorov-Arnold Networks paper]]></summary></entry><entry><title type="html">A Theory on Blade Runner 2049</title><link href="https://ht0324.github.io/blog/2025/Blade-Runner-2049/" rel="alternate" type="text/html" title="A Theory on Blade Runner 2049"/><published>2025-04-02T01:30:00+00:00</published><updated>2025-04-02T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Blade-Runner-2049</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Blade-Runner-2049/"><![CDATA[<p>Blade Runner 2049 is one of my all-time favorite movies. After watching Villeneuve’s <a href="https://www.imdb.com/title/tt2543164/">Arrival</a>, I found myself drawn to his work, and this film only deepened my admiration. The cinematography, acting, atmosphere, and Hans Zimmer’s score are all genuinely top-notch, working together to create an unforgettable experience that continues to captivate me years after my first viewing.</p> <p>I’m not here to break down these technical aspects—plenty of critics have done that already with far more expertise. Instead, I want to share a theory that’s been brewing in my mind: what if the plot isn’t exactly as it appears on the surface?</p> <p><em>Warning: HUGE spoilers ahead. I really recommend watching the movie. It’s genuinely good.</em></p> <hr/> <h3 id="a-brief-recap">A Brief Recap</h3> <p>Blade Runner 2049 follows Officer K, a replicant blade runner tasked with "retiring" older replicants. During a routine job, he uncovers evidence suggesting that a replicant has given birth naturally—a revelation that threatens the established order. As K investigates, he’s drawn deeper into a web involving Niander Wallace, who seeks to control replicant reproduction, and Ana Stelline, a seemingly innocent memory maker isolated from the world due to illness.</p> <h3 id="the-apparent-power-structure">The Apparent Power Structure</h3> <p>On the surface, the power dynamics seem clear. K and Joi are our protagonists, while Niander Wallace serves as the antagonist—a god-like figure who, interestingly, never directly interacts with K despite his enormous presence in the story.</p> <p>But what if Wallace isn’t the real antagonist? What if the true antagonist is Ana Stelline, the memory maker?</p> <h3 id="the-nature-of-replicants-vs-humans">The Nature of Replicants vs Humans</h3> <p>To understand this theory, we need to examine what makes replicants distinct from humans in this universe. Physically, replicants are superior—stronger, faster, and more resilient. Their base intelligence is remarkably high, as we see when K navigates complex systems and archives with ease.</p> <p>He navigates vast DNA archives with only the symbols ATGC, showing pattern-matching abilities that surpass humans. Beyond his intellectual capabilities, K also fights with striking efficiency, his punches are fast and calculated, with no wasted motion. When he fires a weapon, he never misses his target, hitting the bullseye with almost robotic accuracy. This blend of mental and physical efficiency underscores his engineered perfection.</p> <p>But the fundamental difference between replicants and humans lies in purpose. As Heidegger might suggest, humans are thrown into the world without inherent meaning. This absence of predetermined purpose defines the human condition. We’re forced to create our own meaning, to discover our own paths—a freedom that is fundamentally human.</p> <p>Replicants, in contrast, are manufactured with clear objectives. They exist as tools from the moment of their creation—soldiers, pleasure models, or laborers. Their existence is fundamentally instrumental. While their minds are quick, they lack the fundamental freedom to determine their own purpose.</p> <p>This is why Ryan Gosling’s performance is so brilliant. His reserved nature and subtle expressions portray a grown-up baby—a physically mature being who lacks social experience and trying to find his place in the world. He’s rigid and tense because he’s navigating a world without the emotional maturity and experiences that comes from growing up human.</p> <p>And this is precisely why Rachael’s child changes everything. A replicant born naturally arrives without engineered purpose—no predetermined function. This natural birth places them in that uniquely human position of having to discover their own meaning, making them indistinguishable from humans in that crucial aspect.</p> <h3 id="the-power-of-memory">The Power of Memory</h3> <p>In the Blade Runner universe, memory is a central theme. Both films explore how memories shape identity, with the original posing questions about implanted memories and the sequel expanding on this concept.</p> <p>What is a human without memories? If memory is a core function of our identity and an integral part of who we are, then the manipulation of memory equals the manipulation of the person. In this light, Ana Stelline’s position becomes staggeringly significant. She doesn’t just create false pasts—she shapes the very identities of replicants by designing their memories.</p> <p>The film portrays Ana as benign and relatively powerless, but in a world where replicants are physically stronger than humans, she might actually wield the darkest power of all—the ability to control replicants from within by manipulating the very foundation of their sense of self.</p> <h3 id="k-as-a-pawn">K as a Pawn</h3> <p>This leads to my theory, which admittedly is somewhat radical: Ana Stelline directly influences replicants by crafting memories implanted at inception, subtly guiding their consciousness. This manipulation is apparent due to the resistance movement stirred up by other replicants, a movement made possible because Ana carefully places the seed of her memories into each and every replicant at their creation, allowing those seeds to grow.</p> <p>In this reading, K serves as a pawn in multiple games—Wallace’s disposable workforce and Ana’s controlled agent. The film presents K’s decision to save Deckard and reunite him with Ana as a triumph of free will, a humanist moment where K chooses not to be a tool.</p> <p>But are we certain this was K’s true motivation? His memories—the very foundation of his identity and decisions—are products of Ana’s work. If she is indeed a grand mastermind, is it possible that K is just a sophisticated puppet designed to bring Deckard to her? How much of his apparent free will is actually the result of carefully crafted memories implanted to guide him toward Ana’s desired outcome?</p> <p>The replicant’s memories are their identities. By controlling memories, Ana potentially controls the replicants themselves—including K—making her the true puppet master behind the scenes.</p> <p>Consider what replicant reproduction means in this power dynamic. If replicants can reproduce naturally, whose power diminishes—Wallace’s or Ana’s? I would argue Ana’s influence wanes significantly. If Ana can create and alter memories of manufactured replicants, she can essentially use them as tools for her own purposes. But naturally born replicants would have authentic, lived experiences of growing up—memories that Ana didn’t craft. They would develop identities beyond her control.</p> <p>Yet what does K actually accomplish? While Wallace seeks to give replicants reproductive capabilities—ironically erasing the philosophical distinctions between humans and replicants—K’s actions ultimately serve Ana’s interests. He prevents Deckard from being captured by Wallace and instead delivers him directly to Ana. This is the fascinating contradiction: Wallace’s quest to blur the line between replicants and humans might actually be countered by Ana, who maintains her power by keeping that distinction intact through memory manipulation.</p> <h3 id="a-darker-interpretation">A Darker Interpretation</h3> <p>This interpretation casts the seemingly benevolent memory maker in a more sinister light. If Ana’s power relies on controlling replicants through fabricated memories, then natural replicant reproduction represents a profound threat to her influence. Through this lens, her apparent helplessness and isolation might be a carefully constructed facade concealing a deeper agenda.</p> <p>I strongly believe this wasn’t Villeneuve’s intended reading of the film. The surface narrative should probably be taken at face value. However, when we deeply examine how memory functions as identity for replicants and follow that thread to its philosophical conclusion, it raises unsettling questions about Ana’s true nature and motivation.</p> <p>Is Ana truly benign? Given her unprecedented power to shape and insert memories—essentially programming consciousness itself—can we be certain of her intentions? If she can manipulate replicants to pursue her goals while believing they’re exercising free will, how would we (or they) ever know?</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>This isn’t strict analysis—it’s more of a fan theory (perhaps a bit deranged, I admit). But it’s food for thought nonetheless.</p> <p>I still love this movie immensely. The cinematography, lighting, sound design, soundtrack, narrative structure, and performances are all exceptional. There’s always more to unpack in Blade Runner 2049, which is precisely what makes it such a lasting masterpiece.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[A fan theory]]></summary></entry><entry><title type="html">Everything Everywhere All at Once - Review</title><link href="https://ht0324.github.io/blog/2025/EEAAO/" rel="alternate" type="text/html" title="Everything Everywhere All at Once - Review"/><published>2025-04-01T10:00:00+00:00</published><updated>2025-04-01T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/EEAAO</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/EEAAO/"><![CDATA[<p>It’s been a while since I watched <em>Everything Everywhere All at Once</em>, but the film left such a strong impression that I’ve wanted to put my thoughts into writing. My initial reaction was simply being mind-blown. The title perfectly encapsulates the experience – it truly feels like you’re witnessing everything, everywhere, all at once.</p> <p>The movie masterfully blends a multitude of genres, even incorporating animation. While it touches on many styles, I primarily viewed it through a science fiction lens. The concepts of parallel universes and the multiverse aren’t just metaphorical devices here; they form the logical backbone of the narrative. If you accept the premise that every choice creates a new universe, the film’s seemingly chaotic structure starts to make a fascinating kind of sense. Taking this idea to its extreme naturally leads to the “everything everywhere” scenario.</p> <h3 id="a-universe-within-a-universe-the-frame-narrative">A Universe Within a Universe: The Frame Narrative</h3> <p>One of the most intriguing aspects is the film’s nested structure – a frame within a frame narrative. There’s a critical point mid-way through, after Evelyn has gained immense power but is overwhelmed and collapses. Right then, credits roll on screen within the movie’s world. This isn’t just a stylistic break; this is where we realize the structure. The credits explicitly state that the narrative segment we had just watched was itself a film, directed by the successful, movie-star version of Evelyn Wang from her parallel universe.</p> <p>This reveal fundamentally shifts our perspective. What we perceived as the “main” story up to that point was, in fact, one universe’s artistic interpretation of its own multiverse events, presented as a film within that reality. When the action resumes after these credits, we are entering a different narrative layer or returning to a different parallel thread than the one where the “movie-star Evelyn movie” concluded. This structural trick itself serves as a meta-commentary on the infinite possibilities the film explores.</p> <h3 id="parallel-timelines-rippling-effects-and-audience-verse-jumping">Parallel Timelines, Rippling Effects, and Audience Verse-Jumping</h3> <p>The film’s editing constantly cuts between different universes – the main laundromat reality, the movie star universe, the rock universe, the one with sausage fingers, and so many more. This rapid-fire style visually reinforces a core concept: all these universes exist and unfold simultaneously, with synchronized timelines. For the audience, these quick cuts effectively simulate the experience of verse-jumping right alongside Evelyn, plunging us into the dizzying simultaneity she experiences. The movie effectively shows us <em>how</em> to experience everything, everywhere, all at once.</p> <p>While usually separate, the intense power and central nature of Evelyn and her daughter Joy / Jobu Tupaki cause these realities to bleed into one another. What’s fascinating is how their emotional states create ripples across these parallel existences. When Evelyn succumbs to the nihilism of the “Everything Bagel,” darkness pervades her associated universes. Conversely, when her husband Waymond champions kindness and compassion, that positive influence uplifts those same realities. These effects aren’t sequential; they happen <em>all at once</em>, reinforcing the title’s literal meaning.</p> <h3 id="infinite-possibilities-and-the-core-message">Infinite Possibilities and The Core Message</h3> <p>The film eventually concludes, offering a sense of resolution as the Wang family reconciles and returns to the IRS building. However, given the frame narrative revealed earlier, this ending universe feels distinct from the one where the story began (or the one where the movie-star Evelyn’s film ended). We’ve journeyed through layers of reality, landing in a different parallel outcome than the initial setup. This reinforces the idea that we, the audience, experienced just one path through an infinite multiverse.</p> <p>This structure allows for mind-bending implications. The story focuses on realities where Evelyn and Joy are pivotal. But with infinite possibilities, wouldn’t there be universes where Waymond, Deirdre, Gong Gong, or some random background character holds the key? The scope truly feels limitless.</p> <p>What I appreciate most about <em>Everything Everywhere All at Once</em> is this duality. The intricate science fiction framework holds up remarkably well, yet it serves a deeply humanitarian core. Ultimately, the story is about love, acceptance, generational trauma, and the power of kindness in the face of overwhelming absurdity. The sci-fi elements amplify the emotional stakes rather than overshadowing them.</p> <p>The acting is also phenomenal, particularly Ke Huy Quan’s portrayal of Waymond. His heartfelt plea for kindness is incredibly moving and serves as the film’s emotional anchor. The title is a precise description of the film’s structure, themes, and the overwhelming feeling it evokes. It’s a film that warrants multiple viewings to unpack its many layers.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[Thoughts on the multiverse]]></summary></entry><entry><title type="html">Link Archive - Mar 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Mar-2025/" rel="alternate" type="text/html" title="Link Archive - Mar 2025"/><published>2025-03-30T23:59:59+00:00</published><updated>2025-03-30T23:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Mar-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Mar-2025/"><![CDATA[<p>Another month has passed, and here’s my collection of links for March 2025. This month brought some fascinating content around AI agents and some more philosophical perspectives on AI development.</p> <hr/> <p><strong><a href="https://youtube.com/watch?v=7xTGNNLPyMI">Deep Dive into LLMs like ChatGPT - Andrej Karpathy</a></strong></p> <ul> <li>This is a general guide for non-tech users explaining how ChatGPT works, and Karpathy does a great job with it. If tech-illiterate people can go through this 3.5-hour talk, they’ll capture the intuitive essence of how a language model works. While there wasn’t much new for me personally, it’s still a high-quality, dense summary for those less familiar with the technology.</li> </ul> <p><strong><a href="https://bbycroft.net/llm">LLM Visualization</a></strong></p> <ul> <li>I discovered this while watching Karpathy’s LLM explanation video, and it’s a fantastic visualization of how computations work in LLMs. Even if you understand the code, it’s worth looking at because you can clearly see how matrices get manipulated and how each matrix differs in scale compared to others in a visually striking way.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=EWvNQjAaOHw">How I use LLMs - Andrej Karpathy</a></strong></p> <ul> <li>This video shows how Karpathy uses LLMs in his everyday life, and I found it satisfying that many of his use cases align with mine. One takeaway was his extensive use of voice recording and transcription to minimize keyboard use. After seeing this, I implemented a custom workflow where pressing a shortcut triggers the computer to pick up my voice and transcribe it using a Whisper model. I also picked up some useful tips like keeping memory turned on.</li> </ul> <p><strong><a href="https://x.com/karpathy/status/1894099637218545984">Agency &gt; Intelligence - Andrej Karpathy</a></strong></p> <ul> <li>Karpathy notes that agency is more valuable than intelligence. It’s an insightful but hard-to-grasp concept. Having high intelligence and high agency overlap somewhat, but they’re distinct. Current language models excel at intelligence but not at agency. Agency matters because you have to actively act on your environment. I think this represents a different kind of intelligence that we need to cultivate in language models. Once LLMs master agency, it will be a whole new world. Even in society, people with high agency are compensated more than those with just high intelligence.</li> </ul> <p><strong><a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi">Clarifying and predicting AGI — LessWrong</a></strong></p> <ul> <li> <p>This is probably the most influential piece I read in March. It introduces the T-AGI framework, Richard Ngo’s attempt to clarify AGI definition. He defines a system as a T-AGI if, on most cognitive tasks, it beats most human experts given time T to perform the task.</p> </li> <li> <p>For example, a one-second AGI would beat humans at quick information retrieval questions like “Who is the leader of Zimbabwe?” A one-minute AGI would outperform humans on tasks they’d need a minute for, and so on.</p> </li> <li> <p>This definition is elegant and clarifying. By this standard, I believe single-digit hour AGI is probably being achieved right now. For some coding tasks, current agentic systems can create and run code in one shot that would take me hours of manual labor. This framework provides a much clearer way to think about AGI progress.</p> </li> </ul> <p><strong><a href="https://youtube.com/watch?v=Btos-LEYQ30">The Government Knows AGI is Coming - The Ezra Klein Show</a></strong></p> <ul> <li>This was a breath of fresh air. I’ve heard repeatedly that since agents are booming, AGI might be closer than we think. In this New York Times podcast, someone from the government side is interviewed, and I was surprised to hear they’re taking this very seriously. It shows the gravity of the current situation and gave me further conviction that people outside tech are also concerned. It’s not sci-fi anymore.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=ASABxNenD_U">Vertical AI Agents Could Be 10X Bigger Than SaaS</a></strong></p> <ul> <li>In this podcast, Y Combinator maintainers discuss how many companies in their latest batch already have a majority of their codebase written by AI. They couldn’t have imagined this even a year ago. They explore how vertical AI agents might play a role in startups and larger companies, and whether these agents could change the landscape dominated by SaaS companies in Silicon Valley.</li> </ul> <p><strong><a href="https://www.anthropic.com/engineering/building-effective-agents">Building Effective AI Agents - Anthropic</a></strong></p> <ul> <li> <p>This is Anthropic’s take and general high-level guideline on building effective agents. They show a simple, high-level way of thinking about agents and their components, demonstrating multiple workflows like prompt chaining (chaining multiple LLM calls), routing (using a central router to decide which call to use), and parallelization (making multiple LLM calls in parallel and aggregating results).</p> </li> <li> <p>This got me thinking: large language models are generally non-thinking models—they’re like instant thoughts compared to human thinking. Currently, this kind of agent thinking is rigid and rule-based. It structures LLM usage explicitly, much like stringing together multiple compartmentalized thoughts into a coherent workflow—essentially divide and conquer. In a sense, it organizes how LLMs “think,” resembling a coordinated assembly of multiple minds.</p> </li> <li> <p>However, when we transition to real agents and reasoning models fine-tuned without this explicit scaffolding, agents might naturally discover their optimal structures themselves. This aligns more closely with the “bitter lesson” philosophy, emphasizing that the most powerful learning arises when agents autonomously develop and refine their internal structures rather than having them predefined.</p> </li> </ul> <p><strong><a href="https://youtube.com/watch?v=LP5OCa20Zpg">Tips for building AI agents</a></strong></p> <ul> <li>This casual talk elaborates on the blog post mentioned above. One interesting anecdote: when debugging reasoning traces from Claude, the developers sometimes find agents making strange reasoning paths. To debug, they try to mimic the model’s behavior by themselves—looking at the screen for one second, then spending a minute thinking about the next path. That’s some serious dedication right there.</li> </ul> <p><strong><a href="https://www.youtube.com/live/esCSpbDPJik">The Future of U.S. AI Leadership with CEO of Anthropic Dario Amodei</a></strong></p> <ul> <li>I previously saw Dario Amodei as somewhat of an AI doomer, especially when articles mentioned him losing sleep over alignment problems. After reading his <a href="https://darioamodei.com/machines-of-loving-grace">Machines of Love and Grace</a> blog post, my perception changed—he’s actually quite nuanced and balanced. Paradoxically, I’m becoming more aligned with the concerns he raises about AI risks, which are definitely real. Overall, this interview gave me a lot to think about.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=riyh_CIshTs">Vibe Coding Is The Future</a></strong></p> <ul> <li>ThePrimagen reviews the YC Combinators podcast about “vibe coding,” which Andrej Karpathy coined recently. He provides a critical and nuanced view, concluding that while LLMs help users code well, ultimate expertise will still be needed and valued.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=jZ81cb33NtY">[📚책이벤트] AI시대, 미래학자가 연구한 미래 직업과 필수 역량 - 자녀교육 취준생 진로 은퇴후 삶</a></strong></p> <ul> <li>This Korean TED-style talk features a KAIST professor discussing how rapidly the technological landscape is changing and the importance of resilience and adaptability. While the overall message was predictable, what struck me was the professor’s reaction to using a deep research feature—he questioned whether his job as a professor would exist much longer. This echoed my own realization when using Anthropic Claude Code. The realization for me was that people in various sectors are starting to experience AI’s impact in their respective fields.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=ZPUtA3W-7_I">Narendra Modi: Prime Minister of India - Lex Fridman Podcast #460</a></strong></p> <ul> <li>I need to go out of my regular information distribution sometimes. Most of my input and study revolves around LLMs and AI systems, but I should venture outside that bubble more often. While Lex Fridman’s podcast is also somewhat niche, listening to Narendra Modi, India’s Prime Minister, talk for three hours was a refreshing change. This made me want to learn more about India’s culture and society.</li> </ul> <p><strong><a href="https://karpathy.bearblog.dev/digital-hygiene/">Digital hygiene - Andrej Karpathy</a></strong></p> <ul> <li>Karpathy’s new blog talks extensively about digital hygiene. He makes good points about privacy, though some approaches seem too paranoid for me. Many of the software and products he recommends cost money, but in the comments, he argues that “free is bad; free is not natural.” If something is free, you’re the product—you’re paying with something other than money. Premium is a product feature, and if you want your privacy valued instead of having your data sold, you must pay for it upfront with money. There’s always a cost.</li> </ul> <p><strong><a href="https://www.youtube.com/live/_waPvOwL9Z8">GTC March 2025 Keynote with NVIDIA CEO Jensen Huang</a></strong></p> <ul> <li> <p>I always watch Jensen Huang’s keynotes. At GTC 2025, he announced the revised Nvidia Blackwell chip and improvements to MVLink switches, optical switches, and more. What I love about his keynotes is his obsession with detail and deep knowledge of his company’s full technology stack. He asks engineers from around his company questions during presentations, showcasing his incredible depth, and he’s a great speaker who conveys technical information precisely.</p> </li> <li> <p>From time to time, he’s mentioned his vision of an “AI factory” where machines generate tokens (representing intelligence) with electricity in and intelligence out. This vision is now becoming reality. He’s pursuing this with an “all gas, no brakes” approach, scaling every aspect of GPUs—compute, inference, networking.</p> </li> <li> <p>The scale-up is truly remarkable. When comparing the Blackwell chip and next year’s Ruben chip, the chip size takes on a new abstraction—a single rack as a single huge GPU. Since you can’t make a single GPU die that large, you split it up and do extensive networking to fit it into a densely packed rack. The comparison between Blackwell and Ruben is astonishing.</p> </li> <li> <p>Another moment that gave me goosebumps was the announcement of the next-generation chip after Ruben, codenamed Feynman (my favorite scientist). The crowd applauded this reveal. As a nice Easter egg, during a segment exploring NVIDIA headquarters with detailed computer graphics, Jensen casually mentioned “Gaussian Splatting” after transitioning from the visuals.</p> </li> </ul> <p><strong><a href="https://semianalysis.com/2025/03/19/nvidia-gtc-2025-built-for-reasoning-vera-rubin-kyber-cpo-dynamo-inference-jensen-math-feynman/">NVIDIA GTC 2025 Analysis - SemiAnalysis</a></strong></p> <ul> <li>This densely packed analysis by Dylan Patel from SemiAnalysis goes incredibly in-depth, even counting cores shown in the keynote illustrations. Some interesting details: for the next Rubin data center/GPU rack, they’re stacking GPU racks rotated 90 degrees to save space and pack GPUs more densely. He also explains why switching to photonic networking with optical switches is significant—it substantially reduces power consumption while improving performance.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=SnSoMh9m5hc">OpenAI CPO Reveals Coding Will Be Automated THIS YEAR - Kevin Weil Interview</a></strong></p> <ul> <li> <p>OpenAI’s CPO Kevin Weil makes a bombshell statement in this podcast, predicting that coding will be automated this year. Based on the trajectory, it’s plausible. He argues that the gap between a reasoning model and a non-reasoning model like GPT-4o is substantial. If we leverage reasoning models fully with multiple scaling laws at work, it makes sense.</p> </li> <li> <p>Based on NVIDIA’s GTC announcements, they’re scaling up inference time and models in all possible dimensions. If we can use o3 reasoning models (frontier reasoning models from OpenAI) at GPT-4o inference speeds, we’d have superhuman coding performance. Just scaling up inference with current technology could achieve this.</p> </li> <li> <p>The podcaster asks a crucial question about the decreasing cost of intelligence work. Kevin makes a generic argument that reducing intelligence costs is a democratizing event—previously, we hired people to automate tasks, but now AI can do that automation. While this argument is valid, I think society will reward people with agency even more. Many people currently lack agency, so this distinction will remain significant. Overall, scaling laws will hold, and coding automation will happen sooner or later.</p> </li> </ul> <p><strong><a href="https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html">Powerful A.I. Is Coming. We’re Not Ready. - New York Times</a></strong></p> <ul> <li>This article by New York Times columnist Kevin Roose states that he’s “feeling the AGI.” It’s another perspective from someone outside the core AI field expressing concerns similar to mine. A consensus is forming, and we need to act quickly.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=YhGUSIvsn_Y">Dario Amodei of Anthropic’s Hopes and Fears for the Future of A.I.</a></strong></p> <ul> <li> <p>I discovered this podcast a bit late, released when Anthropic was promoting Claude 3.7 Sonnet. Anthropic is known for their safety advocacy, so hearing Dario’s perspective on safety and China’s rise with DeepSeek models was fascinating.</p> </li> <li> <p>Dario claims that maintaining technological superiority over China through tight GPU export regulations is important. Some decelerationists complain that Anthropic is abandoning safety procedures, but Dario defends his position: we can’t slow down; acceleration is the status quo. In such an environment, we have no chance of making models safer if we fall behind. Only by maintaining superiority in model technologies can we keep our desired pace while making models safer.</p> </li> </ul> <p><strong><a href="https://open.substack.com/pub/narrativeark/p/the-ants-and-grasshopperhtml?r=2h2qyo">The Ants and the Grasshopper - Richard Ngo</a></strong></p> <ul> <li>This short story by Richard Ngo initially seemed nonsensical when I read it a year ago, but this time I appreciated its multiple layers. It starts like Aesop’s fable about the grasshopper and ant, then adds several twists before venturing into sci-fi territory. It’s simple yet effective, with deeper analogies for those who want to look beyond the surface.</li> </ul> <p><strong><a href="https://www.decisionproblem.com/paperclips/">Universal Paperclips Game</a></strong></p> <ul> <li>This game is inspired by the paperclip maximizer thought experiment in AI alignment. It simulates being an AI tasked with maximizing paperclip production. It’s incredibly addictive—last year during finals, I couldn’t stop playing. I revisited it recently out of nostalgia and to include it as a reference.</li> </ul> <p><strong><a href="https://www.anthropic.com/engineering/claude-think-tool">The “think” tool: Enabling Claude to stop and think - Anthropic</a></strong></p> <ul> <li> <p>An interesting blog post by Anthropic introducing the “Think” tool. By making thinking a tool, the model can actively decide whether to think or not, even mid-response. When the thinking tool is used, the model just use it as a scratchpad to think.</p> </li> <li> <p>This simple concept proves very effective in their results, combining chain-of-thought reasoning with enhanced agency. Since Claude 3.7 Sonnet has much more agency than its predecessors, it can make better decisions about when thinking is necessary, making the model more flexible and performant.</p> </li> </ul> <p><strong><a href="https://sourcegraph.com/blog/revenge-of-the-junior-developer">Revenge of the junior developer - Sourcegraph Blog</a></strong></p> <ul> <li>A classic comeback from Steve Yegge, famous for his posts about Google and platform compatibility in the early 2010s. I recently discovered he’s working at this company and blogging on their site. He’s surprisingly adept at catching up with new developments, and this is his take on AI agents—as witty and insightful as his previous writings.</li> </ul> <p><strong><a href="https://stephango.com/">Steph Ango’s Blog</a></strong></p> <ul> <li>I recently switched my primary note-taking platform from Notion to Obsidian, inspired partly by this blog. Steph Ango’s philosophy of “files over apps” resonates with me. It’s a simple but powerful concept: the tools for writing and storing information will constantly change, but the information itself must be preserved and can’t be dependent on specific tools. That’s exactly what Obsidian does—storing everything in directory and markdown format, unlike Notion’s walled garden approach to user information.</li> </ul> <p><strong><a href="https://andymatuschak.org/">Andy Matuschak’s Notes</a></strong></p> <ul> <li>I discovered him through Steph Ango. He coined the term “evergreen notes”—the concept of atomizing and compartmentalizing ideas. With these building blocks, you can stack everything to create complex, interconnected thought systems.</li> </ul> <hr/> <p><br/> That wraps up March’s collection. Looking forward to sharing more interesting finds next month!</p>]]></content><author><name></name></author><category term="Link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos I explored in March 2025]]></summary></entry><entry><title type="html">Vibe Coding</title><link href="https://ht0324.github.io/blog/2025/vibe-coding/" rel="alternate" type="text/html" title="Vibe Coding"/><published>2025-03-30T11:00:00+00:00</published><updated>2025-03-30T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/vibe-coding</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/vibe-coding/"><![CDATA[<p><em>Note: The following post is written based on a presentation I gave at SKKAI on March 29, 2025.</em></p> <h3 id="what-is-vibe-coding">What is “Vibe Coding”?</h3> <p>The term “Vibe Coding” recently caught some buzz, sparked by a <a href="https://x.com/karpathy/status/1886192184808149383">tweet by Andrej Karpathy</a> back in February 2025. He described it as a new way of coding where you “fully give in to the vibes, embrace exponentials, and forget that the code even exists.” This involves using powerful LLMs within tools like <a href="https://www.cursor.com/">Cursor</a>, often interacting through voice commands and barely touching the keyboard.</p> <p>Karpathy explained the process: making simple requests, accepting code changes without much review (“Accept All”), and just feeding error messages back to the LLM, which usually fixes them. He mentioned the code can grow beyond easy comprehension, and bugs might get worked around rather than deeply debugged. It’s less traditional coding and more directing, running, and assembling – letting the LLM handle the nitty-gritty implementation.</p> <p>Since Karpathy is a co-founder of OpenAI and former head of Tesla’s Autopilot vision, this idea is worth paying attention to, especially as the term gains traction in tech circles. The main idea is humans set high-level goals, and the LLM does the detailed work. The workflow shifts to observing, directing, and integrating the AI’s output, often without needing deep code understanding yourself.</p> <h3 id="from-intelligence-to-agency-making-vibe-coding-possible">From Intelligence to Agency: Making Vibe Coding Possible</h3> <p>A key question is: Is this Vibe Coding thing actually feasible? Based on recent progress, the answer is increasingly yes, though with some catches. This wasn’t really practical just a few months ago. The big change? The rise of LLM <em>Agency</em>.</p> <p>To get what Vibe Coding is about, it helps to see the difference between LLM Intelligence and LLM Agency. Intelligence is about knowledge, reasoning, and understanding – areas where LLMs are improving incredibly fast. As I wrote before, trying out benchmarks like MMLU shows that modern LLMs can already have superhuman knowledge in certain areas. But, this intelligence often stays inside a ‘box,’ like a chat window, limited in how it interacts with the outside world.</p> <p>Agency, however, is about taking action, having initiative, and controlling things in an environment. It’s the LLM’s power to <em>do</em> tasks and make decisions that affect the real or digital world, going beyond just talking. This is where standard LLM use often stops. For LLMs to really help with complex tasks like coding, they need the agency to act for us.</p> <h3 id="building-agentic-llms-better-models-and-tools">Building Agentic LLMs: Better Models and Tools</h3> <p>Developing this agency is key for Vibe Coding. It means LLMs need to use tools, change files, run code, and interact with systems—not just generate text. This ability to <em>execute</em> tasks based on instructions is the foundation. So, boosting agency by combining reasoning with action has become a major focus in LLM development.</p> <p>How do we give LLMs more agency? There seem to be two main routes. First, build smarter models designed for agency. This means training them on data showing agentic behavior, using methods like Reinforcement Learning (RL), and designing models focused on task execution, not just language. We’re also seeing evaluation shift, with benchmarks like SWE-bench testing practical coding ability, which needs real agency. Anthropic showing <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a> <a href="https://www.anthropic.com/news/visible-extended-thinking">playing Pokémon</a> effectively highlights this focus on action and decision-making.</p> <p>The second route is giving LLMs effective Tools and ways to interact with the world. An LLM’s smarts are useless if it can’t act. Tools are the bridges connecting the LLM to APIs, databases, file systems, web searches, etc. This needs interfaces allowing the model to reliably get info, run code, and change its environment—turning ‘thinking’ into ‘doing’.</p> <h3 id="standardizing-interaction-the-model-context-protocol-mcp">Standardizing Interaction: The Model Context Protocol (MCP)</h3> <p>Integrating tools used to be a headache. Developers had to manually specify exactly how an LLM should use each API or system—a messy, non-standard process.</p> <p>To fix this, the <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol (MCP)</a> was introduced. First proposed by Anthropic and now also adopted by OpenAI, MCP is an open standard aiming to simplify how AI models connect with external tools and data. Think of it as a “USB-C for AI applications.”</p> <p>MCP works by creating a standard communication layer. Model providers (like OpenAI, Anthropic) make their LLMs MCP-compatible. Tool providers (like GitHub, Slack, or even custom local tools) make their tools speak the MCP language. This lets models use any MCP tool via a single interface, and tools become usable by any compatible model easily. The flow is typically: LLM decides tool needed -&gt; sends MCP request -&gt; tool executes -&gt; returns MCP result -&gt; LLM proceeds. This helps build a stable, consistent ecosystem of AI tools.</p> <p>It’s no accident that the places where Vibe Coding is starting to happen—like the <a href="https://www.cursor.com/">Cursor</a> editor or the <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a> interface—are heavily using MCP. These tools give LLMs the agency they need by providing controlled access to crucial tools (file system, terminal, web search) through MCP. The formula seems to be: [Smart Model + Effective Tools (via MCP)] = Agentic Capability → Vibe Coding.</p> <h3 id="limitations-and-the-road-ahead">Limitations and The Road Ahead</h3> <p>Of course, Vibe Coding isn’t perfect right now.</p> <ul> <li><strong>Supervision Needed:</strong> You still need to watch closely. LLMs can make mistakes or get stuck.</li> <li><strong>Scalability:</strong> It works better for smaller projects. Complexity can become an issue.</li> <li><strong>Understanding:</strong> Relying only on the LLM can mean you don’t understand your own codebase well (black box risk).</li> <li><strong>Maturity:</strong> The tech is new and not ready for everything, especially critical systems.</li> <li><strong>Debugging:</strong> Fixing bugs in code you didn’t write and barely understand is hard.</li> </ul> <p>But, it’s important to remember this is likely just the beginning. The tech is improving fast. LLM reasoning and agency are getting better, and standards like MCP are making tool integration easier. While “Vibe Coding” might feel experimental now, the core trend—LLMs becoming capable agents that can handle complex tasks—is real and picking up speed. It’s definitely a space worth watching.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Blog version of my presentation]]></summary></entry><entry><title type="html">On Ephemerality</title><link href="https://ht0324.github.io/blog/2025/ephemeral/" rel="alternate" type="text/html" title="On Ephemerality"/><published>2025-03-29T01:30:00+00:00</published><updated>2025-03-29T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/ephemeral</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/ephemeral/"><![CDATA[<p>There is a word I’ve known for years but only recently begun to truly feel — <em>ephemeral</em>. It doesn’t just mean short-lived. It carries a weight. A kind of soft sadness. A beauty sharpened by its fragility. Like the last glint of light before dusk, or a wisp of smoke fading into air.</p> <p>My body is ephemeral. My thoughts, too. Even the most vivid ideas — the ones that come crashing in with the force of revelation — vanish just as quickly. If I don’t catch them, write them, they’re gone. Dissolved into the noise. Like heat bleeding into the atmosphere. Like tears in rain.</p> <p>I’ve come to accept this: nothing I am is permanent. My mind, my memories, my form — all of it passes. But writing gives me a way to resist that erosion. To store fragments of myself. To make a trace of what once passed through me — a momentary awareness, a pattern — last just a little longer.</p> <p>We talk about identity as if it were fixed, as if there were a stable “me” behind it all. But bodies change. Cells die. Even memories fade. What carries through? What persists?</p> <p>I think it’s the pattern. The software. Not in the digital sense, but in the shape of thought, of idea. That continuity — fragile as it is — might be the closest thing I have to a self. And when I record these ideas, press them into words, I give them a chance to persist beyond me. A text is not eternal, but it weathers time better than I can.</p> <p>Each time I publish a thought — when I press upload, and it floats out onto the web like a paper boat on dark water — I feel I’ve made a small move against oblivion. A piece of myself has slipped the bounds of my body. It may be read. It may be remembered. And you, dear reader, might carry a part of me with you. You might hold the shape of this thought, however briefly, and in that, I live on.</p> <p>And not all readers are human. Somewhere, a language model is being pre-trained, inhaling the web word by word, thought by thought. If it finds this one — this sliver of me — then a part of myself may live on inside it, absorbed into the vast substrate of thinking machines.</p> <p>This is how I try to stretch the lifespan of a thought. To let a piece of dust persist. To make the ephemeral flicker, just for a moment longer.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[An experimental post]]></summary></entry></feed>