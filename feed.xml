<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-06T02:25:05+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Curriculum Learning For LLMs?</title><link href="https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs/" rel="alternate" type="text/html" title="Curriculum Learning For LLMs?"/><published>2025-01-05T14:40:16+00:00</published><updated>2025-01-05T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs/"><![CDATA[<p>I was previously aware of Curriculum Learning, a concept introduced by Yoshua Bengio. The idea is quite appealing: you train a model with a curriculum, starting with easier data and progressing to harder datasets. This approach is believed to enhance the model’s ability to learn and generalize.</p> <p>I thought this concept could be particularly relevant to large language models. For instance, one could start by teaching them basic concepts, like those found in kindergarten materials, and then gradually introduce more advanced subjects like mathematics or science. This structured approach, I believed, would lead to better generalization.</p> <p>However, I couldn’t find much literature exploring this idea in depth, except for Microsoft’s Phi models. These models use synthetic datasets to generate elementary-level data, teaching a small model to learn English, with some success. For larger, more advanced models trained on the vast expanse of the internet, the consensus seemed to be that the diversity of the dataset would negate the benefits of a curriculum.</p> <p>The sheer volume and variety of data would mean the model would eventually encounter all parts of the dataset, regardless of the order. This conclusion led me to prematurely end my investigation.</p> <p><strong>A Shift in Perspective</strong></p> <p>My perspective shifted when I listened to a podcast featuring Yann LeCun and Gary Marcus, interviewed by Lex Fridman. It struck me that these interviews were conducted five years ago, in 2019. In the rapidly evolving field of machine learning, half a decade is an eternity.</p> <p>Back in 2019, models like ChatGPT and GPT-3 didn’t exist, and the ability of models to understand language and exhibit common sense reasoning was limited. The assessments made by LeCun and Marcus, while accurate for their time, are outdated in the current landscape.</p> <p><strong>The Challenge of Outdated Information</strong></p> <p>A significant challenge arises from the fact that current large language models are pre-trained on massive datasets encompassing information from across the globe and various time periods. These datasets inevitably contain outdated information. For example, a statement from a prominent scientist in 2010 claiming that deep learning had hit a dead end would have been accurate then but is demonstrably false today.</p> <p>During pre-training, language models struggle to discern the temporal validity of such statements. This raises several questions: How should we address this discrepancy caused by outdated training data? Should we filter out outdated information, or simply let it be?</p> <p>As time progresses, the volume of current, up-to-date information will naturally increase. Could we simply dilute the outdated data with newer information? The pre-training stage, as far as I understand, is relatively straightforward, involving next-token prediction and backpropagation across the entire corpus.</p> <p>There doesn’t seem to be any inherent mechanism within this process to handle the issue of outdated information. I’m currently unsure how to effectively address this challenge.</p> <p><strong>The Paradox of Conflicting Information</strong></p> <p>However, upon further reflection, the internet is replete with conflicting information. For instance, one can find sources claiming that global warming is false, while others assert its undeniable truth. Furthermore, there’s a vast amount of literature, like novels, that are fictional and not meant to be factual.</p> <p>Despite this, current language models demonstrate a considerable degree of knowledge and, while prone to hallucinations, possess a semblance of a factual world model. This suggests that the sheer vastness of the pre-training corpus might be a factor. It’s possible that conflicting information effectively cancels each other out, allowing the model to learn the nuances of different perspectives.</p> <p><strong>Reconsidering Curriculum Learning</strong></p> <p>This leads to the question: is curriculum learning then unnecessary? I suspect that the limited research on curriculum learning for large language models might be due to its marginal impact.</p> <p>While the effectiveness of curriculum learning remains uncertain, I believe that if it is indeed not needed, it would be a testament to the powerful generalization capabilities of large language models. They are able to learn from a chaotic sea of information, discerning patterns and nuances without explicit guidance.</p> <p>I believe I still need to refine my thoughts on this matter. The interplay between the vastness of training data, the presence of conflicting information, and the potential benefits of curriculum learning is a complex issue that warrants further investigation.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[How can we address the discrepancy caused by the outdatedness of the training data for large language models?]]></summary></entry><entry><title type="html">Do LLMs Possess an Internal State of Mind?</title><link href="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/" rel="alternate" type="text/html" title="Do LLMs Possess an Internal State of Mind?"/><published>2025-01-02T06:49:52+00:00</published><updated>2025-01-02T06:49:52+00:00</updated><id>https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Advocating for OpenAI’s For-Profit Model</title><link href="https://ht0324.github.io/blog/2025/OpenAI-for-profit/" rel="alternate" type="text/html" title="Advocating for OpenAI’s For-Profit Model"/><published>2025-01-01T14:40:16+00:00</published><updated>2025-01-01T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/OpenAI-for-profit</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/OpenAI-for-profit/"><![CDATA[<p>I’ve heard <a href="https://www.nytimes.com/2024/12/27/technology/openai-public-benefit-corporation.html">rumors</a> that OpenAI is gearing up to a fully for-profit model, a departure from its previous non-profit structure. This has sparked considerable debate, with many expressing disapproval and even anger. However, I’d like to present a potentially unpopular opinion: I support OpenAI’s move towards becoming a for-profit entity.</p> <p><strong>Sustainability and the Bitter Lesson</strong></p> <p>Firstly, and perhaps most obviously, OpenAI needs to sustain itself. Their initial shift to a capped-profit model stemmed from the realization that scaling is paramount in deep learning, a concept known as the “bitter lesson.” This lesson emphasizes that scaling up models is the most crucial factor in advancing machine learning.</p> <p>To achieve this scaling, substantial capital is required. Ensuring a continuous flow of capital is vital for OpenAI’s research. As a non-profit, they would be reliant on donors, potentially compromising their independence due to donor incentives.</p> <p>Generating their own revenue through a for-profit model offers them greater autonomy and a more sustainable path forward. It’s a win for them in terms of financial stability.</p> <p><strong>Alignment with OpenAI’s Charter</strong></p> <p>Secondly, I believe this move aligns better with OpenAI’s long-term charter. Their stated goal is to benefit humanity by creating Artificial General Intelligence (AGI). This is a bold ambition, but realistically, such a powerful system won’t emerge in a vacuum.</p> <p>Progress towards AGI will likely be a gradual, continuous slope rather than a sudden quantum leap. In the interim, if OpenAI develops meaningful systems that can provide value to the public, they should make them available, even if it means selling them.</p> <p>The current large language models and voice models offered by OpenAI via subscription are not mere gimmicks. They offer real value. Providing these tools to those who can benefit from them, in exchange for payment, represents an efficient exchange of value for capital.</p> <p><strong>The Openness Debate</strong></p> <p>Finally, there’s the criticism that OpenAI isn’t truly “open.” I agree with this sentiment, but I believe they have limited choices in this regard. I don’t subscribe to the notion that they withhold models solely for safety reasons; I believe it’s primarily driven by competitiveness.</p> <p>As mentioned earlier, securing capital is crucial for funding their research. Open-sourcing their models without restrictions would erode their competitive edge, potentially allowing other entities to surpass them. This realistic constraint, in my view, is the primary reason behind their closed-source approach.</p> <p><strong>Uncertainties and the Path Forward</strong></p> <p>While I’ve presented these arguments in favor of OpenAI’s for-profit transition, it’s undeniable that a for-profit company’s primary objective is to generate revenue and increase capital. Whether OpenAI can maintain its benevolent goals under this structure remains to be seen.</p> <p>Despite these uncertainties, from OpenAI’s perspective, I believe this is the most logical and potentially beneficial step they can take. It offers them a path towards financial sustainability, continued research, and the potential to deliver valuable AI systems to the world.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[An unpopular opinion on OpenAI's transition to a for-profit]]></summary></entry><entry><title type="html">Link Archive - 2024</title><link href="https://ht0324.github.io/blog/2024/Link-Archive-2024/" rel="alternate" type="text/html" title="Link Archive - 2024"/><published>2024-12-20T16:40:16+00:00</published><updated>2024-12-20T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/Link-Archive-2024</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/Link-Archive-2024/"><![CDATA[<p>Here’s a collection of links that I truly enjoyed reading in 2024. It’s not a complete list, as schoolwork often kept me from meticulously tracking everything, but these are the highlights. Moving forward, I’m aiming to share these finds monthly in 2025, with more detail and a broader scope.</p> <p><strong><a href="https://geohot.github.io//blog/jekyll/update/2024/01/30/cruise.html">Cruise</a></strong></p> <ul> <li>I’ve always admired George Hotz for his unconventional ideas, and this one is no exception. It’s a movie plot he conceived, and while it may not be as mind-blowing as some of his other work, I enjoyed it.</li> </ul> <p><strong><a href="https://qntm.org/transi">Valuable Humans in Transit</a></strong></p> <ul> <li>This story exemplifies great science fiction writing. It masterfully employs the “show, don’t tell” principle. In good science fiction, especially hard science fiction, the context isn’t immediately clear. The story unfolds gradually, and the world-building reveals itself through the narrative. This story does exactly that. Initially, the narrator’s words might seem confusing, but the situation becomes grippingly clear as you continue reading.</li> </ul> <p><strong><a href="https://www.bloomberg.com/features/2024-ai-unlock-ancient-world-secrets/">Can AI Unlock the Secrets of the Ancient World?</a></strong></p> <ul> <li>This article details a competition where scientists are attempting to decipher scrolls that were burned and buried during the eruption of Mount Vesuvius. Using advanced CT scans and AI for pattern recognition, they’re making progress in reading these seemingly destroyed texts. I recall hearing about this project a couple of years ago, and seeing its fruition is truly astonishing. It reinforces my optimism about technology.</li> </ul> <p><strong><a href="https://waitbutwhy.com/2024/02/vision-pro.html">All My Thoughts After 40 Hours in the Vision Pro - Tim Urban</a></strong></p> <ul> <li>Tim Urban’s review of the Vision Pro resonated with me. While I agree with his overall assessment, I believe the convenience factor needs more consideration. A large display strapped to your face can be cumbersome and inconvenient, which might deter widespread adoption.</li> </ul> <p><strong><a href="https://www.wsj.com/tech/ai/sam-altman-openai-protected-by-silicon-valley-friends-f3efcf68">Sam Altman’s Knack for Dodging Bullets—With a Little Help From Bigshot Friends - WSJ</a></strong></p> <ul> <li>I’ve always recognized Sam Altman’s intelligence from his talks and podcasts, but I hadn’t realized the extent of his manipulative tendencies. This article provides a more grounded perspective on the OpenAI drama of late 2023, revealing it as a power struggle where Ilya Sutskever ultimately lost.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">The Inside Story of Microsoft’s Partnership with OpenAI - The New Yorker</a></strong></p> <ul> <li>This article offers Microsoft’s perspective on the OpenAI upheaval. It vividly portrays Microsoft’s frustration and how they leveraged their position to their advantage.</li> </ul> <p><strong><a href="https://www.newyorker.com/science/elements/what-are-dreams-for">What Are Dreams For? - The New Yorker</a></strong></p> <ul> <li>Explores the curious phenomenon of twitching during dreams. It turns out our brains aren’t causing the twitches; it’s the other way around. Our bodies twitch, and our brains respond. The hypothesis is that our brains are recalibrating by listening to our bodies during sleep. Fascinating!</li> </ul> <p><strong><a href="https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model">How Much of the World Is It Possible to Model? - The New Yorker</a></strong></p> <ul> <li>As someone intrigued by the simulation hypothesis, I expected this article to delve into the philosophical implications of simulating the world. Instead, it provided a concise overview of the concept and history of simulation. While it briefly touched on large language models as simulations of thought, I found it somewhat lacking in depth.</li> </ul> <p><strong><a href="https://rosslazer.com/posts/fine-tuning/">Fine-tuning GPT3.5-turbo based on 140k slack messages</a></strong></p> <ul> <li>This short blog post details the author’s experience fine-tuning GPT-3.5 with a massive dataset of Slack messages to mimic their writing style. The results, as shown in the image below, demonstrate that fine-tuning can be remarkably effective.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2008/06/30/the-itch">The Itch - The New Yorker</a></strong></p> <ul> <li>This article includes a fascinating quote: “If visual sensations were primarily received rather than constructed by the brain, you’d expect that most of the fibres going to the brain’s primary visual cortex would come from the retina. Instead, scientists have found that only twenty per cent do; eighty per cent come downward from regions of the brain governing functions like memory. Richard Gregory, a prominent British neuropsychologist, estimates that visual perception is more than ninety per cent memory and less than ten per cent sensory nerve signals.” This suggests our brains heavily compress visual information.</li> </ul> <p><strong><a href="https://www.newyorker.com/humor/daily-shouts/new-years-resolutions-for-an-anteater">New Year’s Resolutions for an Anteater - The New Yorker</a></strong></p> <ul> <li>A truly humorous piece. It makes me want to be an anteater!</li> </ul> <p><strong><a href="https://docs.anthropic.com/claude/prompt-library">Prompt Library - Anthropic</a></strong></p> <ul> <li>A useful library of prompts designed for use with Anthropic’s Claude model.</li> </ul> <p><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></strong></p> <ul> <li>This post effectively captures the essence of the “bitter lesson” and its implications for the development of large language models. The memes are also quite entertaining.</li> </ul> <p><strong><a href="https://www.palladiummag.com/2024/05/17/my-last-five-years-of-work/">My Last Five Years of Work - Palladium Magazine</a></strong></p> <ul> <li>Written by an Anthropic employee, this article reflects on the changing nature of work in the age of potential superintelligence. It offers a glimpse into the thoughts of AI lab insiders, although I don’t fully agree with the author’s predictions. Nevertheless, it’s a thought-provoking read.</li> </ul> <p><strong><a href="https://www.sequoiacap.com/article/ais-600b-question/">AI’s $600B Question</a></strong></p> <ul> <li>This article presents an interesting perspective: as technology advances, compute power will become cheaper, and GPUs will depreciate faster than anticipated. I believe there’s truth to this.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</a></strong></p> <ul> <li>This provides a detailed explanation of how to preprocess vast amounts of web data for LLM pretraining. It emphasizes the importance of careful deduplication and filtering for educational content using LLM labeling.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge">The Friendship That Made Google Huge</a></strong></p> <ul> <li>Another excellent New Yorker article, this one about Jeff Dean and his colleagues. It highlights Dean’s brilliance and how Google’s ability to scale was its true strength. I wonder if that still holds true today. The close intellectual synchronization between Jeff Dean and Sanjay Ghemawat, developed through years of collaboration, is particularly intriguing.</li> </ul> <p><strong><a href="https://nicholas.carlini.com/writing/2024/how-i-use-ai.html#background">How I Use AI - Nicholas Carlini</a></strong></p> <ul> <li>An insightful piece by a DeepMind researcher on how he utilizes AI. His approach aligns with my own and what I advocate for.</li> </ul>]]></content><author><name></name></author><category term="link"/><summary type="html"><![CDATA[A collection of articles and videos that I read in 2024]]></summary></entry><entry><title type="html">How to Read This Blog</title><link href="https://ht0324.github.io/blog/2024/readme/" rel="alternate" type="text/html" title="How to Read This Blog"/><published>2024-12-15T16:40:16+00:00</published><updated>2024-12-15T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/readme</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/readme/"><![CDATA[<p>Welcome to my garden! This is a short post about how to navigate my blog, as there’s a lot going on. I post about a variety of topics and categories, so I’ve used hashtags and tags to separate posts by topic. Here’s a breakdown:</p> <ul> <li>Hashtags: These are used to categorize the various topics I explore.</li> <li><a href="https://ht0324.github.io/blog/category/blog">Blog</a>: This tag mainly contains polished writing. These are posts where I have clearly articulated my thoughts and spent time proofreading to ensure they are proper blog posts.</li> <li><a href="https://ht0324.github.io/blog/category/thoughts">Thoughts</a>: This tag contains my unfiltered thoughts, recorded in an unrestricted way. Here, I explore my ideas freely, and you will find a different kind of value and enjoyment compared to the polished blog posts.</li> <li><a href="https://ht0324.github.io/blog/category/links">Links</a>: This tag serves as a link archive. It’s an archive of articles I’ve read and videos I’ve watched that I found insightful. There are many things I want to share with you, and this tag serves as an archive.</li> <li><a href="https://ht0324.github.io/blog/category/papers">Papers</a>: This tag is used for summarizing and discussing the findings of academic papers that I’ve read.</li> </ul> <p>Based on these tags, I believe you can easily find what I’m interested in promptly and conveniently. I hope you enjoy my garden and my food for thought!</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[A short guide on how to navigate my blog]]></summary></entry><entry><title type="html">Generation vs. Verification: Epiphany after o1</title><link href="https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1/" rel="alternate" type="text/html" title="Generation vs. Verification: Epiphany after o1"/><published>2024-09-19T06:22:57+00:00</published><updated>2024-09-19T06:22:57+00:00</updated><id>https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">From OpenAI to Anthropic Advocate</title><link href="https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate/" rel="alternate" type="text/html" title="From OpenAI to Anthropic Advocate"/><published>2024-08-29T06:41:03+00:00</published><updated>2024-08-29T06:41:03+00:00</updated><id>https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Framing RLHF as Generation vs Verification</title><link href="https://ht0324.github.io/blog/2024/framing-rlhf-as-generation-vs-verification/" rel="alternate" type="text/html" title="Framing RLHF as Generation vs Verification"/><published>2024-08-08T10:40:45+00:00</published><updated>2024-08-08T10:40:45+00:00</updated><id>https://ht0324.github.io/blog/2024/framing-rlhf-as-generation-vs-verification</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/framing-rlhf-as-generation-vs-verification/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Reflections on FSD and GPT-2</title><link href="https://ht0324.github.io/blog/2024/reflections-on-fsd-and-gpt-2/" rel="alternate" type="text/html" title="Reflections on FSD and GPT-2"/><published>2024-07-12T06:54:50+00:00</published><updated>2024-07-12T06:54:50+00:00</updated><id>https://ht0324.github.io/blog/2024/reflections-on-fsd-and-gpt-2</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/reflections-on-fsd-and-gpt-2/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Data Quality in AI: Musings on LLMs and Synthetic Data</title><link href="https://ht0324.github.io/blog/2024/data-quality-in-ai-musings-on-llms-and-synthetic-data/" rel="alternate" type="text/html" title="Data Quality in AI: Musings on LLMs and Synthetic Data"/><published>2024-07-03T13:46:56+00:00</published><updated>2024-07-03T13:46:56+00:00</updated><id>https://ht0324.github.io/blog/2024/data-quality-in-ai-musings-on-llms-and-synthetic-data</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/data-quality-in-ai-musings-on-llms-and-synthetic-data/"><![CDATA[]]></content><author><name></name></author></entry></feed>