<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-02T06:34:58+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Theory on Blade Runner 2049</title><link href="https://ht0324.github.io/blog/2025/Blade-Runner-2049/" rel="alternate" type="text/html" title="A Theory on Blade Runner 2049"/><published>2025-04-02T01:30:00+00:00</published><updated>2025-04-02T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Blade-Runner-2049</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Blade-Runner-2049/"><![CDATA[<p>Blade Runner 2049 is one of my all-time favorite movies. After watching Villeneuve’s "Arrival," I found myself drawn to his work, and this film only deepened my admiration. The cinematography, acting, atmosphere, and Hans Zimmer’s score are all genuinely top-notch, working together to create an unforgettable experience that continues to captivate me years after my first viewing.</p> <p>I’m not here to break down these technical aspects—plenty of critics have done that already with far more expertise. Instead, I want to share a theory that’s been brewing in my mind: what if the plot isn’t exactly as it appears on the surface?</p> <p><em>Warning: HUGE spoilers ahead. I really recommend watching the movie. It’s genuinely good.</em></p> <hr/> <h3 id="a-brief-recap">A Brief Recap</h3> <p>Blade Runner 2049 follows Officer K, a replicant blade runner tasked with "retiring" older replicants. During a routine job, he uncovers evidence suggesting that a replicant has given birth naturally—a revelation that threatens the established order. As K investigates, he’s drawn deeper into a web involving Niander Wallace, who seeks to control replicant reproduction, and Ana Stelline, a seemingly innocent memory maker isolated from the world due to illness.</p> <h3 id="the-apparent-power-structure">The Apparent Power Structure</h3> <p>On the surface, the power dynamics seem clear. K and Joi are our protagonists, while Niander Wallace serves as the antagonist—a god-like figure who, interestingly, never directly interacts with K despite his enormous presence in the story.</p> <p>But what if Wallace isn’t the real antagonist? What if the true antagonist is Ana Stelline, the memory maker?</p> <h3 id="the-nature-of-replicants-vs-humans">The Nature of Replicants vs Humans</h3> <p>To understand this theory, we need to examine what makes replicants distinct from humans in this universe. Physically, replicants are superior—stronger, faster, and more resilient. Their base intelligence is remarkably high, as we see when K navigates complex systems and archives with ease.</p> <p>Take K, for example. He navigates vast DNA archives with only the symbols ATGC, showing pattern-matching abilities that surpass humans. Beyond his intellectual capabilities, K also fights with striking efficiency, his punches are fast and calculated, with no wasted motion. When he fires a weapon, he never misses his target, hitting the bullseye with almost robotic accuracy. This blend of mental and physical efficiency underscores his engineered perfection.</p> <p>But the fundamental difference between replicants and humans lies in purpose. As Heidegger might suggest, humans are thrown into the world without inherent meaning. This absence of predetermined purpose defines the human condition. We’re forced to create our own meaning, to discover our own paths—a freedom that is fundamentally human.</p> <p>Replicants, in contrast, are manufactured with clear objectives. They exist as tools from the moment of their creation—soldiers, pleasure models, or laborers. Their existence is fundamentally instrumental. While their minds are quick, they lack the fundamental freedom to determine their own purpose.</p> <p>This is why Ryan Gosling’s performance is so brilliant. His reserved nature and subtle expressions portray a grown-up baby—a physically mature being who lacks social experience. He’s rigid and tense because he’s navigating a world without the emotional maturity that comes from growing up human.</p> <p>And this is precisely why Rachael’s child changes everything. A replicant born naturally arrives without engineered purpose—no predetermined function. This natural birth places them in that uniquely human position of having to discover their own meaning, making them indistinguishable from humans in that crucial aspect.</p> <h3 id="the-power-of-memory">The Power of Memory</h3> <p>In the Blade Runner universe, memory is a central theme. Both films explore how memories shape identity, with the original posing questions about implanted memories and the sequel expanding on this concept.</p> <p>What is a human without memories? If memory is a core function of our identity and an integral part of who we are, then the manipulation of memory equals the manipulation of the person. In this light, Ana Stelline’s position becomes staggeringly significant. She doesn’t just create false pasts—she shapes the very identities of replicants by designing their memories.</p> <p>The film portrays Ana as benign and relatively powerless, but in a world where replicants are physically stronger than humans, she might actually wield the darkest power of all—the ability to control replicants from within by manipulating the very foundation of their sense of self.</p> <h3 id="k-as-a-pawn">K as a Pawn</h3> <p>This leads to my theory, which admittedly is somewhat radical: Ana Stelline directly influences replicants by crafting memories implanted at inception, subtly guiding their consciousness. This manipulation is apparent due to the resistance movement stirred up by other replicants, a movement made possible because Ana carefully places the seed of her memories into each and every replicant at their creation, allowing those seeds to grow.</p> <p>In this reading, K serves as a pawn in multiple games—Wallace’s disposable workforce and Ana’s controlled agent. The film presents K’s decision to save Deckard and reunite him with Ana as a triumph of free will, a humanist moment where K chooses not to be a tool.</p> <p>But are we certain this was K’s true motivation? His memories—the very foundation of his identity and decisions—are products of Ana’s work. If she is indeed a grand mastermind, is it possible that K is just a sophisticated puppet designed to bring Deckard to her? How much of his apparent free will is actually the result of carefully crafted memories implanted to guide him toward Ana’s desired outcome?</p> <p>The replicant’s memories are their identities. By controlling memories, Ana potentially controls the replicants themselves—including K—making her the true puppet master behind the scenes.</p> <p>Consider what replicant reproduction means in this power dynamic. If replicants can reproduce naturally, whose power diminishes—Wallace’s or Ana’s? I would argue Ana’s influence wanes significantly. If Ana can create and alter memories of manufactured replicants, she can essentially use them as tools for her own purposes. But naturally born replicants would have authentic, lived experiences of growing up—memories that Ana didn’t craft. They would develop identities beyond her control.</p> <p>Yet what does K actually accomplish? While Wallace seeks to give replicants reproductive capabilities—ironically erasing the philosophical distinctions between humans and replicants—K’s actions ultimately serve Ana’s interests. He prevents Deckard from being captured by Wallace and instead delivers him directly to Ana. This is the fascinating contradiction: Wallace’s quest to blur the line between replicants and humans might actually be countered by Ana, who maintains her power by keeping that distinction intact through memory manipulation.</p> <h3 id="a-darker-interpretation">A Darker Interpretation</h3> <p>This interpretation casts the seemingly benevolent memory maker in a more sinister light. If Ana’s power relies on controlling replicants through fabricated memories, then natural replicant reproduction represents a profound threat to her influence. Through this lens, her apparent helplessness and isolation might be a carefully constructed facade concealing a deeper agenda.</p> <p>I strongly believe this wasn’t Villeneuve’s intended reading of the film. The surface narrative should probably be taken at face value. However, when we deeply examine how memory functions as identity for replicants and follow that thread to its philosophical conclusion, it raises unsettling questions about Ana’s true nature and motivation.</p> <p>Is Ana truly benign? Given her unprecedented power to shape and insert memories—essentially programming consciousness itself—can we be certain of her intentions? If she can manipulate replicants to pursue her goals while believing they’re exercising free will, how would we (or they) ever know?</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>This isn’t strict analysis—it’s more of a fan theory (perhaps a bit deranged, I admit). But it’s food for thought nonetheless.</p> <p>I still love this movie immensely. The cinematography, lighting, sound design, soundtrack, narrative structure, and performances are all exceptional. There’s always more to unpack in Blade Runner 2049, which is precisely what makes it such a lasting masterpiece.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[A fan theory]]></summary></entry><entry><title type="html">Everything Everywhere All at Once - Review</title><link href="https://ht0324.github.io/blog/2025/EEAAO/" rel="alternate" type="text/html" title="Everything Everywhere All at Once - Review"/><published>2025-04-01T10:00:00+00:00</published><updated>2025-04-01T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/EEAAO</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/EEAAO/"><![CDATA[<p>It’s been a while since I watched <em>Everything Everywhere All at Once</em>, but the film left such a strong impression that I’ve wanted to put my thoughts into writing. My initial reaction was simply being mind-blown. The title perfectly encapsulates the experience – it truly feels like you’re witnessing everything, everywhere, all at once.</p> <p>The movie masterfully blends a multitude of genres, even incorporating animation. While it touches on many styles, I primarily viewed it through a science fiction lens. The concepts of parallel universes and the multiverse aren’t just metaphorical devices here; they form the logical backbone of the narrative. If you accept the premise that every choice creates a new universe, the film’s seemingly chaotic structure starts to make a fascinating kind of sense. Taking this idea to its extreme naturally leads to the “everything everywhere” scenario.</p> <h3 id="a-universe-within-a-universe-the-frame-narrative">A Universe Within a Universe: The Frame Narrative</h3> <p>One of the most intriguing aspects is the film’s nested structure – a frame within a frame narrative. There’s a critical point mid-way through, after Evelyn has gained immense power but is overwhelmed and collapses. Right then, credits roll on screen within the movie’s world. This isn’t just a stylistic break; this is where we realize the structure. The credits explicitly state that the narrative segment we had just watched was itself a film, directed by the successful, movie-star version of Evelyn Wang from her parallel universe.</p> <p>This reveal fundamentally shifts our perspective. What we perceived as the “main” story up to that point was, in fact, one universe’s artistic interpretation of its own multiverse events, presented as a film within that reality. When the action resumes after these credits, we are entering a different narrative layer or returning to a different parallel thread than the one where the “movie-star Evelyn movie” concluded. This structural trick itself serves as a meta-commentary on the infinite possibilities the film explores.</p> <h3 id="parallel-timelines-rippling-effects-and-audience-verse-jumping">Parallel Timelines, Rippling Effects, and Audience Verse-Jumping</h3> <p>The film’s editing constantly cuts between different universes – the main laundromat reality, the movie star universe, the rock universe, the one with sausage fingers, and so many more. This rapid-fire style visually reinforces a core concept: all these universes exist and unfold simultaneously, with synchronized timelines. For the audience, these quick cuts effectively simulate the experience of verse-jumping right alongside Evelyn, plunging us into the dizzying simultaneity she experiences. The movie effectively shows us <em>how</em> to experience everything, everywhere, all at once.</p> <p>While usually separate, the intense power and central nature of Evelyn and her daughter Joy / Jobu Tupaki cause these realities to bleed into one another. What’s fascinating is how their emotional states create ripples across these parallel existences. When Evelyn succumbs to the nihilism of the “Everything Bagel,” darkness pervades her associated universes. Conversely, when her husband Waymond champions kindness and compassion, that positive influence uplifts those same realities. These effects aren’t sequential; they happen <em>all at once</em>, reinforcing the title’s literal meaning.</p> <h3 id="infinite-possibilities-and-the-core-message">Infinite Possibilities and The Core Message</h3> <p>The film eventually concludes, offering a sense of resolution as the Wang family reconciles and returns to the IRS building. However, given the frame narrative revealed earlier, this ending universe feels distinct from the one where the story began (or the one where the movie-star Evelyn’s film ended). We’ve journeyed through layers of reality, landing in a different parallel outcome than the initial setup. This reinforces the idea that we, the audience, experienced just one path through an infinite multiverse.</p> <p>This structure allows for mind-bending implications. The story focuses on realities where Evelyn and Joy are pivotal. But with infinite possibilities, wouldn’t there be universes where Waymond, Deirdre, Gong Gong, or some random background character holds the key? The scope truly feels limitless.</p> <p>What I appreciate most about <em>Everything Everywhere All at Once</em> is this duality. The intricate science fiction framework holds up remarkably well, yet it serves a deeply humanitarian core. Ultimately, the story is about love, acceptance, generational trauma, and the power of kindness in the face of overwhelming absurdity. The sci-fi elements amplify the emotional stakes rather than overshadowing them.</p> <p>The acting is also phenomenal, particularly Ke Huy Quan’s portrayal of Waymond. His heartfelt plea for kindness is incredibly moving and serves as the film’s emotional anchor. The title is a precise description of the film’s structure, themes, and the overwhelming feeling it evokes. It’s a film that warrants multiple viewings to unpack its many layers.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[Thoughts on the multiverse]]></summary></entry><entry><title type="html">Vibe Coding</title><link href="https://ht0324.github.io/blog/2025/vibe-coding/" rel="alternate" type="text/html" title="Vibe Coding"/><published>2025-03-30T11:00:00+00:00</published><updated>2025-03-30T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/vibe-coding</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/vibe-coding/"><![CDATA[<p><em>Note: The following post is written based on a presentation I gave at SKKAI on March 29, 2025.</em></p> <h3 id="what-is-vibe-coding">What is “Vibe Coding”?</h3> <p>The term “Vibe Coding” recently caught some buzz, sparked by a <a href="https://x.com/karpathy/status/1886192184808149383">tweet by Andrej Karpathy</a> back in February 2025. He described it as a new way of coding where you “fully give in to the vibes, embrace exponentials, and forget that the code even exists.” This involves using powerful LLMs within tools like <a href="https://www.cursor.com/">Cursor</a>, often interacting through voice commands and barely touching the keyboard.</p> <p>Karpathy explained the process: making simple requests, accepting code changes without much review (“Accept All”), and just feeding error messages back to the LLM, which usually fixes them. He mentioned the code can grow beyond easy comprehension, and bugs might get worked around rather than deeply debugged. It’s less traditional coding and more directing, running, and assembling – letting the LLM handle the nitty-gritty implementation.</p> <p>Since Karpathy is a co-founder of OpenAI and former head of Tesla’s Autopilot vision, this idea is worth paying attention to, especially as the term gains traction in tech circles. The main idea is humans set high-level goals, and the LLM does the detailed work. The workflow shifts to observing, directing, and integrating the AI’s output, often without needing deep code understanding yourself.</p> <h3 id="from-intelligence-to-agency-making-vibe-coding-possible">From Intelligence to Agency: Making Vibe Coding Possible</h3> <p>A key question is: Is this Vibe Coding thing actually feasible? Based on recent progress, the answer is increasingly yes, though with some catches. This wasn’t really practical just a few months ago. The big change? The rise of LLM <em>Agency</em>.</p> <p>To get what Vibe Coding is about, it helps to see the difference between LLM Intelligence and LLM Agency. Intelligence is about knowledge, reasoning, and understanding – areas where LLMs are improving incredibly fast. As I wrote before, trying out benchmarks like MMLU shows that modern LLMs can already have superhuman knowledge in certain areas. But, this intelligence often stays inside a ‘box,’ like a chat window, limited in how it interacts with the outside world.</p> <p>Agency, however, is about taking action, having initiative, and controlling things in an environment. It’s the LLM’s power to <em>do</em> tasks and make decisions that affect the real or digital world, going beyond just talking. This is where standard LLM use often stops. For LLMs to really help with complex tasks like coding, they need the agency to act for us.</p> <h3 id="building-agentic-llms-better-models-and-tools">Building Agentic LLMs: Better Models and Tools</h3> <p>Developing this agency is key for Vibe Coding. It means LLMs need to use tools, change files, run code, and interact with systems—not just generate text. This ability to <em>execute</em> tasks based on instructions is the foundation. So, boosting agency by combining reasoning with action has become a major focus in LLM development.</p> <p>How do we give LLMs more agency? There seem to be two main routes. First, build smarter models designed for agency. This means training them on data showing agentic behavior, using methods like Reinforcement Learning (RL), and designing models focused on task execution, not just language. We’re also seeing evaluation shift, with benchmarks like SWE-bench testing practical coding ability, which needs real agency. Anthropic showing <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a> <a href="https://www.anthropic.com/news/visible-extended-thinking">playing Pokémon</a> effectively highlights this focus on action and decision-making.</p> <p>The second route is giving LLMs effective Tools and ways to interact with the world. An LLM’s smarts are useless if it can’t act. Tools are the bridges connecting the LLM to APIs, databases, file systems, web searches, etc. This needs interfaces allowing the model to reliably get info, run code, and change its environment—turning ‘thinking’ into ‘doing’.</p> <h3 id="standardizing-interaction-the-model-context-protocol-mcp">Standardizing Interaction: The Model Context Protocol (MCP)</h3> <p>Integrating tools used to be a headache. Developers had to manually specify exactly how an LLM should use each API or system—a messy, non-standard process.</p> <p>To fix this, the <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol (MCP)</a> was introduced. First proposed by Anthropic and now also adopted by OpenAI, MCP is an open standard aiming to simplify how AI models connect with external tools and data. Think of it as a “USB-C for AI applications.”</p> <p>MCP works by creating a standard communication layer. Model providers (like OpenAI, Anthropic) make their LLMs MCP-compatible. Tool providers (like GitHub, Slack, or even custom local tools) make their tools speak the MCP language. This lets models use any MCP tool via a single interface, and tools become usable by any compatible model easily. The flow is typically: LLM decides tool needed -&gt; sends MCP request -&gt; tool executes -&gt; returns MCP result -&gt; LLM proceeds. This helps build a stable, consistent ecosystem of AI tools.</p> <p>It’s no accident that the places where Vibe Coding is starting to happen—like the <a href="https://www.cursor.com/">Cursor</a> editor or the <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a> interface—are heavily using MCP. These tools give LLMs the agency they need by providing controlled access to crucial tools (file system, terminal, web search) through MCP. The formula seems to be: [Smart Model + Effective Tools (via MCP)] = Agentic Capability → Vibe Coding.</p> <h3 id="limitations-and-the-road-ahead">Limitations and The Road Ahead</h3> <p>Of course, Vibe Coding isn’t perfect right now.</p> <ul> <li><strong>Supervision Needed:</strong> You still need to watch closely. LLMs can make mistakes or get stuck.</li> <li><strong>Scalability:</strong> It works better for smaller projects. Complexity can become an issue.</li> <li><strong>Understanding:</strong> Relying only on the LLM can mean you don’t understand your own codebase well (black box risk).</li> <li><strong>Maturity:</strong> The tech is new and not ready for everything, especially critical systems.</li> <li><strong>Debugging:</strong> Fixing bugs in code you didn’t write and barely understand is hard.</li> </ul> <p>But, it’s important to remember this is likely just the beginning. The tech is improving fast. LLM reasoning and agency are getting better, and standards like MCP are making tool integration easier. While “Vibe Coding” might feel experimental now, the core trend—LLMs becoming capable agents that can handle complex tasks—is real and picking up speed. It’s definitely a space worth watching.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Blog version of my presentation]]></summary></entry><entry><title type="html">On Ephemerality</title><link href="https://ht0324.github.io/blog/2025/ephemeral/" rel="alternate" type="text/html" title="On Ephemerality"/><published>2025-03-29T01:30:00+00:00</published><updated>2025-03-29T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/ephemeral</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/ephemeral/"><![CDATA[<p>There is a word I’ve known for years but only recently begun to truly feel — <em>ephemeral</em>. It doesn’t just mean short-lived. It carries a weight. A kind of soft sadness. A beauty sharpened by its fragility. Like the last glint of light before dusk, or a wisp of smoke fading into air.</p> <p>My body is ephemeral. My thoughts, too. Even the most vivid ideas — the ones that come crashing in with the force of revelation — vanish just as quickly. If I don’t catch them, write them, they’re gone. Dissolved into the noise. Like heat bleeding into the atmosphere. Like tears in rain.</p> <p>I’ve come to accept this: nothing I am is permanent. My mind, my memories, my form — all of it passes. But writing gives me a way to resist that erosion. To store fragments of myself. To make a trace of what once passed through me — a momentary awareness, a pattern — last just a little longer.</p> <p>We talk about identity as if it were fixed, as if there were a stable “me” behind it all. But bodies change. Cells die. Even memories fade. What carries through? What persists?</p> <p>I think it’s the pattern. The software. Not in the digital sense, but in the shape of thought, of idea. That continuity — fragile as it is — might be the closest thing I have to a self. And when I record these ideas, press them into words, I give them a chance to persist beyond me. A text is not eternal, but it weathers time better than I can.</p> <p>Each time I publish a thought — when I press upload, and it floats out onto the web like a paper boat on dark water — I feel I’ve made a small move against oblivion. A piece of myself has slipped the bounds of my body. It may be read. It may be remembered. And you, dear reader, might carry a part of me with you. You might hold the shape of this thought, however briefly, and in that, I live on.</p> <p>And not all readers are human. Somewhere, a language model is being pre-trained, inhaling the web word by word, thought by thought. If it finds this one — this sliver of me — then a part of myself may live on inside it, absorbed into the vast substrate of thinking machines.</p> <p>This is how I try to stretch the lifespan of a thought. To let a piece of dust persist. To make the ephemeral flicker, just for a moment longer.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[An experimental post]]></summary></entry><entry><title type="html">Thoughts on GPT-4o Image Generation</title><link href="https://ht0324.github.io/blog/2025/gpt4-image-generation/" rel="alternate" type="text/html" title="Thoughts on GPT-4o Image Generation"/><published>2025-03-26T01:30:00+00:00</published><updated>2025-03-26T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/gpt4-image-generation</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/gpt4-image-generation/"><![CDATA[<p>Today, OpenAI revealed GPT-4o’s <a href="https://openai.com/index/introducing-4o-image-generation/">image generation capabilities</a>. While this feature was previewed in the initial 4o announcement about <a href="https://openai.com/index/hello-gpt-4o/">a year ago</a>, the actual results are still surprising. Playing with it triggered some realizations about the future implications of direct image generation that I want to share here.</p> <hr/> <h3 id="the-multimodal-approach">The Multimodal Approach</h3> <p>The concept isn’t entirely new. When OpenAI introduced GPT-4o, they described it as modeling input as text, pixels, and sounds - combining all modalities with one big autoregressive transformer. The output would likewise be text, images, and audio.</p> <p>They presented it as straightforward, though obviously there are complicated architectural decisions behind the scenes. But the impact is clear: the model is much more capable at image generation.</p> <p>In previous systems (diffusion models or other chatbot interfaces), images were generated through a two-step process. The language model would generate a sophisticated prompt and feed that into a separate image model. GPT-4o eliminates this bottleneck by directly generating images.</p> <p>Since the large language model directly generates images, it’s much more intelligent in a way. We can also feed in images, and the results speak for themselves. The images show much more consistency. When previous chatbots wanted to modify an image, they had to create a detailed description of that image and feed it into another diffusion model. This was a bottlenecked process, but with 4o, it can handle images directly, modifying them with surprising consistency.</p> <p>What’s particularly impressive is how refined and enhanced the model’s text generation capabilities are within images. Previous diffusion models really struggled with generating clean, readable text in images - it was often distorted, nonsensical, or limited to just a few words. But GPT-4o can generate very clean text, and lots of it. The text is consistently readable and contextually appropriate.</p> <hr/> <h3 id="spark-of-software-20">Spark of Software 2.0</h3> <p>My most visceral moment came from an example where they showed a cat image being iteratively transformed into a game interface. Through multiple iterations, they turned it into an image of a game with a UI, and all the text was remarkably accurate. The model produced an image with a very nice interface, and everything was consistent.</p> <p>That’s when it hit me: if the model can generate UI and text so accurately, in the future our computer interfaces could be entirely AI-generated in real-time with all the context available from the user. Large language models could generate your computer interface frame by frame based on your input and feedback.</p> <p>Imagine a user interface that actually warps and changes based on user needs. Our current operating systems (macOS, Linux, Windows) are all rule-based with fixed definitions. But what if a large language model generated a new UI that helped the user get things done? It would be totally adaptive and different for every user - changing styles and functionality based on preferences or context.</p> <p>What would a word processor look like in that interface? The possibilities seem endless. Currently, an OS has the lower-level kernel with renderers and shaders that produce pixels. But this would be an end-to-end network - a large language model OS directly generating pixels from our input.</p> <p>The concept of world simulators isn’t entirely new - NVIDIA is already using their <a href="https://blogs.nvidia.com/blog/what-is-robotics-simulation/">Isaac Sim platform</a> to <a href="https://blogs.nvidia.com/blog/openusd-sdg-advance-robot-learning/">generate synthetic data for training robot models</a>, Google DeepMind has developed <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a> that can generate interactive 3D environments, and Microsoft Research recently unveiled <a href="https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/">Muse</a>, their first generative AI model designed for gameplay ideation. But what if instead of simulating physical worlds, we used these capabilities to simulate an operating system? That’s where my realization really hit me.</p> <p>One of the most compelling aspects of this approach would be how it leverages context. Context is incredibly important for large language models - your previous conversations, actions, and preferences inform their responses. Current operating systems gather tons of contextual information about how we use them, but this data isn’t being utilized well. What if an OS could learn and adapt from your previous interactions, inputs, preferences, and habits while you used it? Every aspect of your computing experience could be customized not by explicit settings, but by the system understanding you over time.</p> <p>This is basically <a href="https://karpathy.medium.com/software-2-0-a64152b37c35">Software 2.0</a> and <a href="https://x.com/karpathy/status/1723140519554105733">LLM OS</a> that Andrej Karpathy described. I was aware of this idea before, but never I have felt that it would be feasible in such a short time. I think with some effort, this kind of OS might be technically possible right now with large-scale servers and APIs, but it would be largely impractical. Yet the implications are huge, and I believe this kind of OS is inevitable.</p> <p>Of course, it wouldn’t be a fully end-to-end neural network. There would certainly be some rule-based systems guiding the LLMs. But my point still stands - we might be seeing the first glimpse of a new OS approach.</p> <p>Karpathy mainly discussed Software 2.0 replacing rule-based software stacks, which is already happening. But I think Software 2.0 will also eventually replace the OS stack itself.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[What GPT-4o's image capabilities tell us about the future of operating systems]]></summary></entry><entry><title type="html">Llama 3 Paper - Review (Part 2)</title><link href="https://ht0324.github.io/blog/2025/Llama3-part2/" rel="alternate" type="text/html" title="Llama 3 Paper - Review (Part 2)"/><published>2025-03-25T21:00:00+00:00</published><updated>2025-03-25T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Llama3-part2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Llama3-part2/"><![CDATA[<p>This is part 2 of my review of Meta’s Llama 3 technical paper. In <a href="/blog/2025/Llama3-part1">part 1</a>, I covered the core language model architecture, training methodology, and overall performance. Now I’ll dive into the multimodal aspects of the model (vision, video, and speech), which represent significant additions to the Llama ecosystem.</p> <p>What strikes me about Meta’s approach is their consistent focus on compositionality. Instead of training entirely new models from scratch, they extend the existing Llama 3 language models with specialized adapters. This pragmatic approach allows them to add new capabilities while preserving the existing text understanding abilities.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Compositional Multimodal Architecture</strong><br/> Meta adopts a modular, compositional approach for all multimodal capabilities in Llama 3. Rather than training joint models from scratch, they combine pre-trained language models with modality-specific encoders connected through adapter layers. This architecture has several advantages: it enables parallel development of language and vision/audio capabilities, avoids the complexities of joint training on multiple modalities, preserves text-only performance, and reduces computational overhead during inference by processing input modalities efficiently.</p> <p><strong>Vision Encoder and Adapter</strong><br/> The vision module consists of a pre-trained ViT-H/14 image encoder (modified to include 850M parameters) combined with cross-attention layers that connect the visual representations to the language model. These cross-attention layers are substantial, adding about 100B parameters to the 405B model. To preserve fine-grained visual information, they extract features from multiple intermediate layers of the vision encoder rather than just using the final layer output, which helps with tasks requiring detailed localization.</p> <p><strong>Video Recognition Architecture</strong><br/> The video module builds on the image module by adding two key components: a “temporal aggregator” that merges frames to capture temporal relationships, and dedicated video cross-attention layers. The aggregator uses a perceiver resampler architecture to compress multiple frames into a more compact representation. During pre-training, they start with 16 frames (aggregated to 1) and scale up to 64 frames during fine-tuning to handle longer videos more effectively.</p> <p><strong>Speech Understanding Approach</strong><br/> Unlike the vision module, the speech component doesn’t use cross-attention layers. Instead, it generates embeddings that directly integrate with text tokens in the language model. The speech module consists of a 1B-parameter Conformer encoder followed by a smaller adapter. Interestingly, this direct integration approach allows the speech interface to leverage the language model’s existing capabilities without modifying its parameters, which seems to work remarkably well at larger scales.</p> <p><strong>Speech Generation</strong><br/> For text-to-speech capabilities, Meta takes a different approach. Rather than fine-tuning the language model for speech generation, they implement a streaming text-to-speech system that uses Llama 3 embeddings to enhance text normalization and prosody modeling. The embeddings from Llama 3 are used to improve context-aware text normalization and more natural-sounding prosody.</p> <p><strong>Scaling and Training Challenges</strong><br/> Training these multimodal adapters introduces unique challenges beyond those faced when training the core language model. The model computation becomes heterogeneous (some tokens require more processing than others), data shows high variance in token counts across modalities, and there are numerical instability issues from combining different types of representations. Meta addresses these through clever pipeline design, sequence parallelization, and using higher precision for gradient accumulation.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Compositional Design Makes Technical Sense</strong><br/> Initially, I had concerns about the compositional approach. I wondered if mapping the higher-dimensional image modality into the latent space of a language model might cause significant information loss. While the dimensionality itself is an implementation detail, I believe the inherent modality of text fundamentally contains less information than images. However, when I saw GPT-4o generating remarkably accurate images from text prompts and handling complex visual tasks, it became clear that language model latent spaces are surprisingly robust at encoding visual concepts. This suggests the limitation I was worried about may not be as severe in practice. The cross-attention mechanism with multi-layer feature extraction appears to be particularly effective at preserving the detailed information from higher-dimensional modalities.</p> <p><strong>Multi-layer Feature Extraction Preserves Fine-grained Information</strong><br/> One insight I found particularly interesting was how they addressed the problem of CLIP-like models failing to preserve fine-grained localization information. Instead of relying solely on the final layer output, they extract features from multiple intermediate layers of the vision encoder (specifically the 4th, 8th, 16th, 24th, and 31st layers). This approach makes sense because the lower layers retain more spatial and detailed information before it gets abstracted away in higher layers. I hadn’t previously considered this limitation of contrastive learning approaches, but it explains why models like CLIP might struggle with tasks requiring precise visual details or localization.</p> <p><strong>Handling Many-shot Jailbreaking in Long Context Models</strong><br/> Something that caught my attention was the vulnerability of long-context models to many-shot jailbreaking attacks. It’s fascinating how the longer context window enables a new attack vector - they mentioned specifically that 256-shot attacks become possible. What’s impressive is how they mitigated this by fine-tuning models on datasets that include examples of safe behavior even when unsafe behavior appears in context. The fact that they could achieve this without impacting false refusal rates or helpfulness metrics shows the model’s ability to distinguish between demonstrations in context and actual instructions.</p> <p><strong>Safety Becomes Increasingly Granular</strong><br/> What struck me about Meta’s safety approach is how granular it’s becoming. Rather than having a generic “harmful content” classifier, they’re developing increasingly specialized safety mechanisms for specific types of risks. This categorization of safety concerns (into areas like cybersecurity, coding, spear-phishing, etc.) reveals how the frontier of AI safety is evolving from broad mitigation to very specific risk assessment and targeted interventions. It also suggests that as models get more capable, the attack vectors multiply, requiring more complex safety strategies.</p> <p><strong>Contextual Safety Challenges</strong><br/> The paper mentions that 256-shot attacks become possible with longer context windows, which I found fascinating. It shows that simply extending a model’s capabilities (like context length) can introduce entirely new safety vulnerabilities that weren’t relevant before. This suggests a kind of “safety debt” that comes with each capability enhancement - each new ability potentially opens up novel attack vectors that need additional mitigations.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Meta’s approach to expanding Llama 3’s capabilities into multimodal territory shows a thoughtful balance between pragmatism and innovation. Rather than developing entirely new architectures or joint pre-training approaches, they extend the existing language model through specialized adapters and compositional design. This strategy allows them to leverage the strengths of existing pre-trained components while adding new capabilities incrementally.</p> <p>The paper also highlights the ongoing tension between capability enhancement and safety. As models gain new abilities (like longer context), new vulnerabilities emerge that require additional mitigations. This suggests that safety work isn’t a one-time effort but an ongoing process that must evolve alongside model capabilities.</p> <p>As these models continue to develop, I’m particularly interested in seeing how the compositional approach scales to even more modalities and how it affects overall model capabilities. Does adding more modalities lead to emergent abilities through cross-modal transfer? Do certain modalities complement each other in unexpected ways? The Llama 3 paper doesn’t directly address these questions, but it provides a solid foundation for exploring them in future work.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Continuing my analysis of Llama 3]]></summary></entry><entry><title type="html">Llama 3 Paper - Review (Part 1)</title><link href="https://ht0324.github.io/blog/2025/Llama3-part1/" rel="alternate" type="text/html" title="Llama 3 Paper - Review (Part 1)"/><published>2025-03-18T20:30:00+00:00</published><updated>2025-03-18T20:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Llama3-part1</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Llama3-part1/"><![CDATA[<p>Today I’m reviewing Meta’s Llama 3 technical paper. Due to the length and depth of the paper, I’ll be splitting this review into two parts - this is part 1, focusing on the pre-training and infrastructure aspects. The Llama 3 family represents a significant step up from Llama 2, with the flagship 405B parameter model performing competitively against leading models like GPT-4. What makes this paper particularly interesting is its comprehensive description of Meta’s approach to scaling, including data preparation, model training, and infrastructure challenges.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Scaling Up Pre-training</strong><br/> Llama 3 scales substantially beyond previous versions, with the flagship model using 405B parameters (compared to Llama 2’s much smaller size) and trained on approximately 15T tokens, versus 1.8T for Llama 2. The compute used for training the flagship model was nearly 50× more than Llama 2’s largest model. Most impressively, Meta made this massive scaling work with a standard dense Transformer rather than a Mixture-of-Experts (MoE) architecture.</p> <p><strong>Data Quality and Processing</strong><br/> Meta placed strong emphasis on data quality rather than just quantity. Their processing pipeline involved removing PII (personally identifiable information), applying multiple levels of deduplication (URL-level, document-level, and line-level), and using both heuristic and model-based filtering. They also used classifiers to identify and upsample high-quality code and reasoning content. For multilingual support, they implemented language-specific processing and quality filtering.</p> <p><strong>Context Length Scaling</strong><br/> Llama 3 was designed to handle context windows up to 128K tokens. Rather than training on long sequences from the beginning (which would be computationally prohibitive due to the quadratic scaling of self-attention), they used a multi-phase approach – first training on 8K contexts and then gradually increasing to 128K tokens in the final stages of pre-training over approximately 800B tokens.</p> <p><strong>Hardware and Infrastructure</strong><br/> Training at this scale required massive hardware resources and sophisticated infrastructure. The 405B model was trained on up to 16K H100 GPUs. They used a combination of tensor parallelism, pipeline parallelism, context parallelism, and data parallelism (what they call “4D parallelism”) to efficiently distribute computation. They report achieving 38-43% Model FLOPs Utilization (MFU), which is impressive at this scale.</p> <p><strong>Post-Training Alignment</strong><br/> After pre-training, the models underwent extensive post-training alignment using a combination of supervised fine-tuning (SFT), rejection sampling, and Direct Preference Optimization (DPO). One interesting note is their deliberate choice to avoid more complex reinforcement learning algorithms like PPO, which they found less stable and harder to scale.</p> <p><strong>Specialized Capabilities</strong><br/> The paper details multiple specialized capabilities added during post-training, including code generation (with execution-based feedback), multilingual performance, reasoning, tool-use, and factuality. For many of these, they developed specialized data generation pipelines, often leveraging earlier iterations of Llama 3 itself to generate training data.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Value of Simple, Stable Architectures</strong><br/> One of the most interesting choices Meta made was sticking with a dense Transformer architecture rather than using a Mixture-of-Experts approach. They explicitly state this was to “maximize training stability,” which signals that at huge scale, reliability and predictability might be more valuable than theoretical efficiency. This matches what DeepSeek researchers have also mentioned about the challenges of scaling MoE models.</p> <p><strong>Data Quality Trumps Architecture Complexity</strong><br/> The paper dedicates substantial space to discussing data curation, which suggests that data quality remains one of the most crucial factors for performance. Even with massive compute resources, Meta still invested heavily in filtering, curation, and quality assessment. Their use of multiple levels of deduplication, model-based quality filtering, and domain-specific pipelines reinforces how important data quality is to the final result.</p> <p><strong>Model-Bootstrapped Data Creation</strong><br/> The way Meta used earlier versions of Llama 3 to generate data for subsequent training iterations is fascinating. For specialized capabilities like code generation, they used a bootstrapping approach where the model itself generated samples, which were then filtered based on execution results. This self-improvement cycle – where models help train their successors – is becoming increasingly common but is still remarkable to see at this scale.</p> <p><strong>Context Parallelism for Long Sequences</strong><br/> The paper’s description of context parallelism (CP) for handling long sequences was particularly interesting. By dividing input sequences into chunks distributed across GPUs and using all-gather operations to collect key-value tensors, they managed to train on 128K context lengths without excessive memory usage. This approach differs from previous techniques I’ve seen and shows how specialized infrastructure is becoming for LLM training.</p> <p><strong>Alignment Complexity and Iteration</strong><br/> The post-training sections reveal how labor-intensive alignment still is. They performed six rounds of alignment, iteratively collecting human preferences, generating synthetic data, and fine-tuning. Each round built on the previous one, using increasingly capable models. The process involves carefully balancing data quality, variety, and complexity – and required extensive human annotation and quality control throughout.</p> <p><strong>Infrastructure Challenges at Scale</strong><br/> The sections on reliability and operational challenges highlight just how difficult training at this scale remains. During a 54-day period, they experienced 419 unexpected interruptions, with GPU issues accounting for nearly 60% of these. They also observed diurnal throughput variations of 1-2% due to environmental temperature fluctuations affecting GPU clocks. These details provide a sobering perspective on the practical challenges of pushing the boundaries of scale.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The Llama 3 paper provides valuable insights into how Meta approached training a competitive frontier model. While OpenAI and Anthropic maintain some lead with their proprietary models, Llama 3 demonstrates that with sufficient scale and careful engineering, it’s possible to build models that approach their capabilities while still releasing weights publicly.</p> <p>What’s most striking is the maturity of Meta’s approach. They’ve clearly learned lessons from previous iterations and focused on reliability, scalability, and maintainability rather than pursuing more exotic architectures. Their decision to use a dense Transformer architecture, combined with a stable and relatively simple alignment procedure, shows a preference for approaches that can be reliably scaled up.</p> <p>The level of infrastructure and pipeline engineering described in the paper is also remarkable. From their custom HTML parser to their extensive parallelism strategies to their reliability engineering, the paper makes clear that training models at this scale requires massive investment not just in raw compute, but in the systems that make that compute usable.</p> <p>Overall, this paper offers a detailed look into what it takes to build competitive models at scale. While the specific approaches may evolve, the general principles – quality data, reliable infrastructure, and careful alignment – are likely to remain important for future models as well.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[My analysis and thoughts on Llama 3]]></summary></entry><entry><title type="html">Scaling Laws Paper - Review</title><link href="https://ht0324.github.io/blog/2025/Scaling-Laws/" rel="alternate" type="text/html" title="Scaling Laws Paper - Review"/><published>2025-03-17T14:00:00+00:00</published><updated>2025-03-17T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Scaling-Laws</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Scaling-Laws/"><![CDATA[<p>I recently reviewed the paper <a href="https://arxiv.org/abs/2001.08361">“Scaling Laws for Neural Language Models”</a>, a foundational work that has significantly shaped how we think about training large language models. This paper is particularly interesting because it’s co-authored by Dario Amodei, who would later leave OpenAI to co-found Anthropic, partly based on insights from this research. While scaling laws are now taken for granted in AI research, this paper represents their origins and formal documentation.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Power-Law Scaling Relationships</strong><br/> The paper demonstrates that language model performance improves predictably as we increase model size, dataset size, and compute used for training. These relationships follow power-law patterns, meaning they show linear improvements on a log-log scale. This clean, consistent pattern holds across many orders of magnitude.</p> <p><strong>Model Size vs. Dataset Size Trade-offs</strong><br/> One of the most interesting findings is the relationship between model size and data requirements. The paper found that performance penalty depends predictably on the ratio N^0.74/D, meaning every time we increase model size by 8x, we only need to increase data by roughly 5x to maintain performance. This sublinear relationship has major implications for efficient resource allocation.</p> <p><strong>Compute-Optimal Training</strong><br/> The paper shows there’s an optimal allocation of compute between model size and training tokens. As available compute increases, the optimal strategy shifts toward training very large models on relatively modest amounts of data, stopping significantly before convergence. This challenges the conventional wisdom that models should be trained until convergence.</p> <p><strong>Sample Efficiency of Large Models</strong><br/> Larger models are significantly more sample-efficient than smaller ones, reaching the same performance levels with fewer optimization steps and data points. This suggests that scaling up model size inherently leads to better generalization and learning capabilities.</p> <p><strong>Architectural Invariance</strong><br/> Surprisingly, the specific architectural details like network width, depth, or attention heads matter much less than simply scaling up the total parameter count. Within a wide range, these details have minimal effects on the final performance compared to the overall model scale.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Compute Allocation Is Critical</strong><br/> What struck me most was how the paper provides concrete guidance on allocating precious compute resources. As AI training demands more and more resources, understanding the optimal balance between model size, data, and training time becomes increasingly important. This paper gives us quantitative relationships to guide those decisions.</p> <p><strong>“Don’t Train to Convergence” Is Surprising</strong><br/> The finding that we can achieve optimal performance by training very large models but stopping significantly short of convergence was unexpected. This challenges the traditional training approach and suggests that rapid training of oversized models might be more compute-efficient than fully training smaller ones.</p> <p><strong>The “Bitter Lesson” Vindicated</strong><br/> This paper strongly aligns with Richard Sutton’s <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">“Bitter Lesson”</a> - the idea that methods leveraging computation tend to outperform human-engineered approaches. The scaling laws empirically validate this perspective, showing that simply scaling up compute and model size leads to predictable improvements without needing clever architecture innovations.</p> <p><strong>Data Requirements Grow Slowly</strong><br/> I was relieved to see that data requirements grow much more slowly than model size in the optimal regime. If this relationship were reversed, we would face much more severe data scarcity problems. This finding suggests that model size, not data, might be the primary bottleneck for future progress.</p> <p><strong>Anthropic’s Research Approach Is Evident</strong><br/> Reading this paper, I could see early signs of what would become Anthropic’s research philosophy. The experimental approach - running many controlled experiments to discover patterns rather than starting from a hypothesis - feels similar to their later work like the Transformer Circuits series. This paper seems to contain some of Anthropic’s research DNA.</p> <p><strong>Variables Lack Inherent Meaning</strong><br/> A limitation worth noting is that the specific coefficients and exponents in the scaling laws don’t have inherent meaning - they’re empirically determined and likely depend on the specific data used. While the general form of the relationship probably generalizes, the exact values might differ across domains.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “Scaling Laws” paper provides a remarkably clear picture of how language model performance scales with model size, data, and compute. Its findings have shaped how the entire field approaches training large models, suggesting that bigger is not just better but also more efficient.</p> <p>What I appreciate most about this work is how it transforms vague intuitions into precise, quantitative relationships. By establishing these power laws, it gives us a framework for making rational decisions about resource allocation in AI training.</p> <p>The implications continue to reverberate through AI research. As we haven’t yet seen these scaling trends plateau, the guidance from this paper remains highly relevant. In many ways, the current race to build larger and more capable AI systems is a direct result of the insights this paper formalized.</p> <p>This paper truly takes the “Bitter Lesson” to heart - showing that scaling computation provides reliable returns without needing architectural breakthroughs. It’s a perspective that has proven incredibly fruitful for advancing AI capabilities.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing OpenAI's paper on scaling laws]]></summary></entry><entry><title type="html">How to Become Irreplaceable</title><link href="https://ht0324.github.io/blog/2025/Irreplacable/" rel="alternate" type="text/html" title="How to Become Irreplaceable"/><published>2025-03-14T15:00:00+00:00</published><updated>2025-03-14T15:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Irreplacable</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Irreplacable/"><![CDATA[<p>Lately, I’ve been thinking a lot about what it means to be irreplaceable—especially now, when it feels like AI is quickly changing what we consider valuable.</p> <p>Traditionally, intelligence was something people paid a premium for. If you were smart, knowledgeable, or skilled, society rewarded you. Intelligence was scarce and valuable. But today, especially after experiencing tools like ChatGPT, it feels clear that intelligence isn’t going to remain scarce for much longer.</p> <p>Just think about it—ChatGPT alone has made access to a certain level of intelligence cheaper than it’s ever been. Five years ago, you would’ve paid a significant amount to have someone tutor you, answer your questions, or explain concepts clearly. Now, anyone can access that kind of intelligence instantly, at a fraction of the cost.</p> <p>This trend isn’t slowing down. In fact, I’m pretty convinced it will accelerate. It’s <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> all over again. Large language models are becoming cheaper and more capable every year, pushing the cost of intelligence lower and lower. For someone like me, who went to university hoping that my knowledge and technical skills would set me apart, that’s a worrying thought.</p> <p>Honestly, I feel an existential anxiety about this. If intelligence itself becomes cheap, what does it mean to provide something truly valuable to society? How can I ensure my skills or knowledge remain valuable enough to earn a good living?</p> <p>The more I think about it, the clearer it becomes that intelligence alone won’t cut it. What will always be scarce—what people will always crave—isn’t intelligence, but identity. Human experiences, individuality, entertainment, personality—these are things machines can’t easily replace, no matter how intelligent they become.</p> <p>Take Taylor Swift as an example. People aren’t drawn to her simply because she’s intelligent or talented (though she is). They’re drawn to her as a person. Her identity—who she is, how she expresses herself—is something uniquely human and impossible to replicate with technology. Taylor Swift is irreplaceable not because of her skills, but because of her identity and the human connection she creates.</p> <p>That’s the kind of scarcity that holds up even as technology advances.</p> <p>So, where does that leave me? Right now, my skills and identity aren’t particularly unique. I’m not providing anything truly scarce. It’s a sobering realization, but also an important one. Maybe the key to staying valuable in this era isn’t about becoming smarter, but becoming more human—expressing something authentic, relatable, or simply enjoyable.</p> <p>I’m still figuring this out. But one thing seems clear: in a world where intelligence is increasingly abundant, identity might just become the most valuable asset we have.</p>]]></content><author><name></name></author><category term="Thought"/><category term="AI"/><category term="Life"/><summary type="html"><![CDATA[As intelligence becomes cheaper, what does it mean to be valuable?]]></summary></entry><entry><title type="html">Claude Code — Three Days In</title><link href="https://ht0324.github.io/blog/2025/Claude-Code-2/" rel="alternate" type="text/html" title="Claude Code — Three Days In"/><published>2025-03-07T12:00:00+00:00</published><updated>2025-03-07T12:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code-2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code-2/"><![CDATA[<p>It’s been three days since I started using <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>. The initial excitement was so strong that I immediately wrote a <a href="https://ht0324.github.io/blog/2025/Claude-Code/">blog post</a> about it. But after a couple more days, having spent more time exploring its capabilities, I’ve found myself with even more thoughts—so here’s a quick follow-up.</p> <hr/> <h3 id="time-saving-understated-yet-mind-blowing">Time-Saving: Understated Yet Mind-Blowing</h3> <p>I know I’ve already emphasized how much time Claude Code saves, but seriously, I can’t stress it enough. Over the past couple of days, I’ve been completely revamping my personal website from the ground up. Let me preface this by saying: I’m not really a web guy. Before Claude Code, my blog was just a clone of the <a href="https://alshedivat.github.io/al-folio/">Al Folio</a> theme. Whenever I wanted to tweak something, it felt daunting. Each tiny modification usually ended in frustration or compromise because the effort required to change something small just wasn’t worth it.</p> <p>Now, if I want to adjust the tag styling or add new functionality—boom, I just instruct Claude Code to do it. It happily handles the rest. The difference in my blog over just these past couple of days is truly night and day. I’ve implemented numerous nooks and crannies, subtle features, and design tweaks that previously felt impossible.</p> <p>Of course, Claude isn’t perfect, but the sheer amount of time it saves is staggering. To put it into perspective, yesterday I wasn’t satisfied with how the tag feature functioned, so I decided to change it up. Claude handled the entire process for about $1 in API costs. A dollar. Less than a cup of coffee. Without it, the same change would have probably taken me hours of tinkering and frustration.</p> <p>At first, Claude felt like a pair-programming partner, but even after just three days, that feeling is fading. Instead, it’s become something more like a virtual assistant—an assistant that can actually do things, not just talk about them.</p> <hr/> <h3 id="chatbots-vs-agents-what-is-vs-do-it">Chatbots vs. Agents: “What Is” vs. “Do It”</h3> <p>Using traditional chatbots like ChatGPT always felt like carefully constructing queries, specifying every little detail of my intent. Most of my interactions with ChatGPT—and likely yours too—are fundamentally knowledge retrieval: asking “What is this?” or “Tell me about that.” ChatGPT essentially compresses vast internet knowledge and provides succinct, precise answers. It’s exceptional at answering questions and explaining concepts, but it’s mostly passive—focused on understanding and summarizing existing knowledge.</p> <p>But Claude Code changed the game completely. With a truly agentic model like Claude, I don’t have to painstakingly spell out exactly how I want something done. Instead, I just specify <em>what</em> I want done, and Claude figures out the <em>how</em>. It doesn’t just summarize information; it actively plans, makes decisions, and implements changes autonomously. This shift from explicitly defined instructions (“what is”) to implicit, actionable intent (“do it”) feels genuinely profound—a step change in capability that transforms the user experience.</p> <p>Of course, because Claude isn’t perfect yet, the collaboration still exists in the form of verifying the results. But as I’ve <a href="https://medium.com/@FdForThought/framing-rlhf-as-generation-vs-verification-4d9e95b88534">said before</a> <a href="https://medium.com/@FdForThought/generation-vs-verification-epiphany-after-o1-713c6f411206">many times</a>: verification is vastly easier than generation, and right now, I’m comfortably on the verification side. I simply check Claude’s implementation by deploying my blog and testing the functionality. This still requires my input, but the mental load is drastically lighter.</p> <hr/> <h3 id="from-collaboration-to-automation">From Collaboration to Automation</h3> <p>While it still feels somewhat collaborative now, I’m realizing that my role is already shrinking from active collaborator to passive verifier. It’s an assistant relationship—not really a partnership anymore. Verification feels simpler and simpler, and honestly, I can foresee even verification becoming automated soon. At that point, human involvement becomes negligible.</p> <p>Previously, I believed in the paradigm that self-feedback loops (generation versus verification) would incrementally increase intelligence. But intelligence and agency are fundamentally different. Learning how to be smart/knowledgable and learning how to act are not the same, and the latter is far more powerful. I’m now convinced that agentic capability will lead to implications far bigger and faster than I initially anticipated.</p> <p>If (or more realistically, <em>when</em>) verification itself is automated, we won’t be collaborating with AI—we’ll be observing it collaborating with itself. That’s when things will get really interesting (and potentially unsettling).</p> <hr/> <h3 id="toward-agentic-intelligence">Toward Agentic Intelligence?</h3> <p>OpenAI previously described levels of AI intelligence: Level 2 (reasoners), Level 3 (agents), and Level 4 (collaborators or organizations). Claude Code sits loosely at Level 3, and my recent experiences hint strongly that Level 4—agents autonomously collaborating with each other—is closer than we might think.</p> <p>As these agentic models improve and increasingly handle their own verification, we’ll approach a point where human oversight becomes unnecessary. This isn’t speculation anymore; using Claude Code has convinced me that the shift toward fully autonomous agentic systems isn’t just possible—it’s imminent.</p> <p>The impact of that shift can’t be overstated. As impressive as Claude Code is today, it’s the worst this technology will ever be. And that’s both thrilling and deeply concerning.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Further reflections on Claude Code]]></summary></entry></feed>