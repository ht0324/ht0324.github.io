<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-18T11:23:25+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Scaling Laws for Neural Language Models - Review</title><link href="https://ht0324.github.io/blog/2025/Scaling-Laws/" rel="alternate" type="text/html" title="Scaling Laws for Neural Language Models - Review"/><published>2025-03-17T14:00:00+00:00</published><updated>2025-03-17T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Scaling-Laws</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Scaling-Laws/"><![CDATA[<p>Today, I’ll review <a href="https://arxiv.org/abs/2001.08361">“Scaling Laws for Neural Language Models”</a>, by OpenAI. This paper explores how language model performance scales with model size, dataset size, and compute. It introduced empirical laws to predict language model performance, guiding how we allocate resources when training models.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Empirical Scaling Laws</strong><br/> The paper describes how language model performance—measured by negative log-likelihood (loss)—scales predictably with three key variables: model size, dataset size, and the amount of compute used for training. These relationships follow a power-law form, meaning improvements become predictable as resources scale.</p> <p><strong>Optimal Resource Allocation</strong><br/> One finding is that scaling doesn’t mean increasing all resources equally. If you increase the model size by a factor of eight, the dataset only needs to increase roughly by a factor of five to avoid performance penalties. This ratio helps guide efficient allocation of resources during model training.</p> <p><strong>Sample Efficiency and Early Stopping</strong><br/> Interestingly, larger models learn faster and use fewer samples to achieve the same performance as smaller models. Another counterintuitive insight is that you don’t have to fully train large models until convergence. Stopping training early (well before convergence) still yields strong performance, saving significant compute.</p> <p><strong>Minimal Impact of Architectural Details</strong><br/> The paper also notes that specifics like model width, depth, or other architecture tweaks don’t dramatically affect the scaling laws. Instead, what primarily matters is the raw scale of parameters, data, and compute used.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Origin of the Scaling Perspective</strong><br/> Today, the idea of scaling large models feels obvious. But reading this paper reminded me how foundational this concept was when it first appeared. The findings showed that scaling wasn’t just about brute force; there was a systematic and predictable relationship guiding how we should scale. It helped solidify scaling as a strategy rather than a guess.</p> <p><strong>Efficient Use of Resources</strong><br/> The specific ratio they present (like increase model by 8x, data by 5x) stood out to me as really interesting. As models and datasets balloon, knowing exactly how to balance them is incredibly valuable. Without this clarity, the risks of wasting resources or hitting diminishing returns rise quickly. It makes resource allocation more scientific, rather than guesswork.</p> <p><strong>Stopping Before Convergence</strong><br/> I was particularly intrigued by the finding that optimal training involves stopping large models well before convergence. This idea was counterintuitive at first, since typical practice often pushes models to full convergence. It suggests we can build massive models and intentionally undertrain them to get optimal results efficiently, which feels like a significant shift in mindset.</p> <p><strong>The Anthropic Connection</strong><br/> This paper included Dario Amodei (co-founder of Anthropic), and it strongly resonated with the research style I see at Anthropic today. Their <a href="https://transformer-circuits.pub/">Transformer Circuits</a> research similarly uses numerous small experiments to draw systematic insights. Rather than traditional hypothesis-driven research, it feels more like reverse engineering. It was interesting to see the roots of Anthropic’s style here.</p> <p><strong>Data Dependency and Limitations</strong><br/> One caveat: the specific numerical relationships are inherently dependent on the particular datasets used. While the overall approach generalizes, the exact ratios aren’t universal. This isn’t really a weakness of the paper itself, but a reminder that scaling laws aren’t fixed truths; they depend on the data.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The paper introduced the scaling laws that now drive much of language model research, showing predictable, systematic relationships between model size, data, compute, and performance. It gave clear guidance on resource allocation and showed that large models generalize better, reaching optimal performance more efficiently.</p> <p>One thing I particularly appreciated was seeing Dario Amodei among the authors. Knowing that he later co-founded Anthropic after realizing the implications of these scaling results made the paper even more interesting. It connected dots between OpenAI’s original scaling research and Anthropic’s current style.</p> <p>Finally, I think the paper aligns strongly with Rich Sutton’s “Bitter Lesson,” emphasizing simple methods at greater scale over complex, engineered solutions. In that sense, it’s a concrete example showing the power of scale and simplicity in AI—exactly what the Bitter Lesson suggests. It takes the lesson at heart.</p> <p>Overall, this paper defined an essential direction in modern AI research, not just by discovering scaling laws, but by reshaping how we think about model training itself. Given how foundational scaling remains in AI today, I suspect the impact of these ideas will continue far into the future.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing OpenAI's paper on scaling laws and the role of model size, data, and compute]]></summary></entry><entry><title type="html">How to Become Irreplaceable</title><link href="https://ht0324.github.io/blog/2025/Irreplacable/" rel="alternate" type="text/html" title="How to Become Irreplaceable"/><published>2025-03-14T15:00:00+00:00</published><updated>2025-03-14T15:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Irreplacable</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Irreplacable/"><![CDATA[<p>Lately, I’ve been thinking a lot about what it means to be irreplaceable—especially now, when it feels like AI is quickly changing what we consider valuable.</p> <p>Traditionally, intelligence was something people paid a premium for. If you were smart, knowledgeable, or skilled, society rewarded you. Intelligence was scarce and valuable. But today, especially after experiencing tools like ChatGPT, it feels clear that intelligence isn’t going to remain scarce for much longer.</p> <p>Just think about it—ChatGPT alone has made access to a certain level of intelligence cheaper than it’s ever been. Five years ago, you would’ve paid a significant amount to have someone tutor you, answer your questions, or explain concepts clearly. Now, anyone can access that kind of intelligence instantly, at a fraction of the cost.</p> <p>This trend isn’t slowing down. In fact, I’m pretty convinced it will accelerate. It’s <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> all over again. Large language models are becoming cheaper and more capable every year, pushing the cost of intelligence lower and lower. For someone like me, who went to university hoping that my knowledge and technical skills would set me apart, that’s a worrying thought.</p> <p>Honestly, I feel an existential anxiety about this. If intelligence itself becomes cheap, what does it mean to provide something truly valuable to society? How can I ensure my skills or knowledge remain valuable enough to earn a good living?</p> <p>The more I think about it, the clearer it becomes that intelligence alone won’t cut it. What will always be scarce—what people will always crave—isn’t intelligence, but identity. Human experiences, individuality, entertainment, personality—these are things machines can’t easily replace, no matter how intelligent they become.</p> <p>Take Taylor Swift as an example. People aren’t drawn to her simply because she’s intelligent or talented (though she is). They’re drawn to her as a person. Her identity—who she is, how she expresses herself—is something uniquely human and impossible to replicate with technology. Taylor Swift is irreplaceable not because of her skills, but because of her identity and the human connection she creates.</p> <p>That’s the kind of scarcity that holds up even as technology advances.</p> <p>So, where does that leave me? Right now, my skills and identity aren’t particularly unique. I’m not providing anything truly scarce. It’s a sobering realization, but also an important one. Maybe the key to staying valuable in this era isn’t about becoming smarter, but becoming more human—expressing something authentic, relatable, or simply enjoyable.</p> <p>I’m still figuring this out. But one thing seems clear: in a world where intelligence is increasingly abundant, identity might just become the most valuable asset we have.</p>]]></content><author><name></name></author><category term="Thought"/><category term="AI"/><summary type="html"><![CDATA[As intelligence becomes cheaper, what does it mean to be valuable?]]></summary></entry><entry><title type="html">Claude Code — Three Days In</title><link href="https://ht0324.github.io/blog/2025/Claude-Code-2/" rel="alternate" type="text/html" title="Claude Code — Three Days In"/><published>2025-03-07T12:00:00+00:00</published><updated>2025-03-07T12:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code-2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code-2/"><![CDATA[<p>It’s been three days since I started using <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>. The initial excitement was so strong that I immediately wrote a <a href="https://ht0324.github.io/blog/2025/Claude-Code/">blog post</a> about it. But after a couple more days, having spent more time exploring its capabilities, I’ve found myself with even more thoughts—so here’s a quick follow-up.</p> <hr/> <h3 id="time-saving-understated-yet-mind-blowing">Time-Saving: Understated Yet Mind-Blowing</h3> <p>I know I’ve already emphasized how much time Claude Code saves, but seriously, I can’t stress it enough. Over the past couple of days, I’ve been completely revamping my personal website from the ground up. Let me preface this by saying: I’m not really a web guy. Before Claude Code, my blog was just a clone of the <a href="https://alshedivat.github.io/al-folio/">Al Folio</a> theme. Whenever I wanted to tweak something, it felt daunting. Each tiny modification usually ended in frustration or compromise because the effort required to change something small just wasn’t worth it.</p> <p>Now, if I want to adjust the tag styling or add new functionality—boom, I just instruct Claude Code to do it. It happily handles the rest. The difference in my blog over just these past couple of days is truly night and day. I’ve implemented numerous nooks and crannies, subtle features, and design tweaks that previously felt impossible.</p> <p>Of course, Claude isn’t perfect, but the sheer amount of time it saves is staggering. To put it into perspective, yesterday I wasn’t satisfied with how the tag feature functioned, so I decided to change it up. Claude handled the entire process for about $1 in API costs. A dollar. Less than a cup of coffee. Without it, the same change would have probably taken me hours of tinkering and frustration.</p> <p>At first, Claude felt like a pair-programming partner, but even after just three days, that feeling is fading. Instead, it’s become something more like a virtual assistant—an assistant that can actually do things, not just talk about them.</p> <hr/> <h3 id="chatbots-vs-agents-what-is-vs-do-it">Chatbots vs. Agents: “What Is” vs. “Do It”</h3> <p>Using traditional chatbots like ChatGPT always felt like carefully constructing queries, specifying every little detail of my intent. Most of my interactions with ChatGPT—and likely yours too—are fundamentally knowledge retrieval: asking “What is this?” or “Tell me about that.” ChatGPT essentially compresses vast internet knowledge and provides succinct, precise answers. It’s exceptional at answering questions and explaining concepts, but it’s mostly passive—focused on understanding and summarizing existing knowledge.</p> <p>But Claude Code changed the game completely. With a truly agentic model like Claude, I don’t have to painstakingly spell out exactly how I want something done. Instead, I just specify <em>what</em> I want done, and Claude figures out the <em>how</em>. It doesn’t just summarize information; it actively plans, makes decisions, and implements changes autonomously. This shift from explicitly defined instructions (“what is”) to implicit, actionable intent (“do it”) feels genuinely profound—a step change in capability that transforms the user experience.</p> <p>Of course, because Claude isn’t perfect yet, the collaboration still exists in the form of verifying the results. But as I’ve <a href="https://medium.com/@FdForThought/framing-rlhf-as-generation-vs-verification-4d9e95b88534">said before</a> <a href="https://medium.com/@FdForThought/generation-vs-verification-epiphany-after-o1-713c6f411206">many times</a>: verification is vastly easier than generation, and right now, I’m comfortably on the verification side. I simply check Claude’s implementation by deploying my blog and testing the functionality. This still requires my input, but the mental load is drastically lighter.</p> <hr/> <h3 id="from-collaboration-to-automation">From Collaboration to Automation</h3> <p>While it still feels somewhat collaborative now, I’m realizing that my role is already shrinking from active collaborator to passive verifier. It’s an assistant relationship—not really a partnership anymore. Verification feels simpler and simpler, and honestly, I can foresee even verification becoming automated soon. At that point, human involvement becomes negligible.</p> <p>Previously, I believed in the paradigm that self-feedback loops (generation versus verification) would incrementally increase intelligence. But intelligence and agency are fundamentally different. Learning how to be smart/knowledgable and learning how to act are not the same, and the latter is far more powerful. I’m now convinced that agentic capability will lead to implications far bigger and faster than I initially anticipated.</p> <p>If (or more realistically, <em>when</em>) verification itself is automated, we won’t be collaborating with AI—we’ll be observing it collaborating with itself. That’s when things will get really interesting (and potentially unsettling).</p> <hr/> <h3 id="toward-agentic-intelligence">Toward Agentic Intelligence?</h3> <p>OpenAI previously described levels of AI intelligence: Level 2 (reasoners), Level 3 (agents), and Level 4 (collaborators or organizations). Claude Code sits loosely at Level 3, and my recent experiences hint strongly that Level 4—agents autonomously collaborating with each other—is closer than we might think.</p> <p>As these agentic models improve and increasingly handle their own verification, we’ll approach a point where human oversight becomes unnecessary. This isn’t speculation anymore; using Claude Code has convinced me that the shift toward fully autonomous agentic systems isn’t just possible—it’s imminent.</p> <p>The impact of that shift can’t be overstated. As impressive as Claude Code is today, it’s the worst this technology will ever be. And that’s both thrilling and deeply concerning.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Further reflections on Claude Code]]></summary></entry><entry><title type="html">VQVAE – Review</title><link href="https://ht0324.github.io/blog/2025/VQVAE/" rel="alternate" type="text/html" title="VQVAE – Review"/><published>2025-03-05T11:30:00+00:00</published><updated>2025-03-05T11:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/VQVAE</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/VQVAE/"><![CDATA[<p>In this post, I review the paper <a href="https://arxiv.org/abs/1711.00937">“Neural Discrete Representation Learning”</a>, commonly known as VQ-VAE. This paper introduces a generative model that combines vector quantization with Variational Autoencoders (VAEs).</p> <p>Its core idea is replacing the continuous latent space typically used in VAEs with discrete embeddings. This addresses the notorious issue of posterior collapse, ensuring the encoder meaningfully contributes to data reconstruction rather than relying solely on a powerful decoder.</p> <p>The discrete latent space makes this model particularly suited to domains naturally represented by discrete data, like speech, language, and structured visual information.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Discrete Latent Variables</strong><br/> Instead of using continuous distributions, VQ-VAE encodes data into discrete latent variables selected from a predefined embedding dictionary. This discretization helps prevent posterior collapse by forcing the encoder to produce meaningful, constrained representations.</p> <p><strong>Vector Quantization (VQ)</strong><br/> Vector Quantization involves mapping encoder outputs to the nearest embeddings in a learned dictionary. Although there’s no straightforward gradient through this discrete step, the authors use a simple “straight-through” estimator, effectively copying gradients from decoder inputs back to encoder outputs.</p> <p><strong>Commitment Loss</strong><br/> To ensure the encoder doesn’t produce arbitrary embeddings, VQ-VAE introduces a commitment loss. This regularization term encourages encoder outputs to remain close to their assigned embedding vectors, stabilizing training and improving the quality of learned representations.</p> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Posterior Collapse is a Bigger Deal Than I Thought</strong><br/> Initially, I underestimated posterior collapse, thinking that if the decoder reconstructed well, the encoder must be doing its job. But I learned that’s not necessarily true—if the decoder is too powerful, it can bypass the encoder entirely, undermining the entire concept of an autoencoder. VQ-VAE addresses this directly through discretization.</p> <p><strong>Constraining the Latent Space is Helpful</strong><br/> Surprisingly, imposing discrete constraints on latent representations helps the model learn better. Intuitively, I thought constraints might harm performance, but VQ-VAE demonstrates that limiting flexibility actually prevents the model from “getting lost,” ultimately improving representation quality.</p> <p><strong>The Gradient Copying Trick is Surprisingly Effective</strong><br/> VQ-VAE’s training includes copying decoder input gradients directly to encoder outputs—a method that feels very ad-hoc. Despite its simplicity and my initial skepticism, this approach works remarkably well, suggesting that straightforward solutions can sometimes outperform more sophisticated ones.</p> <p><strong>Tokenization of Latent Space Could Lead to New Applications</strong><br/> By tokenizing the latent space, VQ-VAE opens potential avenues for using transformer architectures on latent representations. Given that transformers excel with discrete token sequences, VQ-VAE’s discrete embeddings might unlock new approaches for processing continuous data modalities as if they were language-like sequences.</p> <p><strong>Discrete Representations Naturally Align with Certain Data Types</strong><br/> I heard an hypothesis that VQ-VAE performs particularly well with inherently discrete data like language tokens or audio spectrograms. This makes intuitive sense, yet it remains an open question whether discrete latent spaces universally outperform continuous ones across various data domains.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>VQ-VAE introduces a straightforward yet effective method to discretize latent representations in autoencoders. By embracing discrete embeddings, it elegantly sidesteps posterior collapse and paves the way for new model architectures inspired by token-based learning.</p> <p>While discrete latent spaces offer promising advantages, further exploration is necessary to fully understand their limits and strengths across diverse applications.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[My thoughts on the VQ-VAE paper]]></summary></entry><entry><title type="html">Claude Code</title><link href="https://ht0324.github.io/blog/2025/Claude-Code/" rel="alternate" type="text/html" title="Claude Code"/><published>2025-03-04T10:59:59+00:00</published><updated>2025-03-04T10:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code/"><![CDATA[<p>Today, I used <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>, an experimental tool released by Anthropic a while ago. I really wanted to use it right at the start of the announcement, but I kept putting it off because I hadn’t found a use case for it yet. But today, I had to rapidly prototype an app, so I decided to give it a shot.</p> <hr/> <h3 id="claudes-agentic-capabilities">Claude’s Agentic Capabilities</h3> <p>Claude Code is basically <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which has been updated by Anthropic to have more agentic capabilities. They’ve essentially wrapped it with tools that can run commands and execute terminal operations. Claude can see your repository, view your file directory, edit files, and execute terminal commands. So, yeah, it has much more agency.</p> <p>I needed to quickly prototype an app that hosts images, allows filtering by tags, and enables image similarity search. And lo and behold, after just 2-3 hours, I had a functioning prototype.</p> <p>I haven’t used <a href="https://www.cursor.com/">Cursor</a>, the VS Code variant that gained traction some time ago and was widely praised by people in the Bay Area. But maybe now I’m experiencing the epiphany that Cursor users had. The experience wasn’t just about typing in my query and having Claude do everything from start to finish - it felt more like pair programming.</p> <p>It gave me the sense that there was a person behind the terminal, troubleshooting issues and debugging with me. Of course, Claude wasn’t perfect, but the back-and-forth, iterative process was refreshing and much more engaging than just randomly encountering errors and sifting through Stack Overflow links to fix them.</p> <hr/> <h3 id="a-new-kind-of-collaboration">A New Kind of Collaboration(?)</h3> <p>It was a massive time-saver. I provided high-level directions, Claude executed them, and when problems arose, we solved them together. The biggest surprise was how collaborative the process felt - it was much easier and more efficient than doing everything on my own, or switching back and fourth using a chat interface.</p> <p>If I had done this single-handedly, it would have taken days. But for just $3.50 in API costs (which might seem expensive depending on your threshold), I built a prototype that I’m pretty confident in within 2–3 hours.</p> <p>Claude Code also helped me fix bugs on my self-hosted blog. I’m more of an AI guy, not a web guy, so managing my blog often involved trial and error - throwing things at the wall and seeing what stuck. I had to manually fix bugs and do all sorts of tweaks to get it to look the way I wanted.</p> <p>But with Claude Code, I could simply query my repository - it became conversational. That made it much easier to fix persistent issues in a short amount of time. All in all, it’s an incredibly useful tool.</p> <hr/> <h3 id="collaboration-as-a-mirage">Collaboration as a Mirage</h3> <p>But the reason why it felt like a collaborative process is that Claude still isn’t perfect in its agentic execution. Don’t get me wrong - Claude is very capable, but it still requires human intervention. For complex tasks, it doesn’t always get things right on the first try.</p> <p>If we ever reach a point where AI no longer needs intervention - where it can figure out even highly complex instructions entirely on its own - then it would no longer feel like collaboration; it would be full automation.</p> <p>That said, I structured my app development process in a very step-by-step manner, executing tasks sequentially. Maybe that’s why I didn’t run into any major setbacks. If I had given more abstract instructions, I might have been more surprised by what Claude could do. But the key point remains: it still feels like a collaboration because Claude makes mistakes, and I had to intervene to correct them.</p> <p>Looking at the trajectory of AI progress, though, I don’t think this collaborative phase will last long. Eventually, it won’t be a partnership - it will be a replacement.</p> <hr/> <h3 id="the-inevitable-shift">The Inevitable Shift</h3> <p>I’ve recently seen posts on <a href="https://x.com/David_Kasten/status/1893357776702976286">X</a> of anecdotes that from recruiters at frontier labs who now assume that junior staffs are AI replaceable. I was initially skeptical of this claim, but now I kinda believe it. Also, in Andrej Karpathy’s <a href="https://x.com/karpathy/status/1894099637218545984">post</a>, he argues that agency is more valuable than intelligence. This struck me as profound.</p> <p>In terms of intelligence, today’s top LLMs are already more knowledgeable than humans. But in terms of agency, humans still have the upper hand. However, after my interaction with Claude Code, I caught a glimpse of a future where machines will reach agency supremacy as well. When that happens, it won’t be collaboration - it will be a full <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">Humans Need Not Apply</a>(outdated, but still relevant).</p> <p>I encourage everyone reading this to try it for themselves. Hearing about it isn’t the same as experiencing it firsthand. Maybe I’m exaggerating, but I stand by my belief that this kind of automation is a matter of when, not if. And this is the worst this technology will ever be, it’s only going to get better from here.</p>]]></content><author><name></name></author><category term="Blog"/><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[My experience with Claude Code for rapid prototyping, debugging, and implications of AI agency]]></summary></entry><entry><title type="html">Link Archive - Feb 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/" rel="alternate" type="text/html" title="Link Archive - Feb 2025"/><published>2025-02-28T22:59:59+00:00</published><updated>2025-02-28T22:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/"><![CDATA[<p>Another month, another batch of interesting links I’ve come across. Here’s what caught my attention in February 2025.</p> <hr/> <p><strong><a href="https://youtube.com/watch?v=_1f-o0nqpEI">DeepSeek, China, OpenAI, and AI Megaclusters - Lex Fridman Podcast #459</a></strong></p> <ul> <li>I first encountered Dylan Patel on the Dwarkesh podcast with Asianometry, so I was glad to see him on Lex Fridman’s. He specializes in analyzing AI hyperscalers, so his insights are incredibly in-depth. Basically, highly recommended if you’re interested in the infrastructure side of things.</li> </ul> <p><strong><a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model</a></strong></p> <ul> <li>This is published by a group of researchers at Google DeepMind, particularly Sholto Douglas. It’s a step-by-step guide to scaling up compute using infrastructure, specifically TPUs. I’ve been working my way through it, and it starts with the very basics but gets pretty complicated quickly. It’s refreshing to see closed AI labs offering these “breadcrumbs” to the open-source community, sharing their knowledge.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></strong></p> <ul> <li>Similar in nature to the DeepMind scaling book, but this one’s from the Hugging Face team. It’s also very detailed, with an interactive guide, and, I should say, quite a mouthful.</li> </ul> <p><strong><a href="https://srush.github.io/raspy/">Thinking like Transformer</a></strong></p> <ul> <li>The author created a Python library and a detailed blog post explaining how computation actually works within a transformer network. He also developed an abstract conceptual framework for it. It’s really complicated—I still don’t fully grasp it—but it’s nonetheless fascinating.</li> </ul> <p><strong><a href="https://www.youtube.com/watch?v=a42key59cZQ&amp;list=WL&amp;index=2">Gwern - Anonymous Writer Who Predicted AI Trajectory on $12K/Year Salary</a></strong></p> <ul> <li>This is from the Dwarkesh Podcast. He introduces a blogger named Gwern, and you really need to read his work to understand. He has an extensive blog called <a href="http://gwern.net/">gwern.net</a>. His posts are incredibly in-depth and well-researched, more like separate articles. The sheer depth is staggering, unlike anything I’ve encountered, even in reputable sources like the New York Times. I think this guy is incredibly capable. It also made me think about how the current internet is dominated by big tech. But in the pre-big tech era, I heard there were these niche, individual creators posting unique content. Gwern feels like a holdover from that era.</li> </ul> <p><strong><a href="https://gwern.net/subculture">The Melancholy of Subculture Society - Gwern</a></strong></p> <ul> <li>Gwern’s writing is extensive and very long, so it requires dedicated time. This is one piece I read, and it’s a musing on how the internet, as technology evolves, is kind of segregating society, and the state of things. I think it’s a well-thought-out piece.</li> </ul> <p><strong><a href="https://zhengdongwang.com/2024/12/29/2024-letter.html">2024 Letter - Zhengdong Wang</a></strong></p> <ul> <li>This blog post, written by a researcher at Google DeepMind, makes a bombshell statement. Regarding the current state of their large language model research and machine learning models, he boldly claims that the models will essentially achieve anything if the evaluations are clearly defined. So, if there are evals, <em>any</em> evals, the model will succeed. It’s an audacious statement, but the implications are mind-blowing. I should probably write a dedicated blog post about this.</li> </ul> <p><strong><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">Sesame Research - Crossing the Uncanny Valley of Voice</a></strong></p> <ul> <li>This startup called Sesame revealed a really interesting voice-to-voice language model. They have a demo, and it works quite well. The key difference between Sesame and ChatGPT’s Advanced Voice Mode is that the model produces nuanced tones, like “ums” and “ahs,” very naturally. It feels incredibly realistic. For the first 10 minutes, I was blown away. But as I probed the model further, it became clear it doesn’t possess the same level of intelligence as LLMs. The same goes for ChatGPT’s Advanced Voice Mode; OpenAI seems to have significantly limited the model to comply with their guidelines. In Sesame’s case, I suspect the limitations are purely due to model and computational constraints.</li> </ul> <hr/> <p><br/> Hope you enjoyed this month’s picks—March should bring even more to explore!</p>]]></content><author><name></name></author><category term="Link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos I explored in February 2025]]></summary></entry><entry><title type="html">Generative Adversarial Nets – Review</title><link href="https://ht0324.github.io/blog/2025/GAN/" rel="alternate" type="text/html" title="Generative Adversarial Nets – Review"/><published>2025-02-27T18:00:00+00:00</published><updated>2025-02-27T18:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/GAN</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/GAN/"><![CDATA[<p>This time, I’m reviewing the classic paper <a href="https://arxiv.org/abs/1406.2661">“Generative Adversarial Nets”</a>, famously known as GAN. This 2014 paper by Ian Goodfellow and colleagues introduced a revolutionary method of training generative models using an adversarial setup between two neural networks—a generator and a discriminator.</p> <p>The central concept is simple yet powerful: a generator attempts to produce realistic samples to fool a discriminator, while the discriminator tries to distinguish real samples from generated ones. This competition pushes both models to improve continually, ultimately enabling the generator to produce incredibly realistic outputs without directly modeling complex probability distributions.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Generator and Discriminator</strong><br/> GAN consists of two neural networks trained simultaneously. The <strong>Generator (G)</strong> learns to produce data that looks like real samples from random noise, and the <strong>Discriminator (D)</strong> learns to classify whether a given sample is real or fake. This interplay drives continuous improvement.</p> <p><strong>Minimax Game and Value Function</strong><br/> GAN training is framed as a minimax game, where D tries to maximize its accuracy, while G tries to minimize it. Mathematically, this interaction is captured by a value function involving two competing optimization steps, one ascending (for D) and one descending (for G).</p> <p><strong>Noise as Input (Latent Representation)</strong><br/> The generator takes random noise as input (often standard Gaussian noise), which it transforms into realistic data. This noise acts as a latent representation, similar in purpose to latent spaces in other generative models like VAE or diffusion models.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>An Intuitive “Police vs. Counterfeiters” Paradigm</strong><br/> The analogy used by the authors—police versus counterfeiters—made the adversarial training idea very intuitive. The discriminator is essentially a “police officer,” trying to catch counterfeit samples, while the generator, acting as “counterfeiters,” constantly improves to evade detection. This simple analogy clarified the competitive dynamics at the heart of GANs.</p> <p><strong>Why the Order of Optimization Matters</strong><br/> One subtle but critical insight is the optimization order: in theory, the discriminator should reach optimality first before the generator updates. This sequential ordering makes sense because the discriminator provides the target (“answer sheet”) for the generator. Practically, training typically alternates between discriminator and generator updates, often simplifying to a 1:1 step ratio, despite theoretical ideals suggesting otherwise.</p> <p><strong>The “Saturation” Problem</strong><br/> Initially, the idea of the generator’s gradient saturating (becoming ineffective) was unclear. The authors point out that if the discriminator becomes too strong too early, the generator’s gradients become nearly zero because it consistently outputs samples easily identified as fake. Understanding this clarified the importance of balancing the discriminator and generator strengths.</p> <p><strong>Noise as a Form of Latent Space</strong><br/> Initially, calling the generator input “noise” felt unintuitive. Why noise? After considering diffusion models and VAE, I realized noise serves as a random seed or latent code that gets mapped to structured data. Essentially, it’s just a convenient way to introduce randomness into the otherwise deterministic neural network framework, enabling continuous generation and meaningful interpolation between points in this latent space.</p> <p><strong>Interpolation and Connection to VAEs</strong><br/> Interestingly, GANs achieve interpolation naturally despite the absence of a clearly defined encoder (unlike VAEs). VAEs explicitly model latent spaces to allow interpolation, but GANs achieve this indirectly. Because the generator learns from the discriminator’s feedback rather than directly fitting discrete data points, it inherently learns a continuous representation. This indirect approach was fascinating because it avoids explicitly modeling complicated distributions yet still produces meaningful interpolations.</p> <p><strong>Simplicity of Theoretical Results</strong><br/> The theoretical results, particularly the global optimality condition (pg = pdata), were straightforward yet elegant. By defining an optimal discriminator, the proof shows a simple Jensen-Shannon divergence between the true and generated distributions. Unlike VAE’s mathematically heavy derivations, GAN presents a more intuitive theoretical grounding.</p> <p><strong>GANs vs. VAEs – Simpler but Powerful</strong><br/> Reflecting on VAE and GAN together, I realized GAN’s elegance lies in its simplicity. While VAE explicitly approximates intractable distributions with mathematical rigor, GAN sidesteps complexity through the clever adversarial setup. This simplicity initially may have seemed too naive to work, but history proved otherwise, showing that GANs indeed leveraged the powerful discriminative capabilities already established in neural networks.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>“Generative Adversarial Nets” introduced a simple yet groundbreaking framework that pits generators against discriminators to produce realistic data without explicitly modeling complex distributions. Its intuitive adversarial concept, practical simplicity, and powerful theoretical grounding explain why GANs rapidly became foundational in generative modeling.</p> <p>Exploring GAN alongside VAE deepened my understanding of how different approaches tackle generative tasks, showing GAN’s unique combination of simplicity, intuition, and effectiveness.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the original GAN paper]]></summary></entry><entry><title type="html">Auto-Encoding Variational Bayes – Review</title><link href="https://ht0324.github.io/blog/2025/VAE/" rel="alternate" type="text/html" title="Auto-Encoding Variational Bayes – Review"/><published>2025-02-24T14:00:00+00:00</published><updated>2025-02-24T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/VAE</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/VAE/"><![CDATA[<p>In this post, I’m reviewing the paper <a href="https://arxiv.org/abs/1312.6114">“Auto-Encoding Variational Bayes”</a>, better known as the Variational Autoencoder (VAE) paper. Published by Kingma and Welling, this paper changed how generative models handle latent variables by introducing a practical and efficient way to perform approximate Bayesian inference.</p> <p>The central challenge addressed here is efficiently training models that involve continuous latent variables with intractable posterior distributions. By cleverly combining stochastic gradient methods with a reparameterization trick, VAE enables effective learning in models previously considered too complex or computationally infeasible.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Variational Autoencoder (VAE)</strong><br/> VAE is a generative model built around an encoder-decoder structure, with the crucial twist of modeling latent variables probabilistically rather than deterministically. It introduces an approximate posterior distribution to represent latent variables, enabling the model to capture uncertainty explicitly.</p> <p><strong>Evidence Lower Bound (ELBO)</strong><br/> ELBO, or Evidence Lower Bound, is central to VAE training. It balances reconstruction accuracy (how well the model reconstructs input data) against the complexity of the latent representation, quantified through KL divergence. Maximizing ELBO effectively encourages the latent distribution to align closely with a chosen prior, typically a standard Gaussian.</p> <p><strong>Reparameterization Trick</strong><br/> The paper introduces a “reparameterization trick,” which transforms a random sampling step into a deterministic operation combined with stochastic noise. This allows gradients to flow through sampling operations, making end-to-end optimization via stochastic gradient descent possible.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Essence of ELBO (Evidence Lower Bound)</strong><br/> Initially, ELBO felt abstract, and the paper didn’t clearly illustrate how it emerged mathematically. After deeper reflection (and some external clarifications), I realized ELBO comes directly from Bayesian inference principles, where we replace an intractable posterior with a simpler approximate distribution. ELBO effectively quantifies how well this approximation matches the true posterior, balancing model accuracy and complexity.</p> <p><strong>KL Divergence and Its Role as a Regularizer</strong><br/> One core insight I gained is the intuitive role of KL divergence. KL divergence measures how much the approximate posterior deviates from the chosen prior distribution. Minimizing KL divergence ensures that the model doesn’t rely excessively on overly complex or arbitrary latent representations, essentially acting as a regularizer that simplifies latent spaces.</p> <p><strong>Why the Reparameterization Trick Matters</strong><br/> The reparameterization trick initially seemed trivial but turned out to be crucial. Without this trick, sampling latent variables would break differentiability, making gradient-based optimization impossible. Reparameterization elegantly solves this by separating randomness (sampling) from deterministic parameters, enabling efficient end-to-end training through backpropagation.</p> <p><strong>Intuition Behind Probabilistic Latent Spaces</strong><br/> Traditional autoencoders map data deterministically into latent spaces, limiting their generative capabilities. By introducing probability distributions in the latent space, VAE smoothly maps continuous regions, allowing for meaningful interpolation and generation. This continuous latent representation enables more natural data generation compared to deterministic counterparts.</p> <p><strong>Complexity in Mathematical Derivations</strong><br/> The mathematical rigor of VAE made me appreciate the complexity of early deep-learning research. The derivations involved Bayesian inference, integral calculus, and careful manipulations to yield neat equations like ELBO. It became clear that foundational AI research required substantial mathematical fluency—something less critical today, perhaps, but undeniably essential back then.</p> <p><strong>Relation to EM Algorithm and Bayesian Methods</strong><br/> The paper also highlighted connections to the Expectation-Maximization (EM) algorithm and Bayesian methods. VAE generalizes and scales ideas traditionally handled by EM, using neural networks and stochastic optimization instead of traditional iterative approaches. Understanding this relation gave me a deeper context of where VAE fits into the broader landscape of machine learning methods.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>“Auto-Encoding Variational Bayes” introduces the Variational Autoencoder—a pivotal approach blending Bayesian inference with neural networks. Through ELBO maximization, KL divergence regularization, and the innovative reparameterization trick, VAEs enable efficient learning in previously intractable scenarios.</p> <p>Despite the mathematical complexity, the intuition is clear: by making latent representations probabilistic, VAE unlocks powerful generative capabilities and remains a foundational concept in modern generative modeling.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the VAE paper, diving into variational inference, reparameterization, and why ELBO matters]]></summary></entry><entry><title type="html">NeRF – Representing Scenes as Neural Radiance Fields - Review</title><link href="https://ht0324.github.io/blog/2025/NERF/" rel="alternate" type="text/html" title="NeRF – Representing Scenes as Neural Radiance Fields - Review"/><published>2025-02-20T20:30:00+00:00</published><updated>2025-02-20T20:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/NERF</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/NERF/"><![CDATA[<p>In this post I’ll talk about the paper <a href="https://arxiv.org/abs/2003.08934">“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis”</a>. This paper introduces NeRF, a method to represent complex scenes as continuous neural fields, enabling high-quality view synthesis from sparse images.</p> <p>NeRF uses a simple yet effective architecture: a fully-connected neural network (MLP) that takes continuous 5D inputs (3D coordinates plus viewing angles) and outputs both color and volume density. This setup allows synthesizing previously unseen views simply by querying the network and performing differentiable volume rendering along camera rays.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Neural Radiance Field (NeRF)</strong><br/> A NeRF is essentially a neural network that maps a 5D coordinate (3D spatial location <code class="language-plaintext highlighter-rouge">(x, y, z)</code> plus viewing direction <code class="language-plaintext highlighter-rouge">(θ, φ)</code>) to two outputs: volume density (opacity) and RGB color. The beauty lies in representing an entire scene within a compact neural network rather than explicitly storing a dense voxel grid or using complex 3D models.</p> <p><strong>Volume Rendering</strong><br/> To synthesize novel views, NeRF employs classical volume rendering techniques. Camera rays traverse the scene, accumulate opacity and color values from sampled points, and project these values into an image. Crucially, this process is differentiable, allowing the network to learn directly from input images without any explicit 3D geometry supervision.</p> <p><strong>Positional Encoding</strong><br/> Interestingly, directly feeding spatial coordinates into a neural network doesn’t work well for capturing fine details. The authors cleverly use positional encoding, mapping coordinates into higher-dimensional spaces using sinusoidal functions, helping the network learn high-frequency variations in geometry and appearance.</p> <p><strong>Hierarchical Sampling (Coarse &amp; Fine Networks)</strong><br/> To make the rendering efficient, NeRF uses a hierarchical sampling strategy. Initially, it samples points coarsely to estimate areas of importance, then densely samples those areas with a second “fine” network. This two-stage approach ensures computational efficiency and improved quality simultaneously.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Surprising Power of Simple MLPs</strong><br/> Initially, I assumed NeRF would require complex architectures. Surprisingly, a plain fully-connected network (MLP) was sufficient. This challenged my assumption that complex scenes need complicated models; NeRF elegantly achieves complexity through smart input encoding and sampling strategies rather than architectural complexity.</p> <p><strong>Positional Encoding Makes a Huge Difference</strong><br/> At first glance, positional encoding seemed like just a minor tweak. But it’s crucial. Without positional encoding, the model struggles to capture high-frequency details like textures and sharp edges. This was counterintuitive, as I originally thought that neural networks naturally handle continuous inputs well. Positional encoding acts like giving the network a “cheat sheet” for frequencies it should pay attention to.</p> <p><strong>Hierarchical Sampling – Efficiency through Bias</strong><br/> The hierarchical sampling was something I wouldn’t have thought of myself. Instead of uniformly sampling every point, NeRF first broadly samples (“coarse”) to identify important regions. Then, based on these initial guesses, it strategically places more samples in regions with higher density (or significance). It’s a smart trick: effectively biasing sampling toward regions that matter, vastly improving computational efficiency.</p> <p><strong>Overfitting as a Feature, Not a Bug</strong><br/> One of the most interesting realizations was that NeRF intentionally overfits to a specific scene. Unlike traditional deep learning models aiming for generalization, NeRF constructs a specialized network for each scene, optimizing solely for accuracy within that context. It felt unconventional but makes sense because NeRF’s goal isn’t generalization—it’s to achieve photorealistic rendering for a given scene.</p> <p><strong>Why This Matters for Interpolation and Rendering Quality</strong><br/> NeRF’s method naturally leads to excellent interpolation between views, providing realistic novel perspectives even from limited viewpoints. Because it learns a continuous representation instead of discrete samples, it effortlessly generates smooth transitions between views. It’s elegant, simple, yet incredibly powerful.</p> <p><strong>Comparisons and Connections with VAEs and GANs</strong><br/> Reflecting on my earlier reviews (VAE and GAN), I realized NeRF shares a fundamental idea: continuous latent representations. While VAEs explicitly enforce structured latent distributions and GANs rely on adversarial learning, NeRF takes a more straightforward approach by embedding spatial coordinates directly into neural networks. However, it still captures continuous structures that allow interpolation, something VAE explicitly constructs and GAN achieves implicitly.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>NeRF brilliantly demonstrates how neural networks can represent entire scenes using only a few images and positional encodings. By intentionally overfitting and employing hierarchical sampling, it achieves stunning photorealism without explicit geometric models.</p> <p>It’s intriguing how simple design choices can dramatically shift the approach—and effectiveness—of neural models in tasks like 3D view synthesis. This simplicity and elegance are precisely why NeRF has quickly become foundational in neural rendering research.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Diving into NeRF — a simple yet clever idea of neural networks representing continuous 3D scenes]]></summary></entry><entry><title type="html">Direct Preference Optimization (DPO) - Review</title><link href="https://ht0324.github.io/blog/2025/DPO/" rel="alternate" type="text/html" title="Direct Preference Optimization (DPO) - Review"/><published>2025-02-17T22:00:00+00:00</published><updated>2025-02-17T22:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/DPO</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/DPO/"><![CDATA[<p>In this post, I’ll share my thoughts on the paper <a href="https://arxiv.org/abs/2305.18290"><em>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em></a>, usually called DPO. This paper is interesting because it approaches the widely used Reinforcement Learning with Human Feedback (RLHF) paradigm in a fundamentally simpler way.</p> <p>The paper starts with the motivation behind alignment: aligning language models (LMs) with human preferences. Existing methods like PPO-based RLHF are effective but notoriously complex and unstable. DPO aims to simplify this by cleverly rethinking how reward modeling and preference alignment can be done.</p> <p>Let’s dive in.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Direct Preference Optimization (DPO)</strong><br/> DPO removes the explicit reward modeling step that’s common in RLHF. Instead of first training a separate reward model and then using RL to maximize that reward, DPO cleverly formulates the reward implicitly within the language model itself. This allows solving the alignment problem using a simple supervised classification objective.</p> <p><strong>Bradley-Terry Model</strong><br/> DPO builds upon the Bradley-Terry model, a statistical approach for estimating the probability that one option is preferred over another. While it initially seemed random that they picked such an old statistical model, it’s been used previously by OpenAI and DeepMind in RL settings. The Bradley-Terry formula gives an intuitive probability distribution over preferences, making it suitable for learning from human feedback.</p> <p><strong>Implicit Reward Representation</strong><br/> The key insight in DPO is that the “reward” for a given output can be represented implicitly as the log ratio of the probabilities from the learned LM and a reference LM (usually the SFT or supervised fine-tuned model). Instead of explicitly modeling rewards, DPO directly optimizes this ratio to reflect human preferences.</p> <p><strong>Simplified Objective (No RL Required)</strong><br/> By using the implicit reward representation, DPO turns preference alignment into a supervised learning problem. The objective is simply to increase the probability of preferred outputs and decrease the probability of non-preferred ones, scaled by a weighting term reflecting confidence in the model’s preference ranking.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Removing the Explicit Reward Model is Clever</strong><br/> Initially, I thought the explicit reward model was essential to RLHF. It seemed natural: first learn rewards from humans, then optimize those rewards. DPO surprised me by showing you can skip that step completely. Instead of explicitly modeling human preferences, DPO encodes them directly into the language model’s probabilities. It’s neat, elegant, and way simpler.</p> <p><strong>The Bradley-Terry Model as the Theoretical Foundation</strong><br/> At first glance, the Bradley-Terry model felt arbitrary to me—it’s a decades-old statistical model, after all. But its use in RLHF contexts actually dates back to earlier work by OpenAI and DeepMind. Bradley-Terry is intuitive because it translates pairwise human preference data directly into probabilities. DPO smartly leverages this model to avoid more complicated RL setups.</p> <p><strong>DPO’s Loss Function Explained</strong><br/> The loss function in DPO initially confused me because of its signs and terms. However, the intuition eventually clicked: it increases the likelihood of preferred outputs and decreases the likelihood of dispreferred ones. Crucially, the loss is weighted by how “wrong” the model is about these preferences. If the model is confident but incorrect, it makes larger corrections. This weighting makes the optimization stable and effective.</p> <p><strong>Why Stability is Important (and How DPO Ensures it)</strong><br/> Standard RLHF methods like PPO often become unstable because they rely on explicit reward models and require careful tuning. DPO bypasses this by using a fixed reference model (usually the SFT model). By avoiding explicit reward estimation and complicated online updates, DPO remains stable, even as the model improves.</p> <p><strong>Surprising Performance Without Complex Tuning</strong><br/> The experimental results clearly showed that DPO achieves comparable or better performance compared to traditional RLHF methods. What was most surprising to me was that DPO achieves high alignment quality without requiring extensive hyperparameter tuning or complex reward sampling procedures. It simplifies the process dramatically.</p> <p><strong>Philosophical Shift: Overfitting vs. Generalization</strong><br/> An interesting aspect of DPO (and similar recent methods like ORPO) is that it operates entirely offline. Unlike traditional RL methods, which involve interactive environments and trajectories, DPO treats preference alignment purely as supervised learning. It’s a philosophical shift from generalization towards targeted optimization or even intentional overfitting to human preferences. It made me rethink what RLHF truly means.</p> <p><strong>KL Divergence and Model Drift</strong><br/> I found the authors’ point about KL divergence important but not fully explained in the paper. DPO achieves high alignment quality without significant drift from the original supervised fine-tuned (SFT) model. This matters because extensive drift can degrade other aspects of the model, such as coherence or factual accuracy. Staying close to the original SFT model helps maintain overall quality.</p> <hr/> <h3 id="some-things-that-were-initially-confusing">Some Things That Were Initially Confusing</h3> <ul> <li><strong>The leap in equation (4)</strong>: I struggled at first to understand how they substituted the optimal policy into their objective so cleanly. It made perfect sense mathematically afterward, but felt like a creative jump rather than an obvious derivation.</li> <li><strong>Weighted gradient interpretation</strong>: Initially, the weighting term in the gradient was counterintuitive due to signs. Eventually, I understood it as correcting more strongly when the model’s implicit reward ordering is confidently wrong.</li> <li><strong>KL Divergence Interpretation</strong>: It wasn’t clear to me initially why minimizing KL divergence from the reference (initial) policy is inherently desirable. Later I understood it prevents excessive drift from a stable baseline, which preserves other desirable model properties.</li> </ul> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>DPO simplifies RLHF dramatically by removing the explicit reward modeling step, making fine-tuning easier, more stable, and faster. Although it’s not strictly reinforcement learning in a traditional sense (it lacks online updates and explicit reward signals), it captures the essence of aligning models to human preferences in a cleaner and more accessible way.</p> <p>The simplicity and clarity of DPO’s core idea—implicitly encoding rewards directly into language models—makes it very appealing. While it’s not the only method (ORPO takes this even further), DPO is an insightful approach that significantly reduces complexity without sacrificing quality.</p> <p>Overall, it’s a valuable reminder that sometimes the best advances come not from adding complexity, but from cleverly stripping it away.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing DPO, a simpler way to fine-tune language models to human preferences without explicitly modeling rewards]]></summary></entry></feed>