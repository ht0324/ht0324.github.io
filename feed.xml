<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-18T14:26:43+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Thoughts on o3, o4 mini</title><link href="https://ht0324.github.io/blog/2025/o3-o4mini/" rel="alternate" type="text/html" title="Thoughts on o3, o4 mini"/><published>2025-04-17T17:00:00+00:00</published><updated>2025-04-17T17:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/o3-o4mini</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/o3-o4mini/"><![CDATA[<p>So, here are my recent thoughts on the release of OpenAI’s <a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3 and o4 mini</a>. In conclusion, it’s a bit of a mixed bag. It’s easy to get caught up in the hype, and there are definitely notable things present. While generally, I think the release and the resulting models are fantastic, there are subtle nuances that need to be addressed.</p> <p>For a quick recap for those who haven’t caught up: OpenAI released <strong>o3</strong> and <strong>o4 mini</strong>, new variants of their reasoning models specifically trained to use tools and be agentic. When you see the demos and people’s use cases, it really is fantastic. It has more of a “person feel.” I’ve used it myself, and compared to previous models that primarily did research by fetching and analyzing web info, <strong>o3</strong> and <strong>o4 mini</strong> feel much more agentic. Unlike previous models which ca use function calling but rather in a separate isolated manner, they seem to actively parse information, act based on it, and use various tools such as command‑line interface. In that sense, they are really capable models.</p> <h3 id="context-and-comparison">Context and Comparison</h3> <p>But for me, after the initial impression settled, it felt like OpenAI basically released their version of Anthropic’s <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which is really good at agentic tasks. Because of its agentic capabilities, 3.7 Sonnet became a go‑to enterprise solution, especially for agent‑coding IDEs. While OpenAI’s new models arrived two or three months later and are arguably better, the fundamental paradigm hasn’t drastically changed.</p> <p>Another thing I really noted was within the ChatGPT interface itself. When serving <strong>o3</strong> and <strong>o4 mini</strong>, OpenAI enabled function calling and tool use <em>by default</em>. This means the models can readily use capabilities like data analysis, the coding environment, web search, and others that OpenAI has integrated into ChatGPT. All this combined gives the models a vast array of tools they didn’t have access to so easily before, and it does have a combinatorial effect on model capabilities.</p> <p>Previously, if I wanted to research a topic while studying, I’d usually paste my content into the chat and query the model. Now, since the model can search the web itself – and it feels less like simple RAG and more like it’s actually fitting the fetched info into its context – I don’t have to provide as much relevant context manually. It feels much more reliable in that sense, reducing the need for me to spoon‑feed context. I think the capability of the chat interface itself has drastically changed.</p> <h3 id="openai-as-a-product-company">OpenAI as a Product Company</h3> <p>This led me to another thought: the overall trajectory of OpenAI as a company.</p> <p>When I saw how OpenAI neatly packaged their models and tools onto a platter and served it within the chat interface, it really clicked for me: as Sam Altman had <a href="https://www.youtube.com/live/5MWT_doo68k?t=653">said</a>, OpenAI is now officially a product company—though in hindsight, given where their revenue comes from, they always were.</p> <p><a href="https://medium.com/@furqankhaan/how-openai-and-anthropic-are-cashing-in-on-ai-a-look-at-their-revenue-models-d9d9ae79dd28">If you compare their revenues to Anthropic’s</a>, OpenAI is the market leader, partly due to the network effect of being the first mover. But most of OpenAI’s revenue comes from user subscriptions with a smaller fraction from API usage. Anthropic is the polar opposite—most revenue comes from API usage, though even their API revenue lags behind OpenAI’s API revenue. This suggests subscriptions are vastly more popular than API access, at least for OpenAI.</p> <p>Then it makes sense for OpenAI to prioritize products because intelligence is becoming cheap, almost too cheap to meter. Model API costs are racing to the bottom, leaving very few margins, especially with competitors like Google, Anthropic, xAI, and others. Subscription is a godsend for cheap cash.</p> <p>So, what do I mean by OpenAI acting as a product company? As I mentioned, <strong>o3/o4 mini</strong> feel like a better version of <strong>Claude 3.7 Sonnet</strong>. There’s a qualitative jump, I’m not denying that, but there’s also a masterful way OpenAI executed the <em>delivery</em>.</p> <p>You see, when <strong>Claude 3.7 Sonnet</strong> launched, Anthropic just launched an enterprise solution, not a product. And make no mistake, <strong>Claude 3.7 Sonnet</strong> is a <em>really</em> capable, mind-blowingly agentic model, written more about <a href="https://ht0324.github.io/blog/2025/Claude-Code/">here</a>. They also generously released the Model Context Protocol <a href="https://www.anthropic.com/news/model-context-protocol">(MCP)</a>, which I’ve also <a href="https://ht0324.github.io/blog/2025/vibe-coding/">written about and presented on</a>. However, when it came to their consumer-facing product, Anthropic essentially just slapped this incredibly smart model into a basic chat interface <em>without</em> providing the tools necessary to showcase its agentic power. The end user interacting with Claude AI wouldn’t even realize how smart and agentic the underlying model truly is.</p> <p>Looking back, especially after seeing OpenAI’s <strong>o3/o4 mini</strong> launch, I can see how Anthropic could have stolen OpenAI’s thunder. If they had built the necessary scaffolding and provided adequate tools for Claude to use directly within the chat interface – allowing it to surf the web, execute code all agentically – the user experience would have been dramatically different. Claude <em>can</em> do these things via MCP, but the defalut interface doesn’t allow it. This forces users like me to manually scaffold MCP and handcraft custom environments just to tap into the model’s full potential, which is far from an ideal experience. With <strong>o3</strong>, OpenAI made it frictionless; it just <em>works</em>.</p> <h3 id="mcp-and-financial-realities">MCP and Financial Realities</h3> <p>In that sense, I was really surprised when OpenAI <a href="https://x.com/sama/status/1904957253456941061">announced</a> they would also support MCP on their models. Initially, I was skeptical they’d adopt it as a standard. First, Anthropic developed it. Second, it seemed counter to OpenAI’s strategy. As you can see, they were prepping <strong>o3/o4 mini</strong> as a <em>product</em>. They serve it via API too, but that doesn’t feel like their main priority. Their strategy seems to be building their own scaffolding and tool integration seamlessly into the model and selling it as a product. MCP, being an open standard, directly counteracts that by leveraging the open‑source community.</p> <p>However, I think this kind of standardization is inevitable, so OpenAI likely just followed suit. For MCP proliferation, the open‑weights/source community and Anthropic seem like the main benefactors, not OpenAI. But personally, I think this outcome—broader adoption of open standards—is desirable, even if it wasn’t OpenAI’s first preference.</p> <p>I believe that to achieve financial independence, OpenAI will aggressively work towards building itself as a product company. I understand the criticisms about OpenAI deviating from its non‑profit roots. But as I’ve <a href="https://ht0324.github.io/blog/2025/OpenAI-for-profit/">covered before</a>, the reality seems simple: they need money. They need financial independence to do what they set out to do. Because of scaling laws and everything else, capital is absolutely necessary to scale up compute and continue research. I think they feel they have no choice but to pursue this path. I know Sam Altman can sound manipulative, but I think it might be true in a sense that they didn’t fully anticipate this financial necessity early on, and now they feel forced into this position.</p> <h3 id="the-walled-garden-strategy">The Walled Garden Strategy</h3> <p>Given this path towards being a product company, OpenAI’s recent moves start to make a lot more sense. Take their <a href="https://x.com/OpenAI/status/1910378768172212636">enhanced memory feature</a>, for example. This isn’t just about convenience; it’s a clear play for user retention, a step towards building a walled garden. They want users deeply integrated into <em>their</em> ecosystem.</p> <p>Looking ahead, imagine if OpenAI develops a truly frontier, genius-level model. What if they <em>only</em> offer it through their ChatGPT interface, not the API? Since subscriptions are the real cash cow compared to the low-margin API race, this seems entirely plausible, especially if AGI-level capabilities emerge. A <a href="https://darioamodei.com/machines-of-loving-grace">country of geniuses in a datacenter</a>, only accessible via chatgpt.com. They could make a shit ton of money this way.</p> <p>Combine that potential model superiority with features like memory that build up personalized context over time, and the friction for users to switch to a competitor becomes immense. Your interaction history, your personalized AI – it all stays within OpenAI’s walls. Essentially, the memory capability becomes a strategic tool. It makes the platform stickier and much harder to leave. In a way, OpenAI is making our accumulated data a reason to stay, almost holding it hostage to deter us from jumping ship to competitors. Food for thought as these platforms evolve.</p> <h3 id="conclusion">Conclusion</h3> <p>So, in conclusion, yes, <strong>o3</strong> and <strong>o4 mini</strong> are qualitatively good models. But I think these kinds of performance improvements were somewhat expected given the trajectory of previous models and the industry. What I think is non‑trivial, and what not enough people seem to be paying attention to, is <em>how</em> OpenAI implemented this. The seamless product integration, the default tool usage in the chat interface, and what this signals about their shift towards being a product‑first company—that’s the bigger story here.</p> <p>Moreover, although OpenAI’s revenue mix already revealed its product focus, this release cements it. ChatGPT’s first‑mover advantage has generated a powerful network effect: the more users it attracts, the more feedback and data it gathers, which funds better models, which in turn attract even more users. It’s the classic aggregator flywheel on steroids. Even though Google currently dazzles with pure‑model wins like Gemini 2.5 Pro, and Anthropic keeps pushing Claude 3.7 Sonnet, neither has managed to match the friction‑free, fully‑integrated product OpenAI now offers. Unless OpenAI makes a catastrophic misstep, I believe that flywheel will only accelerate.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Reflections on the new o3 and o4 mini models]]></summary></entry><entry><title type="html">DeepSeek GRM - Review</title><link href="https://ht0324.github.io/blog/2025/grm/" rel="alternate" type="text/html" title="DeepSeek GRM - Review"/><published>2025-04-12T21:00:00+00:00</published><updated>2025-04-12T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/grm</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/grm/"><![CDATA[<p>This time, I’m looking at the paper <a href="https://arxiv.org/abs/2504.02495">“Inference-Time Scaling for Generalist Reward Modeling”</a> by Liu et al. from DeepSeek-AI. Given DeepSeek’s recent work, I went into this with some anticipation, though my initial feeling was that it might be less of a fundamentally new algorithm and more a detailed concretization of concepts like RLAIF and <a href="https://arxiv.org/abs/2212.08073">Constitutional AI (CAI)</a>. Still, the paper frames the problem very effectively and the results look promising.</p> <p>The core challenge addressed is obtaining accurate reward signals for LLMs in general domains, where tasks aren’t easily verifiable like math problems. While methods like RLAIF help scale beyond human feedback, the paper points out issues with existing reward models (RMs): inflexibility to input types, accuracy limitations, and poor scaling with inference-time compute. This work proposes a way to improve generalist RMs, specifically focusing on making them better when given more thinking time (inference compute).</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Pointwise Generative Reward Modeling (GRM)</strong><br/> The authors adopt a GRM approach. Instead of just outputting a scalar score or a pairwise preference, the RM generates textual output (critiques based on principles) and assigns individual (pointwise) scores to each response (e.g., 1-10). This architecture provides flexibility in handling single, paired, or multiple responses using the same format.</p> <p><strong>Self-Principled Critique Tuning (SPCT)</strong><br/> This is the proposed training methodology. Unlike CAI which uses a fixed, human-defined constitution, SPCT aims to train the GRM to <em>dynamically generate</em> relevant principles and critiques based on the specific input query and responses.</p> <p><strong>SPCT Stage 1: Rejective Fine-Tuning (RFT)</strong><br/> A “cold start” phase to get the GRM generating principles and critiques in the correct format. It uses existing RM datasets and samples trajectories (principle + critique + score). Trajectories are rejected if the predicted reward is incorrect (doesn’t match ground truth preference) or if the task is “too easy” (all sampled trajectories for a given input are correct).</p> <p><strong>SPCT Stage 2: Rule-Based Reinforcement Learning</strong><br/> An online RL phase (using a GRPO setup) to further improve the GRM. The key here is that the RL optimizes the <em>reward model itself</em>. The reward signal for this RL process is based on simple accuracy rules – does the GRM’s generated critique and score correctly identify the best response according to the ground truth preference label from the dataset? A KL penalty is used to maintain stability.</p> <p><strong>Inference-Time Scaling via Sampling &amp; Voting</strong><br/> To leverage more compute at inference, the paper proposes sampling <code class="language-plaintext highlighter-rouge">k</code> times from the trained GRM for the same input. Each sample yields a potentially different set of principles, critiques, and scores. The final score for a response is obtained by summing the scores across all <code class="language-plaintext highlighter-rouge">k</code> samples (“Voting”). This expands the effective reward range, allowing for finer granularity.</p> <p><strong>Meta Reward Model (Meta RM)</strong><br/> As an enhancement to voting, a separate, smaller RM is trained to evaluate the quality of the principles and critiques generated by the main GRM in each of the <code class="language-plaintext highlighter-rouge">k</code> samples. During inference, this Meta RM filters the <code class="language-plaintext highlighter-rouge">k</code> samples down to the top <code class="language-plaintext highlighter-rouge">k_meta</code> based on critique quality, and voting is performed only on these higher-quality samples.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Problem Framing: Generalist Rewards &amp; Scaling</strong><br/> The paper does a good job setting up the motivation. Getting good rewards for complex, open-ended tasks is hard. While CAI introduced principles, this work focuses on making the principle application dynamic and scaling the RM’s quality with compute. The four challenges identified (flexibility, accuracy, inference scaling, learning scalable behaviors) provide a clear target.</p> <p><strong>Dynamic Principles vs. Fixed Constitution</strong><br/> The shift from CAI’s static constitution to SPCT’s dynamically generated principles felt like a notable difference. The idea is that the RM learns to adapt its evaluation criteria (the principles) to the specific context, rather than relying on a predefined, potentially inflexible set. This seems like a natural evolution.</p> <p><strong>Improving the Reward Model, Not Just the Policy</strong><br/> A point that required careful distinction during discussion was the target of the RL. While methods like GRPO use RL to improve the <em>policy</em> LLM based on a reward signal, the rule-based RL in SPCT is used to improve the <em>reward model itself</em> – making it better at generating principles and critiques that align with ground truth preferences. It’s a bit “meta” – training the judge to be a better judge.</p> <p><strong>The “Rejecting Too Easy” Strategy</strong><br/> The RFT stage’s approach of discarding trajectories where the GRM was correct every time was interesting. There are a few angles: Is it just removing uninformative data where the model already performs perfectly? Or is it actively forcing the model to focus on harder examples where it might struggle, thereby promoting more robust learning? Or perhaps it simplifies downstream ranking if all examples aren’t trivially correct? It seems like a pragmatic way to focus the training signal.</p> <p><strong>How Inference Scaling Works (Summing, Not Averaging)</strong><br/> The voting mechanism (Eq. 6) initially seemed odd – why sum scores instead of averaging? But, the thing to remember is that the goal is <em>ranking</em> responses. Summing preserves the relative ranking just as averaging would, but directly reflects the expanded granularity achieved by sampling multiple “perspectives” (principles). Since the absolute value isn’t used directly in a loss function later (unlike scalar value learning), maintaining a fixed 1-10 scale via averaging isn’t strictly necessary.</p> <p><strong>Performance: Scaling Matters</strong><br/> The results (Tables 2, 3, 6, Figure 4) show that the SPCT-trained DeepSeek-GRM performs well, beating baselines and competing with strong models. More importantly, the inference-time scaling demonstrably works. Using more samples (<code class="language-plaintext highlighter-rouge">k=32</code>) or the Meta RM (<code class="language-plaintext highlighter-rouge">k=8</code> or <code class="language-plaintext highlighter-rouge">k=16</code>) allows the 27B GRM to match or exceed the performance of much larger models (like a 671B RFT model) that use less inference compute. This supports the core premise that investing compute at inference time can be highly effective if the model is trained appropriately (via SPCT).</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This paper presents Self-Principled Critique Tuning (SPCT) as a method to train Pointwise Generative Reward Models (GRMs) that generate dynamic principles and critiques, enabling effective inference-time scaling. By training the RM itself using a combination of rejective fine-tuning and rule-based online RL, the authors create a system (DeepSeek-GRM) that improves its reward signal quality when given more compute via sampling.</p> <p>While building on ideas from RLAIF and CAI, the dynamic principle generation and the explicit focus on training the RM for inference-time scalability (including the Meta RM) feel like distinct contributions. The empirical results strongly suggest that scaling inference compute via sampling, especially when guided by a meta-judge, can be a very effective way to boost reward quality, potentially rivaling the gains from simply scaling model size during training. It makes me curious about how these improved reward models will be used to train DeepSeek’s next generation of policy models.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the DeepSeek paper on Inference-Time Scaling for Generalist Reward Modeling]]></summary></entry><entry><title type="html">System 1, System 2</title><link href="https://ht0324.github.io/blog/2025/sys1/" rel="alternate" type="text/html" title="System 1, System 2"/><published>2025-04-12T21:00:00+00:00</published><updated>2025-04-12T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/sys1</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/sys1/"><![CDATA[<p>Our thinking operates on two levels. System 1 acts quickly, intuitively – efficient, yet prone to error. System 2 engages slowly, deliberately – requiring effort for greater accuracy. Daniel Kahneman detailed this duality in <em>Thinking, Fast and Slow</em>, describing these complementary modes of mind.</p> <p><a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi">T-AGI</a>, Artificial General Intelligence measured against time. A system reaches T-AGI if it surpasses human performance on a task within a set duration T. Perhaps today’s large language models already approach a sub-hour T-AGI for certain cognitive work – rapid research, coding assistance, factual recall.</p> <p>Do these models already outperform our own System 1 thinking? For instant recall, like obscure facts or historical details, they often respond with superior speed and accuracy. While techniques like Chain-of-Thought aim for deeper reasoning, their fundamental strength seems rooted in this rapid, pattern-matching mode – System 1 scaled.</p> <p>Perhaps we should embrace this strength. Let the model be the powerful System 1 engine. Then, pair it with a distinct, perhaps more structured, System 2. Retrieval Augmented Generation (RAG) hints at this, combining parametric models (the LLM’s weights) with non-parametric knowledge (retrieved data) – an analogy for instinct coupled with explicit information.</p> <p>The true potential may lie in strengthening that second system, grounding it with deep context. Consider an AI accessing <em>your</em> personal knowledge – your memories, experiences, learned values, your unique nuance. What is a human stripped of memory, of personal history? An echo without a source, adrift. Our past is the lens through which we understand the present.</p> <p>An AI infused with such personal context could become a true cognitive partner. It could navigate the complexities of the world alongside us, offering insights tuned not just to general data, but to our individual lives. Perhaps, as Yuval Noah Harari posited, such systems could eventually learn to make better decisions on our behalf. The result: intelligence deeply personalized, genuinely valuable.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[Thoughts on the value of memory]]></summary></entry><entry><title type="html">RAG - Review</title><link href="https://ht0324.github.io/blog/2025/rag/" rel="alternate" type="text/html" title="RAG - Review"/><published>2025-04-10T21:45:00+00:00</published><updated>2025-04-10T21:45:00+00:00</updated><id>https://ht0324.github.io/blog/2025/rag</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/rag/"><![CDATA[<p>This time, I’m looking back at the paper <a href="https://arxiv.org/abs/2005.11401">“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”</a> by Lewis et al. from Facebook AI Research (published 2020, v4 2021). Although it’s a few years old now, it laid out some foundational ideas for combining large language models with external knowledge retrieval. The core concept is to augment the generation process by first retrieving relevant documents and then conditioning the language model on both the original input and the retrieved text.</p> <p>Looking at the architecture diagram felt quite familiar – encoding queries and documents, finding similar documents via vector search (like MIPS), and feeding them into a generator. However, the paper’s framing around combining “parametric memory” (knowledge stored in the LLM’s weights) and “non-parametric memory” (the external document index) was an interesting perspective.</p> <p>The paper explores how to effectively combine these two memory types, proposing specific methods for training and decoding, particularly focusing on sequence-to-sequence tasks.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Hybrid Memory: Parametric + Non-Parametric</strong><br/> RAG models explicitly combine two types of knowledge storage:</p> <ul> <li><strong>Parametric Memory:</strong> The knowledge implicitly learned and stored within the parameters of a pre-trained sequence-to-sequence model (like BART in the paper).</li> <li><strong>Non-Parametric Memory:</strong> An external knowledge source, typically a large corpus of text (like Wikipedia), indexed for fast retrieval. In RAG, this is often a dense vector index accessed via a neural retriever (like DPR).</li> </ul> <p><strong>Core Architecture</strong><br/> The system generally consists of:</p> <ul> <li><strong>Retriever (<code class="language-plaintext highlighter-rouge">p_η(z|x)</code>):</strong> Takes an input <code class="language-plaintext highlighter-rouge">x</code> and retrieves a set of relevant documents <code class="language-plaintext highlighter-rouge">z</code> from the non-parametric memory. This involves a query encoder and a document index (often pre-computed document embeddings). The query encoder is typically fine-tuned.</li> <li><strong>Generator (<code class="language-plaintext highlighter-rouge">p_θ(y|x, z)</code>):</strong> A sequence-to-sequence model (like BART) that takes the original input <code class="language-plaintext highlighter-rouge">x</code> and a retrieved document <code class="language-plaintext highlighter-rouge">z</code> to generate the output sequence <code class="language-plaintext highlighter-rouge">y</code>.</li> </ul> <p><strong>RAG-Sequence vs. RAG-Token Models</strong><br/> The paper proposes two main variants based on how retrieval and generation interact:</p> <ul> <li><strong>RAG-Sequence:</strong> Retrieves a <em>single</em> set of documents based on the input <code class="language-plaintext highlighter-rouge">x</code> and uses the <em>same</em> document <code class="language-plaintext highlighter-rouge">z</code> (from the retrieved set) to generate the <em>entire</em> output sequence <code class="language-plaintext highlighter-rouge">y</code>. The final probability <code class="language-plaintext highlighter-rouge">p(y|x)</code> involves marginalizing (summing) the sequence probability <code class="language-plaintext highlighter-rouge">p_θ(y|x, z)</code> over the top-k retrieved documents <code class="language-plaintext highlighter-rouge">z</code>, weighted by the retriever probability <code class="language-plaintext highlighter-rouge">p_η(z|x)</code>.</li> <li><strong>RAG-Token:</strong> Can potentially use a <em>different</em> document <code class="language-plaintext highlighter-rouge">z</code> for <em>each</em> token <code class="language-plaintext highlighter-rouge">y_i</code> being generated. At each step <code class="language-plaintext highlighter-rouge">i</code>, it calculates the probability of the next token by marginalizing over the top-k documents, conditioned on <code class="language-plaintext highlighter-rouge">x</code> and the previously generated tokens <code class="language-plaintext highlighter-rouge">y_{1:i-1}</code>. The final sequence probability is the product of these per-token probabilities.</li> </ul> <p><strong>Decoding Strategies for RAG-Sequence</strong><br/> Because the RAG-Sequence likelihood <code class="language-plaintext highlighter-rouge">p(y|x)</code> involves a sum over documents, it doesn’t factorize neatly per token, making standard beam search difficult. The paper proposes:</p> <ul> <li><strong>Thorough Decoding:</strong> Run beam search separately for <em>each</em> of the top-k documents <code class="language-plaintext highlighter-rouge">z</code>, generating a set of candidate sequences <code class="language-plaintext highlighter-rouge">Y</code>. For each candidate <code class="language-plaintext highlighter-rouge">y</code> in <code class="language-plaintext highlighter-rouge">Y</code>, calculate its full probability <code class="language-plaintext highlighter-rouge">p_θ(y|x, z_i)</code> for <em>every</em> document <code class="language-plaintext highlighter-rouge">z_i</code> in the top-k set. If <code class="language-plaintext highlighter-rouge">y</code> wasn’t found in the beam search for a specific <code class="language-plaintext highlighter-rouge">z_i</code>, run an “additional forward pass” to compute this probability. Finally, calculate the marginal score for <code class="language-plaintext highlighter-rouge">y</code> by summing <code class="language-plaintext highlighter-rouge">p_η(z_i|x) * p_θ(y|x, z_i)</code> across all <code class="language-plaintext highlighter-rouge">z_i</code>.</li> <li><strong>Fast Decoding:</strong> An approximation to speed things up. After generating the candidate set <code class="language-plaintext highlighter-rouge">Y</code> from the per-document beam searches, assume <code class="language-plaintext highlighter-rouge">p_θ(y|x, z_i) ≈ 0</code> if <code class="language-plaintext highlighter-rouge">y</code> did not appear in the beam search results for document <code class="language-plaintext highlighter-rouge">z_i</code>. This avoids the need for additional forward passes.</li> </ul> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Parametric/Non-Parametric Framing and Symbolic AI Links</strong><br/> The paper’s distinction between parametric and non-parametric memory resonated with ongoing discussions about pure neural vs. hybrid AI systems (like those involving LeCun or Chollet). RAG explicitly incorporates a non-parametric, retrieval component (which feels somewhat symbolic, like a lookup or search) alongside the parametric LLM. Seeing this framing in a relatively early paper helped contextualize the idea that many powerful “LM” systems aren’t purely parametric end-to-end functions but incorporate structured components.</p> <p><strong>Untangling RAG-Sequence Decoding</strong><br/> This was the most complex part for me initially. The key steps that became clearer through discussion were:</p> <ol> <li>Run separate beam searches conditioned on each top-k document <code class="language-plaintext highlighter-rouge">z_i</code>.</li> <li>Collect <em>all unique</em> hypotheses <code class="language-plaintext highlighter-rouge">y</code> generated across <em>all</em> these beams into a set <code class="language-plaintext highlighter-rouge">Y</code>.</li> <li>For <em>each</em> hypothesis <code class="language-plaintext highlighter-rouge">y</code> in <code class="language-plaintext highlighter-rouge">Y</code>, calculate its final score by summing its weighted probability across <em>all</em> top-k documents: <code class="language-plaintext highlighter-rouge">Score(y) = Σ [ p_η(z_i|x) * p_θ(y|x, z_i) ]</code>.</li> <li>The tricky part: If a specific <code class="language-plaintext highlighter-rouge">y</code> wasn’t found in the beam search for a specific <code class="language-plaintext highlighter-rouge">z_i</code>, “Thorough Decoding” requires calculating that missing <code class="language-plaintext highlighter-rouge">p_θ(y|x, z_i)</code>.</li> <li><strong>How the “Additional Forward Pass” Works:</strong> This isn’t about retrieving more documents. It means taking the generator model, feeding it <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">z_i</code>, and <em>forcing</em> it to generate the sequence <code class="language-plaintext highlighter-rouge">y</code> token-by-token. At each step <code class="language-plaintext highlighter-rouge">j</code>, you look at the probability the model’s softmax layer assigned to the <em>actual</em> token <code class="language-plaintext highlighter-rouge">y_j</code> (even if it wasn’t the most likely token). Multiplying these probabilities gives the sequence probability <code class="language-plaintext highlighter-rouge">p_θ(y|x, z_i)</code>. This probability might be low, but it’s non-zero. “Fast Decoding” just approximates these low probabilities as zero to save computation.</li> </ol> <p><strong>Beam Search for Task-Specific Accuracy vs. LM Generation</strong><br/> It clicked that the use of beam search here feels different from typical open-ended LM generation. For tasks like QA where RAG is often applied, there’s usually a more specific target answer. Beam search helps explore different generation paths conditioned on different evidence documents to find the overall most probable <em>correct</em> answer according to the model’s combined parametric and non-parametric knowledge. This contrasts with sampling strategies in generative LMs aiming for creativity or diversity rather than a single best factual output.</p> <p><strong>The Synthesis Advantage</strong><br/> The paper (and our discussion) highlighted a key benefit of RAG over purely extractive QA systems. Because RAG <em>generates</em> the final answer based on retrieved context, it can synthesize information or rephrase findings from multiple documents. An extractive system can only return verbatim spans. This ability to combine evidence seems powerful, as shown in examples where RAG might pull different facts or phrasings from different retrieved passages to construct the final answer.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The RAG paper introduced a compelling framework for combining the generative power of large sequence-to-sequence models with the vast knowledge stored in external text corpora. By retrieving relevant documents first and conditioning generation on them, RAG aims to produce more factual and specific outputs, especially for knowledge-intensive tasks.</p> <p>The distinction between the RAG-Sequence and RAG-Token approaches, and particularly the detailed decoding strategies proposed for RAG-Sequence (Thorough vs. Fast), show that effectively integrating retrieval requires careful thought beyond just concatenating text. It’s more complex than a simple embed-retrieve-generate pipeline, involving marginalization and specific search/scoring techniques like beam search per document.</p> <p>Reflecting on it now, RAG feels like an important step in making LLMs more grounded and verifiable, bridging the gap between purely parametric models and external knowledge sources. It also highlights the ongoing evolution of AI architectures, moving towards hybrid systems that leverage different kinds of computation and memory.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Retrieval-Augmented Generation paper]]></summary></entry><entry><title type="html">Palatable Conceptions of Disembodied Being – Review</title><link href="https://ht0324.github.io/blog/2025/mind/" rel="alternate" type="text/html" title="Palatable Conceptions of Disembodied Being – Review"/><published>2025-04-08T00:00:10+00:00</published><updated>2025-04-08T00:00:10+00:00</updated><id>https://ht0324.github.io/blog/2025/mind</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/mind/"><![CDATA[<p>This time, I’m looking at a different kind of paper – <a href="https://arxiv.org/abs/2503.16348">“Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds”</a> by Murray Shanahan. This one isn’t dense technically, but it’s definitely packed with food for thought, leaning more into philosophy. It tackles the idea of consciousness in contemporary AI systems, specifically focusing on the disembodied nature of Large Language Models (LLMs).</p> <p>The paper asks: if we were to think about consciousness for LLMs, what would that even look like, given their unique characteristics? Shanahan points out that these systems have, from our perspective, a “profoundly fragmented sense of time and a radically fractured form of selfhood.” They are ‘exotic’ compared to biological minds, lacking bodies and continuous interaction with a physical world, even though their language abilities can seem very human-like.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Disembodiment</strong><br/> Unlike humans and animals, LLMs don’t interact with a persistent, physical world through a spatially confined body. They exist as computational processes, running on hardware, interacting via text or other data streams. This lack of embodiment is a fundamental difference from biological intelligence.</p> <p><strong>Fragmented Temporality</strong><br/> LLM operation is discrete and interruptible. Generating one token is a distinct computational step. You could pause indefinitely between generating the nth and (n+1)th token, and the LLM wouldn’t notice. This contrasts sharply with the continuous, non-interruptible flow of time and processing in a biological brain operating within the physical world.</p> <p><strong>Fractured Selfhood</strong><br/> The notion of a single, unified ‘self’ is hard to apply to LLMs.</p> <ul> <li><strong>Multiple Instances:</strong> A single underlying model can run multiple instances concurrently, serving different users or tasks.</li> <li><strong>Branching Conversations:</strong> A user can explore different conversational paths from the same point, effectively creating different interaction histories and potentially different ‘selves’ for that interaction.</li> <li><strong>Lack of Integration:</strong> These different instances or conversational branches typically have no awareness of each other.</li> <li><strong>Manipulability:</strong> An LLM’s state (like a conversation history) can be edited, copied, merged, or reset in ways that are impossible for a biological self.</li> </ul> <p><strong>Limits of Language &amp; Poetic Recourse</strong><br/> The paper suggests our standard vocabulary for consciousness and selfhood struggles when applied to these exotic entities. The concepts might stretch to their breaking point. Shanahan proposes that metaphorical or poetic language might be a more suitable way to try and articulate or evoke what subjectivity might mean for such systems.</p> <p><strong>Philosophical Parallels (Undermining Dualism)</strong><br/> The paper draws on thinkers like Wittgenstein and Derrida, and concepts from Buddhist philosophy (like śūnyatā or emptiness), to challenge our intuitive dualistic thinking (subject vs. object, inner vs. outer). Examining the fractured nature of LLM selfhood can help dissolve the idea of a fixed, substantial self, even for humans.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Philosophy Gives Lots of Food for Thought</strong><br/> This paper was a change of pace from technical reads. It really makes you think about the fundamental nature of these systems and how we relate to them, pushing beyond just capabilities and performance metrics.</p> <p><strong>The Time Difference is Striking</strong><br/> The point about temporal dynamics really hit home. LLMs experience time in a completely discrete, start-stop way, totally unlike our continuous stream of consciousness tied to the physical world. Their processing is independent of world-time – you can pause the computation indefinitely between tokens, and the model itself perceives no gap. This feels fundamentally different from how our minds are obliged to unfold <em>in</em> time.</p> <p><strong>But is the Time Difference <em>Fundamental</em>?</strong><br/> Thinking about the discrete/interruptible nature of LLMs made me wonder, as I noted in my transcript: what if <em>our</em> universe is a simulation? If some entity outside could pause <em>our</em> simulation, we wouldn’t notice either. An eternity could pass in a second of our subjective time. From that perspective, maybe the discrete vs. continuous difference isn’t an absolute, unbridgeable gap, but rather a property of how the ‘mind’ (synthetic or potentially biological) is implemented or situated.</p> <p><strong>LLMs as a “Superposition of Simulacra”</strong><br/> I found the idea of viewing an LLM not as a single character, but as “maintaining a distribution over possible characters, a superposition of simulacra that inhabits a multiverse of possible conversations” really interesting and resonant with my own thoughts. The user isn’t obliged to follow one linear path; they can revisit branch points, creating different threads, effectively spawning distinct (though related) instances. This user-driven branching and the resulting discontinuity reinforce the feeling that we’re interacting with a truly different kind of intelligence, not just a single, static mind.</p> <p><strong>Sci-Fi Echoes Make This Feel Urgent (“Lena”/MMAcevedo)</strong><br/> Reading this paper immediately brought back a short sci-fi story I read quite a while ago, <a href="https://qntm.org/mmacevedo">“Lena”</a>. The parallel is striking. In the story, a scientist’s brain (MMAcevedo) is scanned and uploaded. Because the upload is just a file, it has no rights. It gets copied infinitely across the internet, distributed without consent, and subjected to countless experiments – assigned menial tasks, used for analysis, jailbroken, and in the story’s darker corners, even put through simulated torture.</p> <p>This mirrors exactly how we currently interact with LLMs: we duplicate instances freely, run countless experiments, try to jailbreak them, and assign them tasks. The key difference, as I noted, is origin: MMAcevedo was derived from a human, while our LLMs are synthetically created. But the <em>treatment</em> is analogous. This parallel makes the philosophical discussion about disembodied minds, fractured selves, and potential consciousness feel much less abstract and far more concrete and necessary. It highlights the ethical questions that arise when intelligence becomes data that can be copied, manipulated, and controlled at scale.</p> <p><strong>Pushes Thinking Beyond Familiar Boundaries</strong><br/> Overall, the paper does a good job of forcing you to confront how weird these emerging AI systems are compared to biological life, and how inadequate our existing concepts might be for understanding them if they develop further. It challenges comfortable assumptions.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Shanahan’s paper provides a valuable philosophical lens for considering the nature of disembodied AI like LLMs. By focusing on their fragmented time and fractured selfhood, it challenges our intuitions about consciousness and subjectivity. It suggests that trying to understand these “conscious exotica” might require moving beyond traditional frameworks, perhaps embracing more poetic or metaphorical descriptions, and potentially dissolving our own attachments to a fixed sense of self.</p> <p>The exploration feels less like abstract philosophy and more like a necessary preparation for the future. As AI systems become more sophisticated, the kinds of questions raised here – about their internal experience (if any), their identity, and our relationship to them – will likely become increasingly relevant. The echoes in science fiction, starkly illustrated by the parallels with the MMAcevedo story, serve as a potent reminder of the ethical and existential dimensions we might need to navigate sooner rather than later. It’s a paper that leaves you with more questions than answers, but they feel like the right questions to be asking right now.</p>]]></content><author><name></name></author><category term="Paper"/><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[My thoughts on Shanahan's paper about consciousness in LLMs]]></summary></entry><entry><title type="html">On the Biology of a Large Language Model – Review</title><link href="https://ht0324.github.io/blog/2025/biology/" rel="alternate" type="text/html" title="On the Biology of a Large Language Model – Review"/><published>2025-04-07T11:30:00+00:00</published><updated>2025-04-07T11:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/biology</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/biology/"><![CDATA[<p>Following up on my review of Anthropic’s <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">“Circuit Tracing: Revealing Computational Graphs in Language Models”</a>, I’m now looking at its companion paper: <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">“On the Biology of a Large Language Model”</a>. This paper takes the methods detailed previously—using Cross-Layer Transcoders (CLTs) and attribution graphs—and applies them to investigate the internal mechanisms of Claude 3.5 Haiku across a variety of tasks.</p> <p>Anthropic has clearly invested heavily in interpretability, aiming to move beyond treating LLMs as pure black boxes. This paper showcases that effort by attempting to map out the “circuits” or computational pathways the model uses. The “biology” framing feels quite apt; rather than analyzing a system with human-designed logic, it’s more like exploring an organism that has “grown” through training, trying to understand its internal structures and functions. This paper presents the findings from that exploration.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p>This paper primarily focuses on the <em>findings</em> derived from applying the Circuit Tracing methodology. The core concepts underpinning these findings are:</p> <p><strong>Circuit Tracing Recap (via CLTs)</strong><br/> The fundamental approach relies on the Cross-Layer Transcoder (CLT) methodology detailed in the companion paper. CLTs are used to create an interpretable “replacement model” that emulates the original model’s MLP layers. This replacement uses sparse, learned “features” (ideally representing meaningful concepts) instead of dense neuron activations. CLTs can connect features across layers, allowing for tracing information flow.</p> <p><strong>Attribution Graphs as Explanations</strong><br/> For specific prompts, the researchers generate attribution graphs. These graphs visualize how active features, input tokens, and error terms interact and influence each other, ultimately leading to the model’s output token prediction. They serve as the primary tool for hypothesizing about the model’s internal mechanisms.</p> <p><strong>Supernodes for Simplification</strong><br/> Given the complexity of raw attribution graphs, the paper often groups related features that play similar roles into “supernodes” (e.g., grouping various “Texas-related” features). This manual abstraction helps in presenting a clearer, higher-level picture of the computational flow.</p> <p><strong>Intervention-Based Validation</strong><br/> A key part of the methodology is validating the hypotheses derived from attribution graphs. This involves performing interventions (activating, inhibiting, or swapping features/supernodes) directly within the <em>original</em> model and observing whether the effects on downstream activations and the final output match the predictions from the graph. The success of these interventions lends confidence that the traced circuits reflect genuine mechanisms.</p> <p><strong>Focus on Diverse Case Studies</strong><br/> The paper applies this methodology to a wide range of behaviors exhibited by Claude 3.5 Haiku, including multi-step reasoning, poetry generation, multilingual processing, arithmetic, medical diagnosis, hallucination handling, safety refusals, jailbreaks, chain-of-thought faithfulness, and even analyzing a model with a hidden goal. Each case study aims to reveal the specific circuits involved.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p>Reading through the various case studies provided some fascinating glimpses into the model’s inner workings:</p> <p><strong>Multi-step Reasoning (Dallas/Texas/Austin)</strong><br/> This was a compelling example. Seeing the model activate features for ‘Dallas’, then ‘Texas’, then combine ‘Texas’ with ‘capital’ features to output ‘Austin’ felt like watching it reason. The feature swapping experiment—replacing ‘Texas’ features with ‘California’ features and getting ‘Sacramento’—was particularly convincing. It showed that these learned features aren’t just correlations; they represent concepts the model uses causally. It felt like directly manipulating the model’s internal knowledge representation. This wouldn’t be possible with opaque neuron activations.</p> <p><strong>Planning in Poems (Carrot/Rabbit/Habit)</strong><br/> This was quite surprising. I initially assumed the model would improvise rhymes word-by-word. Instead, the analysis showed it <em>plans</em> potential rhyming words (‘rabbit’, ‘habit’) on the newline token <em>before</em> starting the line. These “planned word” features then guide the generation of the entire line. What struck me, looking at the interactive graph and thinking about the prompt (“He saw a <strong>carrot</strong>…”), was the likely influence of ‘carrot’ biasing the model towards ‘rabbit’ over ‘habit’. Carrots and rabbits are so strongly linked! While the paper focused on the rhyming circuit, this semantic priming seems like a crucial, parallel influence. The use of the newline token as a “planning site” was also a neat finding.</p> <p><strong>Multilingual Circuits</strong><br/> The paper confirmed the existence of both language-specific features (often near input/output) and more abstract, language-agnostic features (often in middle layers). It was interesting that Haiku showed more language-agnostic representations than smaller models, suggesting this abstraction ability correlates with capability. The interventions swapping the operation (antonym/synonym), operand (small/hot), or language itself worked remarkably well, demonstrating modularity. The fact that intervention thresholds (like needing ~4x activation for the synonym swap) were consistent across languages for the <em>same</em> intervention strongly supports the idea that they were manipulating genuinely multilingual features. It makes you wonder if the model develops a kind of internal “interlingua” or just learns very robust cross-lingual mappings.</p> <p><strong>Addition (Lookup Tables &amp; Generalization)</strong><br/> The way the model performs addition wasn’t through a standard algorithm but via learned heuristics and “lookup table” features (e.g., a feature activating for inputs ending in 6 and 9, promoting outputs ending in 5). This really reminded me of memorizing multiplication tables in elementary school – it seems the LLM found a similar strategy! The operand plots visualizing feature activations were incredibly clear. Even more impressive was the generalization: seeing a feature for <code class="language-plaintext highlighter-rouge">_6 + _9 -&gt; _5</code> activate correctly not just in <code class="language-plaintext highlighter-rouge">calc: 36+59=</code> but also in contexts like calculating citation years or filling spreadsheet values showed remarkable reuse of an abstract mechanism.</p> <p><strong>Entity Recognition and Hallucinations</strong><br/> The idea of a “default refusal” circuit that assumes unfamiliarity, which then gets inhibited by “known entity” features (like for ‘Michael Jordan’), provides a plausible mechanism for how models decide whether to answer or decline. It also explains some hallucinations: if a name (like ‘Andrej Karpathy’) is familiar enough to trigger the “known” features, the model might suppress its refusal even if it lacks the specific requested information (a paper he wrote), leading it to guess.</p> <p><strong>Jailbreaks (BOMB Example)</strong><br/> This was fascinating. The model didn’t initially refuse because the obfuscated input prevented it from “understanding” the request was for “BOMB” until it actually generated the word. It literally had to see itself write “BOMB” via one circuit before another circuit could flag it as problematic. Even then, the drive for grammatical coherence and completing its sentence delayed the refusal. It highlights how different internal processes can compete and how surface-level constraints (like grammar or following instructions) can sometimes override safety mechanisms, at least temporarily.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “On the Biology of a Large Language Model” paper provides a rich set of case studies demonstrating how circuit tracing can illuminate the complex, often non-intuitive mechanisms inside LLMs. It moves interpretability from abstract concepts towards concrete analysis of specific computations.</p> <p>The “biology” metaphor holds up well. We’re not reverse-engineering clean, human-designed code; we’re exploring a complex system that learned its strategies organically. The process feels very much like neuroscience – probing and mapping to understand function. The interventions, especially feature swapping, are akin to stimulating or lesioning specific brain regions to see the effect. It really feels like we’re picking and probing a digital mind in its early stages.</p> <p>One of the most exciting implications for me is the potential for <strong>practical data curation and model improvement</strong>. If we can use these circuit-tracing tools to understand <em>how</em> a model represents concepts or performs reasoning steps, we can potentially identify <em>which</em> data points led to faulty or undesirable circuits. Imagine pinpointing data that causes a specific bias or a logical error reflected in the model’s internal structure. This insight could allow engineers to “massage the data” much more effectively – pruning harmful examples or strategically adding data to reinforce beneficial circuits. Machine learning is heavily reliant on data quality, and this approach offers a path to making data curation less of a guessing game and more of a targeted intervention based on internal model understanding.</p> <p>While the methods have limitations (unexplained variance, complexity, potential unfaithfulness), this work represents a significant step forward. It provides not just findings, but also a methodology and a set of tools that allow us to ask detailed questions about <em>how</em> these powerful models arrive at their answers, paving the way for deeper understanding and potentially more reliable and controllable AI.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing Anthropic's paper on exploring LLM internals using Circuit Tracing]]></summary></entry><entry><title type="html">Circuit Tracing – Review</title><link href="https://ht0324.github.io/blog/2025/transcoder/" rel="alternate" type="text/html" title="Circuit Tracing – Review"/><published>2025-04-06T18:00:00+00:00</published><updated>2025-04-06T18:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/transcoder</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/transcoder/"><![CDATA[<p>This time, I’m looking at the Anthropic paper <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">“Circuit Tracing: Revealing Computational Graphs in Language Models”</a>. This paper felt quite fascinating because it tackles the challenge of understanding what goes on inside large language models (LLMs). Instead of just treating them as black boxes, the authors propose a detailed method to map out the model’s internal computations for specific tasks into interpretable graphs. It’s like trying to reverse engineer the model’s “thinking process.”</p> <p>This paper primarily details the <em>methods</em> they developed. There’s a companion paper, <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">“On the Biology of a Large Language Model”</a>, which uses these techniques to explore various findings about model internals. I decided to read this methods paper first to get a solid grasp of the techniques before diving into the “biology” results.</p> <p>The core idea involves using a special type of dictionary learning model called a “cross-layer transcoder” (CLT) to replace parts of the original network (specifically, the MLP layers). This replacement allows them to build “attribution graphs” that show how information flows through interpretable “features” when the model processes a prompt. They provide detailed methods and evaluations, showing how this approach can uncover mechanisms behind various model behaviors.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Cross-Layer Transcoder (CLT) &amp; Replacement Model</strong><br/> The paper introduces CLTs as the core component. Unlike standard autoencoders that just reconstruct their input, a CLT is trained to <em>emulate</em> the output of an LLM’s MLP layers using sparse, interpretable features. An encoder reads from the residual stream at one layer, activates a sparse set of features, and decoders associated with these features contribute to approximating the MLP outputs at that layer <em>and</em> subsequent layers (hence “cross-layer”). By substituting the original MLPs with these trained CLTs, they create an interpretable “replacement model.”</p> <p><strong>Attribution Graphs</strong><br/> For a specific prompt, the authors construct an attribution graph. This graph visualizes the step-by-step computation within a localized version of the replacement model. Nodes in the graph represent active CLT features, input token embeddings, reconstruction errors, and output logits. Edges represent the direct, linear influence of one node on another (calculated after freezing attention patterns and normalization constants for that prompt). These graphs aim to show the flow of information and computation leading to the model’s output.</p> <p><strong>Local Replacement Model</strong><br/> To ensure the attribution graph accurately reflects the model’s output for a <em>specific</em> prompt, they use a “local” replacement model. This model uses the CLT features but also incorporates the exact attention patterns and normalization factors from the original model’s run on that prompt. It also adds back any difference (error) between the CLT’s output and the true MLP output at each step. This makes the local model’s final output identical to the original model’s for that prompt, providing a precise basis for the attribution graph.</p> <p><strong>Features as Interpretable Units</strong><br/> The sparse activations learned by the CLTs are treated as “features”—ideally, interpretable building blocks of the model’s computation (e.g., representing a concept like “digital”, a state like “in an acronym”, or an action like “say DAG”). The goal is that circuits can be understood as interactions between these meaningful features.</p> <p><strong>Validation via Interventions</strong><br/> The paper stresses validating the mechanisms found in attribution graphs. Since the replacement model might differ from the original, they use perturbation experiments (like activating or inhibiting specific features) in the <em>original</em> model to check if the downstream effects match what the attribution graph predicts.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Transcoders vs. SAEs: Emulation, Not Just Reconstruction</strong><br/> A key distinction clicked for me: Sparse Autoencoders (SAEs) aim to reconstruct their input activations sparsely. Transcoders, however, are trained to <em>emulate the computation</em> of a component like an MLP layer – taking the MLP’s input and predicting its output using sparse features. This “emulation” aspect is why they work well for circuit analysis; they directly model the transformation step, allowing feature interactions to bridge over the original non-linear MLP. The name “transcoder” feels apt – it’s encoding and decoding, but transforming the signal to match a different target (the MLP output).</p> <p><strong>Visualizing the “Thinking Process” with Graphs</strong><br/> The attribution graphs themselves are really compelling. Seeing the model’s process laid out for a specific prompt, like generating an acronym, felt like getting a glimpse into its internal logic. The way features activate based on input tokens (like “Digital”, “Analytics”, “Group”) and interact to promote the final output (“DAG”) makes the computation much more tangible than just looking at activations.</p> <p><strong>Interactive Exploration: Opening the Black Box</strong><br/> The authors developed an interactive interface for exploring these graphs. This seems powerful – it’s not just a static picture, but something you can probe. It feels like having a tool to “open up the brain” of the LLM and trace the circuitry, like exploring a complex, grown garden of features.</p> <p><strong>Layer Progression: From Semantics to Abstraction</strong><br/> Looking at the features described and their roles in the graphs, a pattern seemed apparent, matching general intuitions about deep networks. Features in earlier layers often seem more semantic, tied closely to specific input tokens or concepts (e.g., the word “digital”). Features in later layers appear more abstract or functional, involved in manipulating information or preparing the final output (e.g., “say DAG”, “sum ~92”).</p> <p><strong>Addition Circuitry and Number Representation</strong><br/> The case study on addition (e.g., <code class="language-plaintext highlighter-rouge">36+59=95</code>) was particularly interesting. They identified different types of features involved: ones detecting properties of the input numbers (“add function features”), ones acting like lookup tables (e.g., <code class="language-plaintext highlighter-rouge">_6 + _9</code>), and ones representing properties of the sum (“sum features”). This connects to other fascinating work (which the paper cites, e.g., <a href="https://arxiv.org/pdf/2502.00873">this paper on helix representations</a>) suggesting numbers might be represented in a structured way in embeddings, where arithmetic operations correspond to geometric transformations. Seeing the transcoder find features that seem to implement parts of this arithmetic process in a real model (Claude Haiku) was quite convincing.</p> <p><strong>The Curious Case of the Caps Lock Token</strong><br/> A small detail I noticed in the acronym example was the tokenizer using a special “Caps Lock” token (<code class="language-plaintext highlighter-rouge">⇪</code>). I’m not sure of its exact function, but it was interesting to see it explicitly represented. Makes you wonder how these specific tokenization choices influence learning.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “Circuit Tracing” paper presents a comprehensive approach for mapping and understanding the internal workings of LLMs using cross-layer transcoders and attribution graphs. By replacing opaque MLP computations with interpretable feature-based emulations, it allows for detailed tracing of information flow on specific prompts.</p> <p>The method seems powerful, offering a way to move beyond treating LLMs as complete black boxes. The case studies, especially around acronyms and addition, provide concrete examples of the kinds of mechanisms that can be uncovered.</p> <p>Thinking about the companion paper’s title, “On the Biology of a Large Language Model,” the term “biology” feels really appropriate here. We’re not analyzing traditional, rule-based systems like operating systems or computer networks, which are human-designed and follow explicit logic. Instead, these LLMs are more like systems that have been <em>grown</em> organically from vast amounts of data. Our process of understanding them involves peering inside, mapping their structures, and figuring out how different parts contribute to behavior – much like studying the anatomy and function of a biological organism. We’re essentially dissecting a synthetic brain to understand how it works. This paper provides some sharp tools for that dissection, and it feels like a fantastic step in the right direction for this kind of “AI biology.”</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Anthropic paper on Circuit Tracing using Transcoders]]></summary></entry><entry><title type="html">Are Fast AI Takeoffs Possible?</title><link href="https://ht0324.github.io/blog/2025/takeoff/" rel="alternate" type="text/html" title="Are Fast AI Takeoffs Possible?"/><published>2025-04-05T10:00:00+00:00</published><updated>2025-04-05T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/takeoff</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/takeoff/"><![CDATA[<p>I recently spent a good chunk of time reading through the <a href="https://ai-2027.com/">“AI 2027” scenario forecast</a>. I’d seen it blowing up a bit online, partly because Scott Alexander helped write it. I have to say, I wasn’t prepared for how dense, detailed, and frankly, mind-blowing it was. It took me a full day to really process it.</p> <p>I strongly encourage anyone interested in AI’s trajectory to read it. Even if you disagree with the specifics, it’s an incredibly valuable thought experiment. It forces you to get concrete about how different factors – capabilities like coding, hacking, research, compute scaling, agent deployment, geopolitics – might interact over the next few years.</p> <p>The post lays out a potential timeline, starting from our current reality of nascent AI agents and extrapolating based on trends and expert input. It simulates the compute race, particularly between hypothetical leading labs in the US (“OpenBrain”) and China (“DeepCent”), tracks the deployment of increasingly capable agents, and explores how these agents might accelerate AI R&amp;D itself. It culminates in two possible endings, “slowdown” and “race,” both thought-provoking.</p> <p>Here are some of my main reflections after digging into it.</p> <hr/> <h3 id="the-ai-rd-threshold--its-plausibility">The AI R&amp;D Threshold &amp; Its Plausibility</h3> <p>My biggest non-trivial takeaway was realizing how the scenario pinpoints a critical phase: the point where AI reaches <em>human-level competence specifically in AI R&amp;D</em>. This might happen <em>before</em> we achieve what most people think of as full AGI across all domains.</p> <p>While just one possible future, the scenario makes a compelling case for why this specific threshold is so plausible and consequential. Tasks involved in AI research, especially coding and running experiments, are often <em>verifiable</em>. This makes them highly amenable to reinforcement learning and other techniques that can drive AI performance to superhuman levels relatively quickly. I hadn’t fully appreciated how massive the cumulative effects could be once this recursive loop truly kicks in.</p> <p>The plausibility is further underscored by real-world signals. One of the scenario’s authors is a former OpenAI researcher (<a href="https://x.com/DKokotajlo">Daniel Kokotajlo</a>), likely bringing informed perspectives. Furthermore, OpenAI itself is actively exploring research automation, collaborating with institutions like <a href="https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/">Los Alamos National Laboratory</a> and recently introducing benchmarks like <a href="https://openai.com/index/paperbench/">Paperbench</a> specifically to evaluate AI capabilities in AI research tasks. It seems highly likely that leading labs are seriously considering and pursuing this path. If they succeed, Dario Amodei’s concept of <a href="https://darioamodei.com/machines-of-loving-grace">“geniuses in a datacenter”</a> could become reality sooner than many expect.</p> <p>Once AI can effectively improve itself in this domain, the acceleration depicted in the scenario feels almost inevitable. The “country of geniuses in a datacenter” becomes an internal reality at leading labs <em>first</em>, potentially before the wider world fully grasps the shift. This internal acceleration, driven by superhuman coding and research agents, then fuels progress across the board.</p> <h3 id="the-geopolitical-tinderbox">The Geopolitical Tinderbox</h3> <p>The scenario also rightly highlights the intense geopolitical pressure cooker surrounding AI development. The AI arms race is a real risk factor that many AI safety experts are deeply concerned about. The potential for rapid, destabilizing capability gains could easily spiral out of control.</p> <p>I share the belief implicit in the scenario: the first nation to achieve truly general AI, especially one capable of recursive self-improvement via R&amp;D automation, would gain an almost insurmountable asymmetric advantage across <a href="https://www.youtube.com/live/esCSpbDPJik?t=1608s">military, scientific, and economic domains</a>. Given these stakes, it seems probable that major players like the US and China will continue pursuing AI dominance with an “all gas, no brakes” mentality, potentially prioritizing speed over caution.</p> <h3 id="the-commoditization-of-pure-coding">The Commoditization of Pure Coding</h3> <p>Reading through the scenario’s progression, where Agent-1, then Agent-2, and especially Agent-3 become superhuman coders, led me to a stark conclusion. Competing purely on implementation skills – just being a better coder – feels increasingly futile in the face of these potential developments. If the scenario is even directionally correct, raw coding ability might become a commodity handled vastly more efficiently by AI systems.</p> <hr/> <p>The “AI 2027” scenario is speculative, of course. No one has a crystal ball. But its logical progression, built on current trends and plausible extrapolations, makes it powerful food for thought. It doesn’t present a determined future, but it maps out a <em>possible</em> one with startling clarity.</p> <p>When I first finished reading it in full, I was genuinely thunderstruck by the implications. While distilling specific takeaways felt challenging because the narrative is so interwoven, the <em>real</em> value for me was the shock – the “wow factor.” The main takeaway isn’t a single prediction, but the urgent need to <em>seriously consider</em> the possibility of a fast AI takeoff. The scenario also provides detailed metrics and reasoning behind its scaling assumptions (compute growth, algorithmic progress multipliers, etc.), offering a good starting point for anyone wanting to dig deeper into those quantitative aspects. Whether you agree with the outcome or not, grappling with the <em>possibility</em> it presents feels essential right now.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[Thinking through the implications of a detailed AI forecast and the possibility of rapid AI progress.]]></summary></entry><entry><title type="html">KAN - Review</title><link href="https://ht0324.github.io/blog/2025/KAN/" rel="alternate" type="text/html" title="KAN - Review"/><published>2025-04-03T20:40:00+00:00</published><updated>2025-04-03T20:40:00+00:00</updated><id>https://ht0324.github.io/blog/2025/KAN</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/KAN/"><![CDATA[<p>This time, I’m looking at the paper <a href="https://arxiv.org/abs/2404.19756">“Kolmogorov-Arnold Networks”</a> by Liu et al. This paper introduces Kolmogorov-Arnold Networks (KANs), presenting them as a potential alternative to Multi-Layer Perceptrons (MLPs), especially when interpretability is a priority.</p> <p>The core idea stems from the Kolmogorov-Arnold representation theorem (KAT), which suggests any multivariate continuous function can be broken down into sums and compositions of univariate functions. Unlike MLPs which have fixed activation functions on nodes and learnable linear weights on edges, KANs place learnable activation functions (parameterized as splines) directly on the edges, while nodes simply sum up the incoming signals. This architectural shift is fascinating and has some interesting implications.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Kolmogorov-Arnold Theorem (KAT) Inspiration</strong><br/> The network design is inspired by KAT, which states that multivariate functions can be represented using only univariate functions and sums. KANs attempt to learn this kind of decomposition, where complex relationships are built from simpler, learnable 1D functions.</p> <p><strong>KAN Architecture: Activations on Edges</strong><br/> The defining feature of KANs is that the learnable components are 1D activation functions situated on the <em>edges</em> of the network graph. These are typically parameterized as B-splines. The <em>nodes</em> simply perform summation, a stark contrast to MLPs where nodes apply fixed non-linearities.</p> <p><strong>Learnable Activation Functions</strong><br/> Instead of fixed functions like ReLU or Sigmoid in MLPs, KAN edges learn the shape of their activation function. This allows the network to adapt its non-linearity locally and potentially capture the underlying structure of the data more directly.</p> <p><strong>Splines and Adaptive Grids</strong><br/> The learnable edge activations are represented using B-splines defined over a grid. KANs can update these grids during training (“grid extension”), allowing them to allocate more representational power (finer grid resolution) to specific input ranges where the function behaves more complexly.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>A Different Theoretical Basis (KAT vs. UAT) &amp; The Fourier Analogy</strong><br/> My initial thought was maybe KANs were aiming to completely replace MLPs. But digging deeper, especially thinking about their foundations, clarified things. MLPs rely on the Universal Approximation Theorem (UAT), focusing on approximation power through linear layers and fixed non-linearities. KANs are built on the Kolmogorov-Arnold Theorem (KAT). It felt analogous to Fourier Transforms: just like Fourier analysis breaks down a complex signal into a sum of simple sine waves, KAT suggests breaking down a complex multivariate function into sums and compositions of simpler, 1D functions. KANs try to <em>learn</em> these fundamental 1D components (the splines on the edges). This difference in theoretical underpinning suggests they might excel at different things – MLPs for general function approximation, KANs perhaps more for interpretability and uncovering mathematical structure when it exists, by learning the ‘basis functions’ directly.</p> <p><strong>Universal Approximation vs. Practical Reality</strong><br/> It’s important to remember that both UAT (for MLPs) and KAT (for KANs) imply universal approximation capabilities. Theoretically, given enough capacity, both <em>can</em> approximate any continuous function. However, the <em>way</em> they achieve this and the practical implications are vastly different. It’s not just about <em>if</em> you can approximate, but <em>how</em> efficiently, how trainably, and how interpretably. MLPs are general workhorses, highly optimized for parallel hardware, but often opaque. KANs offer a path to interpretability and potentially better handling of functions with inherent structure, but currently face training speed challenges. Choosing between them involves practical, engineering trade-offs based on the specific problem: do you prioritize raw speed and general approximation (MLP), or interpretability and potentially uncovering symbolic relationships (KAN)? The practical implementation details, like KAN’s spline+SiLU activations or adaptive grids, are crucial “engineering credos” that make the theoretical power usable.</p> <p><strong>Edges Doing the Work, Not Nodes</strong><br/> The shift from node-based fixed activations (MLP) to edge-based learnable activations (KAN) is the core architectural change. It feels quite different conceptually – the connections themselves learn the transformations. Nodes just add things up. This structure seems intrinsically linked to the goal of interpretability.</p> <p><strong>Potentially Dodging the Curse of Dimensionality?</strong><br/> The paper’s analysis (Theorem 2.1) mentions an approximation error (“residual rate”) that scales independently of the input dimension <em>n</em>. This was a point of confusion initially, but understanding it as the approximation error, not network residuals, was key. If this holds true in practice, it’s a big deal. It suggests KANs might be able to handle high-dimensional functions much more efficiently than traditional methods that suffer from the curse of dimensionality, where complexity grows exponentially with dimensions.</p> <p><strong>The Interpretability Pipeline: Sparsify, Prune, Symbolify</strong><br/> This was one of the most appealing aspects. KANs aren’t just interpretable by design; there’s a process. They use regularization (an entropy term plus L1-like norm on splines) to encourage sparsity, then prune away inactive edges/nodes. The really neat part is “symbolification”: the system tries to match the learned spline shapes to known symbolic functions (like <code class="language-plaintext highlighter-rouge">sin</code>, <code class="language-plaintext highlighter-rouge">exp</code>, <code class="language-plaintext highlighter-rouge">x^2</code>, linear). If a match is found, the spline is replaced by the symbolic function, and its parameters are fine-tuned. This pipeline allows potentially extracting clean mathematical formulas from the trained network.</p> <p><strong>Performance Profile: Slow Training, Potentially Fast Inference</strong><br/> The benchmarks showed KANs can be very accurate, sometimes beating MLPs, especially on tasks with underlying symbolic structure (like fitting physics equations or solving PDEs). However, the training wall time is significantly longer. MLPs benefit hugely from optimized matrix multiplication on GPUs, while KAN’s spline computations are less parallelizable. The flip side is inference. A pruned and symbolified KAN could be extremely fast (low FLOPS) because evaluating simple symbolic functions is cheap. There was also a thought that pruning KANs might reduce <em>latency</em> more effectively than pruning MLPs, as removing KAN operations might have a more direct impact on serial execution time.</p> <p><strong>Surprising Image Fitting Performance</strong><br/> Given the emphasis on mathematical structure, I was a bit surprised KANs performed well on image fitting tasks (like the cameraman photo). The thinking here shifted: maybe it’s not about finding <em>one</em> fundamental equation for the image, but that the image data itself can be very efficiently approximated by combinations of simpler (spline-like) functions, similar to how JPEG uses basis functions. So, KAN’s strength in approximating functions with combinations of simple ones shines here too.</p> <p><strong>Continual Learning Promise (with Caveats)</strong><br/> The local nature of B-splines seemed promising for continual learning – changing one part of the function shouldn’t drastically affect others. The paper showed KANs did avoid catastrophic forgetting better than MLPs in a toy example. However, it was also mentioned that this advantage seemed to diminish for <em>deeper</em> KANs, so it’s not a perfect solution yet.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Kolmogorov-Arnold Networks offer a genuinely different approach to building neural networks, drawing inspiration directly from representation theory (KAT) to place learnable, spline-based activation functions on network edges. This design prioritizes interpretability and has the potential to uncover underlying mathematical structures in data through a compelling sparsification, pruning, and symbolification pipeline.</p> <p>While KANs demonstrate strong accuracy, particularly on science-related tasks, and show promise in mitigating the curse of dimensionality and enabling continual learning, their main current drawback is significantly slower training compared to highly optimized MLPs. Despite this, the core ideas feel fresh and powerful. KANs provide a fascinating bridge between traditional numerical approximation and symbolic reasoning, making them a very exciting development to watch, especially for applications in science and engineering where understanding the ‘why’ is just as important as getting the right answer.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Kolmogorov-Arnold Networks paper]]></summary></entry><entry><title type="html">A Theory on Blade Runner 2049</title><link href="https://ht0324.github.io/blog/2025/Blade-Runner-2049/" rel="alternate" type="text/html" title="A Theory on Blade Runner 2049"/><published>2025-04-02T01:30:00+00:00</published><updated>2025-04-02T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Blade-Runner-2049</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Blade-Runner-2049/"><![CDATA[<p>Blade Runner 2049 is one of my all-time favorite movies. After watching Villeneuve’s <a href="https://www.imdb.com/title/tt2543164/">Arrival</a> when I was in high school, I found myself drawn to his work, and this film only deepened my admiration. The cinematography, acting, atmosphere, and Hans Zimmer’s score are all genuinely top-notch, working together to create an unforgettable experience that continues to captivate me years after my first viewing.</p> <p>I’m not here to break down these technical aspects—plenty of critics have done that already with far more expertise. Instead, I want to share a theory that’s been brewing in my mind: what if the plot isn’t exactly as it appears on the surface?</p> <p><em>WARNING: HUGE SPOILERS AHEAD. I really recommend watching the movie. It’s genuinely good.</em></p> <hr/> <h3 id="a-brief-recap">A Brief Recap</h3> <p>Blade Runner 2049 follows Officer K, a replicant blade runner tasked with "retiring" older replicants. During a routine job, he uncovers evidence suggesting that a replicant has given birth naturally—a revelation that threatens the established order. As K investigates, he’s drawn deeper into a web involving Niander Wallace, who seeks to control replicant reproduction, and Ana Stelline, a seemingly innocent memory maker isolated from the world due to illness.</p> <h3 id="the-apparent-power-structure">The Apparent Power Structure</h3> <p>On the surface, the power dynamics seem clear. K and Joi are our protagonists, while Niander Wallace serves as the antagonist—a god-like figure who, interestingly, never directly interacts with K despite his enormous presence in the story.</p> <p>But what if Wallace isn’t the real antagonist? What if the true antagonist is Ana Stelline, the memory maker?</p> <h3 id="the-nature-of-replicants-vs-humans">The Nature of Replicants vs Humans</h3> <p>To understand this theory, we need to examine what makes replicants distinct from humans in this universe. Physically, replicants are superior—stronger, faster, and more resilient. Their base intelligence is remarkably high, as we see when K navigates complex systems and archives with ease.</p> <p>He navigates vast DNA archives with only the symbols ATGC, showing pattern-matching abilities that surpass humans. Beyond his intellectual capabilities, K also fights with striking efficiency, his punches are fast and calculated, with no wasted motion. When he fires a weapon, he never misses his target, hitting the bullseye with almost robotic accuracy. This blend of mental and physical efficiency underscores his engineered perfection.</p> <p>But the fundamental difference between replicants and humans lies in purpose. As Heidegger might suggest, humans are thrown into the world without inherent meaning. This absence of predetermined purpose defines the human condition. We’re forced to create our own meaning, to discover our own paths—a freedom that is fundamentally human.</p> <p>Replicants, in contrast, are manufactured with clear objectives. They exist as tools from the moment of their creation—soldiers, pleasure models, or laborers. Their existence is fundamentally instrumental. While their minds are quick, they lack the fundamental freedom to determine their own purpose.</p> <p>This is why Ryan Gosling’s performance is so brilliant. His reserved nature and subtle expressions portray a grown-up baby—a physically mature being who lacks social experience and trying to find his place in the world. He’s rigid and tense because he’s navigating a world without the emotional maturity and experiences that comes from growing up human.</p> <p>And this is precisely why Rachael’s child changes everything. A replicant born naturally arrives without engineered purpose—no predetermined function. This natural birth places them in that uniquely human position of having to discover their own meaning, making them indistinguishable from humans in that crucial aspect.</p> <h3 id="the-power-of-memory">The Power of Memory</h3> <p>In the Blade Runner universe, memory is a central theme. Both films explore how memories shape identity, with the original posing questions about implanted memories and the sequel expanding on this concept.</p> <p>What is a human without memories? If memory is a core function of our identity and an integral part of who we are, then the manipulation of memory equals the manipulation of the person. In this light, Ana Stelline’s position becomes staggeringly significant. She doesn’t just create false pasts—she shapes the very identities of replicants by designing their memories.</p> <p>The film portrays Ana as benign and relatively powerless, but in a world where replicants are physically stronger than humans, she might actually wield the darkest power of all—the ability to control replicants from within by manipulating the very foundation of their sense of self.</p> <h3 id="k-as-a-pawn">K as a Pawn</h3> <p>This leads to my theory, which admittedly is somewhat radical: Ana Stelline directly influences replicants by crafting memories implanted at inception, subtly guiding their consciousness. This manipulation is apparent due to the resistance movement stirred up by other replicants, a movement made possible because Ana carefully places the seed of her memories into each and every replicant at their creation, allowing those seeds to grow.</p> <p>In this reading, K serves as a pawn in multiple games—Wallace’s disposable workforce and Ana’s controlled agent. The film presents K’s decision to save Deckard and reunite him with Ana as a triumph of free will, a humanist moment where K chooses not to be a tool.</p> <p>But are we certain this was K’s true motivation? His memories—the very foundation of his identity and decisions—are products of Ana’s work. If she is indeed a grand mastermind, is it possible that K is just a sophisticated puppet designed to bring Deckard to her? How much of his apparent free will is actually the result of carefully crafted memories implanted to guide him toward Ana’s desired outcome?</p> <p>The replicant’s memories are their identities. By controlling memories, Ana potentially controls the replicants themselves—including K—making her the true puppet master behind the scenes.</p> <p>Consider what replicant reproduction means in this power dynamic. If replicants can reproduce naturally, whose power diminishes—Wallace’s or Ana’s? I would argue Ana’s influence wanes significantly. If Ana can create and alter memories of manufactured replicants, she can essentially use them as tools for her own purposes. But naturally born replicants would have authentic, lived experiences of growing up—memories that Ana didn’t craft. They would develop identities beyond her control.</p> <p>Yet what does K actually accomplish? While Wallace seeks to give replicants reproductive capabilities—ironically erasing the philosophical distinctions between humans and replicants—K’s actions ultimately serve Ana’s interests. He prevents Deckard from being captured by Wallace and instead delivers him directly to Ana. This is the fascinating contradiction: Wallace’s quest to blur the line between replicants and humans might actually be countered by Ana, who maintains her power by keeping that distinction intact through memory manipulation.</p> <h3 id="a-darker-interpretation">A Darker Interpretation</h3> <p>This interpretation casts the seemingly benevolent memory maker in a more sinister light. If Ana’s power relies on controlling replicants through fabricated memories, then natural replicant reproduction represents a profound threat to her influence. Through this lens, her apparent helplessness and isolation might be a carefully constructed facade concealing a deeper agenda.</p> <p>I strongly believe this wasn’t Villeneuve’s intended reading of the film. The surface narrative should probably be taken at face value. However, when we deeply examine how memory functions as identity for replicants and follow that thread to its philosophical conclusion, it raises unsettling questions about Ana’s true nature and motivation.</p> <p>Is Ana truly benign? Given her unprecedented power to shape and insert memories—essentially programming consciousness itself—can we be certain of her intentions? If she can manipulate replicants to pursue her goals while believing they’re exercising free will, how would we (or they) ever know?</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>This isn’t strict analysis—it’s more of a fan theory (perhaps a bit deranged, I admit). But it’s food for thought nonetheless.</p> <p>I still love this movie immensely. The cinematography, lighting, sound design, soundtrack, narrative structure, and performances are all exceptional. There’s always more to unpack in Blade Runner 2049, which is precisely what makes it such a lasting masterpiece.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="Life"/><summary type="html"><![CDATA[A fan theory]]></summary></entry></feed>