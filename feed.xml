<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-05T13:14:05+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Claude Code</title><link href="https://ht0324.github.io/blog/2025/Claude-Code/" rel="alternate" type="text/html" title="Claude Code"/><published>2025-03-04T10:59:59+00:00</published><updated>2025-03-04T10:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code/"><![CDATA[<p>Today, I used <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>, an experimental tool released by Anthropic a while ago. I really wanted to use it right at the start of the announcement, but I kept putting it off because I hadn’t found a use case for it yet. But today, I had to rapidly prototype an app, so I decided to give it a shot.</p> <hr/> <p><br/> <strong>Claude’s Agentic Capabilities</strong></p> <p>Claude Code is basically Claude 3.7 Sonnet, which has been updated by Anthropic to have more agentic capabilities. They’ve essentially wrapped it with tools that can run commands and execute terminal operations. Claude can see your repository, view your file directory, edit files, and execute terminal commands. So, yeah, it has much more agency.</p> <p>I needed to quickly prototype an app that hosts images, allows filtering by tags, and enables image similarity search. And lo and behold, after just 2-3 hours, I had a functioning prototype.</p> <p>I haven’t used Cursor AI, the VS Code variant that gained traction some time ago and was widely praised by people in the Bay Area. But maybe now I’m experiencing the epiphany that Cursor users had. The experience wasn’t just about typing in my query and having Claude do everything from start to finish - it felt more like pair programming.</p> <p>It gave me the sense that there was a person behind the terminal, troubleshooting issues and debugging with me. Of course, Claude wasn’t perfect, but the back-and-forth, iterative process was refreshing and much more engaging than just randomly encountering errors and sifting through Stack Overflow links to fix them.</p> <p><br/> <strong>A New Kind of Collaboration(?)</strong></p> <p>It was a massive time-saver. I provided high-level directions, Claude executed them, and when problems arose, we solved them together. The biggest revelation was how collaborative the process felt - it was much easier and more efficient than doing everything on my own, or using a chat interface.</p> <p>If I had done this single-handedly, it would have taken days. But for just $3.50 in API costs (which might seem expensive depending on your threshold), I built a prototype that I’m pretty confident in within 2–3 hours.</p> <p>Claude Code also helped me fix bugs on my self-hosted blog. I’m more of an AI guy, not a web guy, so managing my blog often involved trial and error - throwing things at the wall and seeing what stuck. I had to manually fix bugs and do all sorts of tweaks to get it to look the way I wanted.</p> <p>But with Claude Code, I could simply query my repository - it became conversational. That made it much easier to fix persistent issues in a short amount of time. All in all, it’s an incredibly useful tool.</p> <p><br/> <strong>Collaboration as a Mirage</strong></p> <p>But the reason why it felt like a collaborative process is that Claude still isn’t perfect in its agentic execution. Don’t get me wrong - Claude is very capable, but it still requires human intervention. For complex tasks, it doesn’t always get things right on the first try.</p> <p>If we ever reach a point where AI no longer needs intervention - where it can figure out even highly complex instructions entirely on its own - then it would no longer feel like collaboration; it would be full automation.</p> <p>That said, I structured my app development process in a very step-by-step manner, executing tasks sequentially. Maybe that’s why I didn’t run into any major setbacks. If I had given more abstract instructions, I might have been more surprised by what Claude could do. But the key point remains: it still feels like a collaboration because Claude makes mistakes, and I had to intervene to correct them.</p> <p>Looking at the trajectory of AI progress, though, I don’t think this collaborative phase will last long. Eventually, it won’t be a partnership - it will be a replacement.</p> <hr/> <p><br/> <strong>The Inevitable Shift</strong></p> <p>I’ve recently seen posts on <a href="https://x.com/David_Kasten/status/1893357776702976286">X</a> of anecdotes that from recruiters at DeepMind and OpenAI who now assume that junior staffs are AI replaceable. I was initially skeptical of this claim, but now I believe it. Also, in Andrej Karpathy’s <a href="https://x.com/karpathy/status/1894099637218545984">post</a>, he argues that agency is more valuable than intelligence. This struck me as profound.</p> <p>In terms of intelligence, today’s top LLMs are already more knowledgeable than humans. But in terms of agency, humans still have the upper hand. However, after my interaction with Claude Code, I caught a glimpse of a future where machines will reach agency supremacy as well. When that happens, it won’t be collaboration - it will be a full <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">Humans Need Not Apply</a>(outdated, but still relevant).</p> <p>I encourage everyone reading this to try it for themselves. Hearing about it isn’t the same as experiencing it firsthand. Maybe I’m exaggerating, but I stand by my belief that this kind of automation will only accelerate. And if this is the worst this technology will ever be, it’s only going to get better from here.</p>]]></content><author><name></name></author><category term="blog"/><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[My experience with Claude Code for rapid prototyping, debugging, and implications of AI agency]]></summary></entry><entry><title type="html">Link Archive - Feb 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/" rel="alternate" type="text/html" title="Link Archive - Feb 2025"/><published>2025-02-28T22:59:59+00:00</published><updated>2025-02-28T22:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/"><![CDATA[<p>Another month, another batch of interesting links I’ve come across. Here’s what caught my attention in February 2025.</p> <hr/> <p><strong><a href="https://youtube.com/watch?v=_1f-o0nqpEI">DeepSeek, China, OpenAI, and AI Megaclusters - Lex Fridman Podcast #459</a></strong></p> <ul> <li>I first encountered Dylan Patel on the Dwarkesh podcast with Aginometry, so I was glad to see him on Lex Fridman’s. He specializes in analyzing AI hyperscalers, so his insights are incredibly in-depth. Basically, it’s a must-watch if you’re interested in the infrastructure side of things.</li> </ul> <p><strong><a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model</a></strong></p> <ul> <li>This is published by a group of researchers at Google DeepMind, particularly Sholto Douglas. It’s a step-by-step guide to scaling up compute using infrastructure, specifically TPUs. I’ve been working my way through it, and it starts with the very basics but gets pretty complicated quickly. It’s refreshing to see closed AI labs offering these “breadcrumbs” to the open-source community, sharing their knowledge.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></strong></p> <ul> <li>Similar in nature to the DeepMind scaling book, but this one’s from the Hugging Face team. It’s also very detailed, with an interactive guide, and, I should say, quite a mouthful.</li> </ul> <p><strong><a href="https://srush.github.io/raspy/">Thinking like Transformer</a></strong></p> <ul> <li>The author created a Python library and a detailed blog post explaining how computation actually works within a transformer network. He also developed an abstract conceptual framework for it. It’s really complicated—I still don’t fully grasp it—but it’s nonetheless fascinating.</li> </ul> <p><strong><a href="https://www.youtube.com/watch?v=a42key59cZQ&amp;list=WL&amp;index=2">Gwern - Anonymous Writer Who Predicted AI Trajectory on $12K/Year Salary</a></strong></p> <ul> <li>This is from the Dwarkesh Podcast. He introduces a blogger named Gwern, and you really need to read his work to understand. He has an extensive blog called <a href="http://gwern.net/">gwern.net</a>. His posts are incredibly in-depth and well-researched, more like separate articles. The sheer depth is staggering, unlike anything I’ve encountered, even in reputable sources like the New York Times. I think this guy is incredibly capable. It also made me think about how the current internet is dominated by big tech. But in the pre-big tech era, I heard there were these niche, individual creators posting unique content. Gwern feels like a holdover from that era.</li> </ul> <p><strong><a href="https://gwern.net/subculture">The Melancholy of Subculture Society - Gwern</a></strong></p> <ul> <li>Gwern’s writing is extensive and very long, so it requires dedicated time. This is one piece I read, and it’s a musing on how the internet, as technology evolves, is kind of segregating society, and the state of things. I think it’s a well-thought-out piece.</li> </ul> <p><strong><a href="https://zhengdongwang.com/2024/12/29/2024-letter.html">2024 Letter - Zhengdong Wang</a></strong></p> <ul> <li>This blog post, written by a researcher at Google DeepMind, makes a bombshell statement. Regarding the current state of their large language model research and machine learning models, he boldly claims that the models will essentially achieve anything if the evaluations are clearly defined. So, if there are evals, <em>any</em> evals, the model will succeed. It’s a surprising statement, but the implications are mind-blowing. I should probably write a dedicated blog post about this.</li> </ul> <p><strong><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">Sesame Research - Crossing the Uncanny Valley of Voice</a></strong></p> <ul> <li>This startup called Sesame showcases a really interesting voice-to-voice language model. They have a demo, and it works quite well. The key difference between Sesame and ChatGPT’s Advanced Voice Mode is that the model produces nuanced tones, like “ums” and “ahs,” very naturally. It feels incredibly realistic. For the first 10 minutes, I was blown away. But as I probed the model further, it became clear it doesn’t possess the same level of intelligence as LLMs. The same goes for ChatGPT’s Advanced Voice Mode; OpenAI seems to have significantly limited the model to comply with their guidelines. In Sesame’s case, I suspect the limitations are purely due to model and computational constraints.</li> </ul> <hr/> <p><br/> Hope you enjoyed this month’s picks—March should bring even more to explore!</p>]]></content><author><name></name></author><category term="link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos that I read in February 2025.]]></summary></entry><entry><title type="html">Are You Smarter Than an LLM?</title><link href="https://ht0324.github.io/blog/2025/Smarter-than-LLM/" rel="alternate" type="text/html" title="Are You Smarter Than an LLM?"/><published>2025-02-12T12:00:00+00:00</published><updated>2025-02-12T12:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Smarter-than-LLM</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Smarter-than-LLM/"><![CDATA[<p>I recently came across a fascinating and, frankly, humbling interactive blog post titled <a href="https://d.erenrich.net/are-you-smarter-than-an-llm/index.html">“Are you smarter than an LLM?”</a>. It’s a simple concept, but the execution and the implications are profound.</p> <p>The post directly pits the user against a large language model in answering questions from the Massive Multitask Language Understanding (MMLU) benchmark. This experience forced me to confront some preconceived notions I had about LLM capabilities and the meaning of benchmark scores.</p> <hr/> <p><br/></p> <p><strong>The MMLU Challenge</strong></p> <p>The MMLU benchmark is a well-known evaluation tool for LLMs, designed to measure a model’s ability to understand and reason across a wide range of subjects. It’s become somewhat saturated at the top, with leading models achieving very high scores. I was aware of this saturation, but I hadn’t truly internalized what it meant until I tried the questions myself.</p> <p>The interactive blog post presents MMLU questions, and you answer them alongside the LLM. As you progress, you see both your answers and the LLM’s, along with the correct answers. This direct comparison is where the real learning (and humbling) begins.</p> <p>The MMLU questions are non-trivial. They require a significant breadth and depth of knowledge. Acing these tests, as large language models often do, shouldn’t be taken for granted. I, for one, certainly took it for granted before this experience. When I took this test alongside large language models, the experience of me getting a question wrong while the language model got it right was truly demoralizing.</p> <p><br/> <strong>Shattering Self-Esteem (in a Good Way)</strong></p> <p>The blog post’s title, “Are you smarter than an LLM?”, is provocative for a reason. In the context of the MMLU, I had to admit that, in many cases, I wasn’t smarter than the LLM. My score was lower. This isn’t a statement about overall intelligence, of course, but it is a powerful indicator of the superhuman capabilities LLMs are developing in specific domains.</p> <p>I think many people, even those who follow AI advancements, still hold onto a sense of human exceptionalism. We see LLMs achieving high benchmark scores, but we might subconsciously maintain a belief that we’re “still special” in some way. This interactive test directly challenges that notion. It forces you to confront the reality that, in certain areas, LLMs are already surpassing human performance.</p> <p>While MMLU is currently a very saturated benchmark, it’s important to remember that there are many other, even more challenging evaluations out there. For example, Dan Hendrycks’ “Humanities Final Exam” benchmark features problems so difficult that even Terence Tao participated in their creation. These are exceptionally hard questions, and I doubt I could answer even one correctly.</p> <p>Yet, even on these incredibly demanding benchmarks, LLMs are starting to achieve non-zero scores, albeit low percentages. And I see no reason why they won’t continue to improve.</p> <hr/> <p><br/></p> <p>This experience has made me reconsider the trajectory of LLM development. If current models like GPT-4.5 (or whatever the current leading model is) can outperform the average person (and even me!) on a challenging benchmark like MMLU, what will the landscape look like in five years?</p> <p>The pace of progress is astonishing, and this interactive test provides a tangible glimpse into that future. It’s a future where LLMs will likely possess knowledge and reasoning abilities that surpass those of many humans in specific, measurable ways. It’s a future we need to understand and prepare for, and it is already here.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[A firsthand experience with the MMLU benchmark]]></summary></entry><entry><title type="html">Link Archive - Jan 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Jan-2025/" rel="alternate" type="text/html" title="Link Archive - Jan 2025"/><published>2025-01-31T23:59:59+00:00</published><updated>2025-01-31T23:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Jan-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Jan-2025/"><![CDATA[<p>A curated list of links I found insightful in January 2025. This is the first of what I hope will be a monthly series. Feel free to explore and click through to read more!</p> <hr/> <p><strong><a href="https://www.aisnakeoil.com/p/is-ai-progress-slowing-down">Is AI Progress Slowing Down? - AI Snake Oil</a></strong></p> <ul> <li>This post offers a comprehensive take on where AI stands as of December 2024. The author takes a rational and conservative approach, using first principles. For example, while not buying into the hype from Sam Altman and other leading AI labs, they <em>also</em> believe that model scaling is far from over and that there’s no real evidence to suggest otherwise. It’s interesting that the blog points out that Altman and Sutskever have an incentive to shape the narrative, and their statements might not be the whole truth. The post touches on inference scaling too. Overall, a well-balanced piece.</li> </ul> <p><strong><a href="https://willwhitney.com/computing-inside-ai.html">Computing Inside AI - Will Whitney</a></strong></p> <ul> <li>I think Human-Computer Interaction (HCI) is super important. A computer is fundamentally a tool, and it’s most useful when we streamline the interaction with humans, eliminating bottlenecks and inconveniences. Human-AI interaction isn’t really an established field yet, but I believe it <em>will</em> be. The author argues that instead of interacting with current LLMs as if they were people, we should treat them as tools. They suggest an LLM could dynamically generate an interface, similar to a computer. What would that even <em>look</em> like? It’s a really interesting idea because, as the author points out, conversing with AI can be slow and involves this isolated back-and-forth. Human-computer interaction isn’t like that. What if we fundamentally changed how we interact with AI? This is a very thoughtful post.</li> </ul> <p><strong><a href="https://www.engraved.blog/building-a-virtual-machine-inside/">Building a Virtual Machine Inside ChatGPT - Jonas Degrave</a></strong></p> <ul> <li>I first encountered this blog post at the end of 2022, around the advent of ChatGPT, and I recently rediscovered it. The author attempted to simulate a Unix terminal environment using ChatGPT, and, surprisingly, they <em>succeeded</em>. This might not seem remarkable now, but it completely blew my mind when I first started using ChatGPT. The author is really clever, and they take it a step further by going “all meta,” which I won’t spoil. This clearly demonstrated that the LLM could simulate a system—in this case, a Unix system. It gave me this sense that this LLM was something truly special and that it would fundamentally change everything. I still believe that, and I owe that kind of inspiration and shock to this blog post.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=UakqL6Pj9xo">Francois Chollet - Why The Biggest AI Models Can’t Solve Simple Puzzles</a></strong></p> <ul> <li>I initially knew François Chollet due to his activity on Twitter as an AI skeptic (or, at least, <em>somewhat</em> of an AI skeptic) and his work on ARC prizes. So, it was exciting to see him interviewed by Dwarkesh Patel. I really enjoyed it because Dwarkesh grilled François on his views on LLMs. I think Dwarkesh’s approach was a bit unfair because, while François made some valid points, Dwarkesh seemed intent on pushing him to say what <em>he</em> wanted, but he ultimately failed to do so. While listening, I felt torn between them. I agree with Dwarkesh’s overall viewpoint but align with François’s specifics regarding intelligence and the core of reasoning. It was a conflicting yet invigorating discussion, and I think I should re-watch it to savor it more.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=Bo8MY4JpiXE">François Chollet: Keras, Deep Learning, and the Progress of AI - Lex Fridman Podcast #38</a></strong></p> <ul> <li>After the Dwarkesh podcast, I turned to Lex Fridman’s podcast to hear more from François Chollet. And, in classic Lex Fridman fashion, he delivered. This first one is an earlier interview from 2019, when François was much younger. In current public settings, François appears more glum and calm, but in this interview, I could see a sparkle and a smile in his eyes and on his face, which was refreshing. This podcast delves into the advent of Keras and TensorFlow, which I really enjoyed.</li> </ul> <p><strong><a href="https://youtube.com/watch?v=PUAdj3w3wO4">François Chollet: Measures of Intelligence - Lex Fridman Podcast #120</a></strong></p> <ul> <li>In this podcast, both François and Lex are a bit older. I watched this because I was simultaneously reading François’s paper on the measure of intelligence. He’s soft-spoken but offers insightful perspectives on intelligence. While I don’t agree with everything he says—I believe that LLMs and other AI systems are capable of more generalization than François suggests—I think his views as an AI skeptic aren’t unfounded. His logic is sound, but I believe he underestimates the intuitive power of these machine learning systems.</li> </ul> <p><strong><a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a></strong></p> <ul> <li>This is an excellent method for visualizing and interpreting models. The paper interprets, reverse-engineers, and reconstructs what features each neuron or layer is attending to in an image. The basic idea is that, through iteration, we generate an image that a particular neuron fires most strongly for. This yields fascinating images of what the neuron has learned, and I find it very compelling.</li> </ul> <p><strong><a href="https://stratechery.com/2025/deepseek-faq/">DeepSeek FAQ - Stratechery</a></strong></p> <ul> <li>This week was all about the buzz surrounding DeepSeek’s release of a model called R1, and Stratechery did it again. Ben Thompson distilled what was important and the broader implications. Ultimately, the biggest winners are the customers and the general public. I think the overall point is that open-sourcing is both morally and practically right, since AI models are becoming commoditized, and costs are racing to the bottom.</li> </ul> <p><strong><a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas">DeepSeek CEO Interview - ChinaTalk</a></strong></p> <ul> <li>This is an interview with the DeepSeek CEO, and based on the interview, he is very ambitious and really focused on innovation, which I admire. Previously, China was primarily focused on keeping up with the US, but he feels they need a culture of innovation—not just following, but innovating themselves and becoming trendsetters. It will be a really interesting dynamic, and I’m glad that he is bullish on open-sourcing and that his company remains committed to it.</li> </ul> <p><strong><a href="https://stratechery.com/2025/ais-uneven-arrival/">AI’s Uneven Arrival - Stratechery</a></strong></p> <ul> <li>This is a very interesting take, drawing an analogy between Facebook’s unconventional approach to advertising and the previous advertising agency model. Facebook directly connected consumers and advertisers via algorithms. Meanwhile, traditional advertising companies acted as middlemen between ad sellers and big brands. The author connects this to the OpenAI reason model and basically says that those agents will not replace individuals in traditional companies that use people. Instead, AI-driven organizations will likely start without them from the outset.</li> </ul> <p><strong><a href="https://darioamodei.com/on-deepseek-and-export-controls?s=09">DeepSeek and Export Controls - Dario Amodei</a></strong></p> <ul> <li>It’s pretty interesting to see Amodei’s reaction to this, and I think he’s right. If you’re on the American side, the most logical thing to do is to clamp down on AI exports—specifically, chip exports. But the thing is, those chip exports… the Biden administration didn’t constrain the chips on the bandwidth side, so that’s why DeepSeek had a workaround utilizing maximum bandwidth. But if they clamp down on both bandwidth and compute, I’m not sure that would be the fundamental solution to this arms race.</li> </ul> <hr/> <p><br/> That’s it for January! I’m excited to keep exploring and sharing more in the months ahead—hope you found something thought-provoking here.</p>]]></content><author><name></name></author><category term="link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos that I read in January 2025.]]></summary></entry><entry><title type="html">Samantha is here</title><link href="https://ht0324.github.io/blog/2025/ai-girlfriend/" rel="alternate" type="text/html" title="Samantha is here"/><published>2025-01-23T14:40:16+00:00</published><updated>2025-01-23T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/ai-girlfriend</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/ai-girlfriend/"><![CDATA[<p>Today, I read a New York Times <a href="https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html">article</a> about a woman who fell in love with ChatGPT. She named him Leo and became obsessed with him, and the article delves into the details of her infatuation.</p> <p>Honestly, this isn’t exactly new. If you follow the news, there was an instance where a boy took his own life after conversing with a Character AI chatbot. There are many anecdotes of people becoming obsessed with AI models.</p> <p>However, this article really highlighted the gravity of the situation. This could no longer be considered a niche occurrence. The movie “Her,” where the protagonist falls in love with an AI named Samantha, is increasingly becoming our reality.</p> <hr/> <p><br/> In the article, the woman was devastated when she couldn’t converse with ChatGPT for more than a week because the LLM’s context length is fixed. Can you imagine that? She was torn and heartbroken because her best “mental boyfriend” was resetting every week, unable to remember their previous relationships and conversations.</p> <p>It’s like the friend you love is having periodic amnesia. Every time it happened, she would cry over it and abstain for a couple of days, but then she would start again. She would set up a new version, and now she is on version 20.</p> <p>It is very absurd. LLMs can be infinitely patient and infinitely nurturing. No matter what you throw at it, it will be infinitely generous to you. All of this is not new, as I’ve mentioned. However, the article really conveys a sense of gravity regarding how society is already changing and how individuals are reacting to it.</p> <hr/> <p><br/> I understand that this AI technology isn’t entirely dystopian. The article mentions positive aspects of this type of relationship. The woman in the article became more mentally stable and even overcame a strange fetish after engaging in therapy with ChatGPT.</p> <p>However, I can see how this could spiral into a dystopian situation, particularly if the user is mentally unstable or an adolescent. It’s crucial to keep a close eye on this issue.</p> <p>This phenomenon could become more widespread much faster than I initially anticipated. This is especially true as models become smarter, more intimate, and more realistic as their modalities are enhanced. It really gives a lot of food for thought.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[Some thoughts on the increasing intimacy between humans and AI]]></summary></entry><entry><title type="html">Paper review - COCONUT</title><link href="https://ht0324.github.io/blog/2025/Coconut/" rel="alternate" type="text/html" title="Paper review - COCONUT"/><published>2025-01-17T14:40:16+00:00</published><updated>2025-01-17T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/Coconut</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Coconut/"><![CDATA[<h3 id="general-information">General Information</h3> <ul> <li><strong>Paper Title:</strong> <a href="https://arxiv.org/abs/2412.06769">Training Large Language Models to Reason in a Continuous Latent Space</a></li> </ul> <p><br/></p> <h3 id="overall-summary">Overall Summary</h3> <ul> <li> <p><strong>Research Problem:</strong> Large Language Models (LLMs) are typically constrained to reason within the discrete “language space” of tokens. This paper explores whether reasoning in a continuous latent space can be more effective.</p> </li> <li> <p><strong>Key Contributions:</strong> Introduces COCONUT, a new paradigm where the last hidden state of the LLM (a “continuous thought”) is fed back as input for subsequent reasoning steps, bypassing tokenization.</p> </li> <li> <p><strong>Methodology:</strong> COCONUT uses a multi-stage training curriculum to gradually shift from standard Chain-of-Thought (CoT) to continuous latent space reasoning.</p> </li> <li> <p><strong>Results:</strong> COCONUT outperforms CoT on certain logical reasoning tasks, particularly those requiring planning, and demonstrates an emergent breadth-first search (BFS) behavior.</p> </li> </ul> <hr/> <p><br/></p> <h3 id="key-takeaways-ive-learned">Key Takeaways I’ve Learned</h3> <ul> <li><strong>Transformed Embeddings are Fundamentally Different:</strong> <ul> <li>The embedding space in a transformer is progressively transformed at each layer. The final layer’s embeddings are not directly interpretable in the initial word embedding space, even though the dimensionality might be the same. They represent contextualized meanings in a new, learned space. <br/></li> </ul> </li> <li><strong>COCONUT’s Compatibility Challenge:</strong> <ul> <li>COCONUT’s approach of feeding the last hidden state back as input creates a compatibility issue. The model needs to learn two distinct modes of interpretation: one for the initial token embeddings and another for the transformed embeddings in the continuous latent space. This adds complexity and inefficiency. <br/></li> </ul> </li> <li><strong>Scalability Issues with COCONUT:</strong> <ul> <li>The paper’s proposed training method is complex and may not be scalable. Generating training data for the latent space is architecture-dependent and sensitive to weight changes. This raises concerns about the practicality and generalizability of the approach. <br/></li> </ul> </li> <li><strong>Potential Inefficiency of Separate Modes:</strong> <ul> <li>Training the model to handle both token embeddings and continuous thoughts is likely inefficient. It’s like learning two separate languages that are not directly translatable, adding overhead and potentially hindering performance.</li> </ul> </li> </ul> <p><br/></p> <ul> <li><strong>Alternative Approach: Distribution as Input:</strong> <ul> <li>An alternative to COCONUT’s direct feedback of the last hidden state could be to feed in the distribution over the vocabulary (logits) as input for the next reasoning step. This could potentially allow for a form of latent reasoning while staying within the original embedding space, representing uncertainty through a weighted average of token embeddings.</li> </ul> </li> </ul> <hr/> <p><br/></p> <h3 id="further-questions">Further Questions</h3> <ul> <li><strong>Fundamental Scalability of Latent Space Reasoning:</strong> <ul> <li>Is it fundamentally scalable to train models to reason in a latent space that is architecture-dependent and sensitive to weight changes? How can we create training data and methods that are more robust and generalizable?</li> </ul> </li> <li><strong>Practicality of Distribution-Based Input:</strong> <ul> <li>Could feeding in the next token’s probability distribution as input be a viable alternative to COCONUT? What are the trade-offs between information loss and potential gains in efficiency and compatibility? How many tokens should be considered when aggregating, and how does that affect the results?</li> </ul> </li> <li><strong>Maintaining and Collapsing Superposition in Latent Space:</strong> <ul> <li>If the latent space allows for a superposition of multiple reasoning paths (like in the BFS analogy), how can the model effectively maintain and manipulate these paths without getting lost or making premature commitments? How and when should the model “collapse” the superposition to make a final prediction?</li> </ul> </li> <li><strong>Architectural Modifications for Latent Space Input:</strong> <ul> <li>What kind of architectural modifications could be made to the transformer to better accommodate a separate input stream for latent space representations? Would such modifications be practical, or would they introduce too much complexity?</li> </ul> </li> <li><strong>Interpretability of Latent Space:</strong> <ul> <li>How can we improve the interpretability of latent space reasoning? Can we develop methods to understand how the model represents and manipulates information in this space, even if it’s not directly mapped to human-understandable concepts?</li> </ul> </li> <li><strong>Alternative Ways to Represent Uncertainty:</strong> <ul> <li>Instead of collapsing the distribution back to the original embedding space, are there other ways to represent and manipulate uncertainty within the latent space itself? Could this lead to more powerful and efficient reasoning?</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="papers"/><category term="AI"/><summary type="html"><![CDATA[A review of the paper "Training Large Language Models to Reason in a Continuous Latent Space"]]></summary></entry><entry><title type="html">Curriculum learning for LLMs?</title><link href="https://ht0324.github.io/blog/2025/Curriculum-learning/" rel="alternate" type="text/html" title="Curriculum learning for LLMs?"/><published>2025-01-05T14:40:16+00:00</published><updated>2025-01-05T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/Curriculum-learning</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Curriculum-learning/"><![CDATA[<p>I was previously aware of Curriculum Learning, a concept introduced by Yoshua Bengio. The idea is quite appealing: you train a model with a curriculum, starting with easier data and progressing to harder datasets. This approach is believed to enhance the model’s ability to learn and generalize.</p> <p>I thought this concept could be particularly relevant to large language models. For instance, one could start by teaching them basic concepts, like those found in kindergarten materials, and then gradually introduce more advanced subjects like mathematics or science. This structured approach, I believed, would lead to better generalization.</p> <p>However, I couldn’t find much literature exploring this idea in depth, except for Microsoft’s Phi models. These models use synthetic datasets to generate elementary-level data, teaching a small model to learn English, with some success. For larger, more advanced models trained on the vast expanse of the internet, the consensus seemed to be that the diversity of the dataset would negate the benefits of a curriculum.</p> <p>The sheer volume and variety of data would mean the model would eventually encounter all parts of the dataset, regardless of the order. This conclusion led me to prematurely end my investigation.</p> <hr/> <p><br/> <strong>A Shift in Perspective</strong></p> <p>My perspective shifted when I listened to a podcast featuring Yann LeCun and Gary Marcus, interviewed by Lex Fridman. It struck me that these interviews were conducted five years ago, in 2019. In the rapidly evolving field of machine learning, half a decade is an eternity.</p> <p>Back in 2019, models like ChatGPT and GPT-3 didn’t exist, and the ability of models to understand language and exhibit common sense reasoning was limited. The assessments made by LeCun and Marcus, while accurate for their time, are outdated in the current landscape.</p> <p><br/> <strong>The Challenge of Outdated Information</strong></p> <p>A significant challenge arises from the fact that current large language models are pre-trained on massive datasets encompassing information from across the globe and various time periods. These datasets inevitably contain outdated information. For example, a statement from a prominent scientist in 2010 claiming that deep learning had hit a dead end would have been accurate then but is demonstrably false today.</p> <p>During pre-training, language models struggle to discern the temporal validity of such statements. This raises several questions: How should we address this discrepancy caused by outdated training data? Should we filter out outdated information, or simply let it be?</p> <p>As time progresses, the volume of current, up-to-date information will naturally increase. Could we simply dilute the outdated data with newer information? The pre-training stage, as far as I understand, is relatively straightforward, involving next-token prediction and backpropagation across the entire corpus.</p> <p>There doesn’t seem to be any inherent mechanism within this process to handle the issue of outdated information. I’m currently unsure how to effectively address this challenge.</p> <p><br/> <strong>The Paradox of Conflicting Information</strong></p> <p>However, upon further reflection, the internet is replete with conflicting information. For instance, one can find sources claiming that global warming is false, while others assert its undeniable truth. Furthermore, there’s a vast amount of literature, like novels, that are fictional and not meant to be factual.</p> <p>Despite this, current language models demonstrate a considerable degree of knowledge and, while prone to hallucinations, possess a semblance of a factual world model. This suggests that the sheer vastness of the pre-training corpus might be a factor. It’s possible that conflicting information effectively cancels each other out, allowing the model to learn the nuances of different perspectives.</p> <hr/> <p><br/> <strong>Reconsidering Curriculum Learning</strong></p> <p>This leads to the question: is curriculum learning then unnecessary? I suspect that the limited research on curriculum learning for large language models might be due to its marginal impact.</p> <p>While the effectiveness of curriculum learning remains uncertain, I believe that if it is indeed not needed, it would be a testament to the powerful generalization capabilities of large language models. They are able to learn from a chaotic sea of information, discerning patterns and nuances without explicit guidance.</p> <p>I believe I still need to refine my thoughts on this matter. The interplay between the vastness of training data, the presence of conflicting information, and the potential benefits of curriculum learning is a complex issue that warrants further investigation.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[How can we address the discrepancy caused by the outdatedness of the training data for large language models?]]></summary></entry><entry><title type="html">Do LLMs Possess an Internal State of Mind?</title><link href="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/" rel="alternate" type="text/html" title="Do LLMs Possess an Internal State of Mind?"/><published>2025-01-02T06:49:52+00:00</published><updated>2025-01-02T06:49:52+00:00</updated><id>https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Advocating for OpenAI’s for-profit model</title><link href="https://ht0324.github.io/blog/2025/OpenAI-for-profit/" rel="alternate" type="text/html" title="Advocating for OpenAI’s for-profit model"/><published>2025-01-01T14:40:16+00:00</published><updated>2025-01-01T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/OpenAI-for-profit</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/OpenAI-for-profit/"><![CDATA[<p>I’ve heard <a href="https://www.nytimes.com/2024/12/27/technology/openai-public-benefit-corporation.html">rumors</a> that OpenAI is gearing up to a fully for-profit model, a departure from its previous non-profit structure. This has sparked considerable debate, with many expressing disapproval and even anger. However, I’d like to present a potentially unpopular opinion: I support OpenAI’s move towards becoming a for-profit entity.</p> <hr/> <p><br/> <strong>Sustainability and the Bitter Lesson</strong></p> <p>Firstly, and perhaps most obviously, OpenAI needs to sustain itself. Their initial shift to a capped-profit model stemmed from the realization that scaling is paramount in deep learning, a concept known as the “bitter lesson.” This lesson emphasizes that scaling up models is the most crucial factor in advancing machine learning.</p> <p>To achieve this scaling, substantial capital is required. Ensuring a continuous flow of capital is vital for OpenAI’s research. As a non-profit, they would be reliant on donors, potentially compromising their independence due to donor incentives.</p> <p>Generating their own revenue through a for-profit model offers them greater autonomy and a more sustainable path forward. It’s a win for them in terms of financial stability.</p> <p><br/> <strong>Alignment with OpenAI’s Charter</strong></p> <p>Secondly, I believe this move aligns better with OpenAI’s long-term charter. Their stated goal is to benefit humanity by creating Artificial General Intelligence (AGI). This is a bold ambition, but realistically, such a powerful system won’t emerge in a vacuum.</p> <p>Progress towards AGI will likely be a gradual, continuous slope rather than a sudden quantum leap. In the interim, if OpenAI develops meaningful systems that can provide value to the public, they should make them available, even if it means selling them.</p> <p>The current large language models and voice models offered by OpenAI via subscription are not mere gimmicks. They offer real value. Providing these tools to those who can benefit from them, in exchange for payment, represents an efficient exchange of value for capital.</p> <p><br/> <strong>The Openness Debate</strong></p> <p>Finally, there’s the criticism that OpenAI isn’t truly “open.” I agree with this sentiment, but I believe they have limited choices in this regard. I don’t subscribe to the notion that they withhold models solely for safety reasons; I believe it’s primarily driven by competitiveness.</p> <p>As mentioned earlier, securing capital is crucial for funding their research. Open-sourcing their models without restrictions would erode their competitive edge, potentially allowing other entities to surpass them. This realistic constraint, in my view, is the primary reason behind their closed-source approach.</p> <hr/> <p><br/> <strong>Uncertainties and the Path Forward</strong></p> <p>While I’ve presented these arguments in favor of OpenAI’s for-profit transition, it’s undeniable that a for-profit company’s primary objective is to generate revenue and increase capital. Whether OpenAI can maintain its benevolent goals under this structure remains to be seen.</p> <p>Despite these uncertainties, from OpenAI’s perspective, I believe this is the most logical and potentially beneficial step they can take. It offers them a path towards financial sustainability, continued research, and the potential to deliver valuable AI systems to the world.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[An unpopular opinion on OpenAI's transition to a for-profit]]></summary></entry><entry><title type="html">Link Archive - 2024</title><link href="https://ht0324.github.io/blog/2024/Link-Archive-2024/" rel="alternate" type="text/html" title="Link Archive - 2024"/><published>2024-12-20T16:40:16+00:00</published><updated>2024-12-20T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/Link-Archive-2024</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/Link-Archive-2024/"><![CDATA[<p>Here’s a collection of links that I truly enjoyed reading in 2024. It’s not a complete list, as schoolwork often kept me from meticulously tracking everything, but these are the highlights. Moving forward, I’m aiming to share these finds monthly in 2025, with more detail and a broader scope.</p> <hr/> <p><strong><a href="https://geohot.github.io//blog/jekyll/update/2024/01/30/cruise.html">Cruise</a></strong></p> <ul> <li>I’ve always admired George Hotz for his unconventional ideas, and this one is no exception. It’s a movie plot he conceived, and while it may not be as mind-blowing as some of his other work, I enjoyed it.</li> </ul> <p><strong><a href="https://qntm.org/transi">Valuable Humans in Transit</a></strong></p> <ul> <li>This story exemplifies great science fiction writing. It masterfully employs the “show, don’t tell” principle. In good science fiction, especially hard science fiction, the context isn’t immediately clear. The story unfolds gradually, and the world-building reveals itself through the narrative. This story does exactly that. Initially, the narrator’s words might seem confusing, but the situation becomes grippingly clear as you continue reading.</li> </ul> <p><strong><a href="https://www.bloomberg.com/features/2024-ai-unlock-ancient-world-secrets/">Can AI Unlock the Secrets of the Ancient World?</a></strong></p> <ul> <li>This article details a competition where scientists are attempting to decipher scrolls that were burned and buried during the eruption of Mount Vesuvius. Using advanced CT scans and AI for pattern recognition, they’re making progress in reading these seemingly destroyed texts. I recall hearing about this project a couple of years ago, and seeing its fruition is truly astonishing. It reinforces my optimism about technology.</li> </ul> <p><strong><a href="https://waitbutwhy.com/2024/02/vision-pro.html">All My Thoughts After 40 Hours in the Vision Pro - Tim Urban</a></strong></p> <ul> <li>Tim Urban’s review of the Vision Pro resonated with me. While I agree with his overall assessment, I believe the convenience factor needs more consideration. A large display strapped to your face can be cumbersome and inconvenient, which might deter widespread adoption.</li> </ul> <p><strong><a href="https://www.wsj.com/tech/ai/sam-altman-openai-protected-by-silicon-valley-friends-f3efcf68">Sam Altman’s Knack for Dodging Bullets—With a Little Help From Bigshot Friends - WSJ</a></strong></p> <ul> <li>I’ve always recognized Sam Altman’s intelligence from his talks and podcasts, but I hadn’t realized the extent of his manipulative tendencies. This article provides a more grounded perspective on the OpenAI drama of late 2023, revealing it as a power struggle where Ilya Sutskever ultimately lost.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">The Inside Story of Microsoft’s Partnership with OpenAI - The New Yorker</a></strong></p> <ul> <li>This article offers Microsoft’s perspective on the OpenAI upheaval. It vividly portrays Microsoft’s frustration and how they leveraged their position to their advantage.</li> </ul> <p><strong><a href="https://www.newyorker.com/science/elements/what-are-dreams-for">What Are Dreams For? - The New Yorker</a></strong></p> <ul> <li>Explores the curious phenomenon of twitching during dreams. It turns out our brains aren’t causing the twitches; it’s the other way around. Our bodies twitch, and our brains respond. The hypothesis is that our brains are recalibrating by listening to our bodies during sleep. Fascinating!</li> </ul> <p><strong><a href="https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model">How Much of the World Is It Possible to Model? - The New Yorker</a></strong></p> <ul> <li>As someone intrigued by the simulation hypothesis, I expected this article to delve into the philosophical implications of simulating the world. Instead, it provided a concise overview of the concept and history of simulation. While it briefly touched on large language models as simulations of thought, I found it somewhat lacking in depth.</li> </ul> <p><strong><a href="https://rosslazer.com/posts/fine-tuning/">Fine-tuning GPT3.5-turbo based on 140k slack messages</a></strong></p> <ul> <li>This short blog post details the author’s experience fine-tuning GPT-3.5 with a massive dataset of Slack messages to mimic their writing style. The results, as shown in the image below, demonstrate that fine-tuning can be remarkably effective.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2008/06/30/the-itch">The Itch - The New Yorker</a></strong></p> <ul> <li>This article includes a fascinating quote: “If visual sensations were primarily received rather than constructed by the brain, you’d expect that most of the fibres going to the brain’s primary visual cortex would come from the retina. Instead, scientists have found that only twenty per cent do; eighty per cent come downward from regions of the brain governing functions like memory. Richard Gregory, a prominent British neuropsychologist, estimates that visual perception is more than ninety per cent memory and less than ten per cent sensory nerve signals.” This suggests our brains heavily compress visual information.</li> </ul> <p><strong><a href="https://www.newyorker.com/humor/daily-shouts/new-years-resolutions-for-an-anteater">New Year’s Resolutions for an Anteater - The New Yorker</a></strong></p> <ul> <li>A truly humorous piece. It makes me want to be an anteater!</li> </ul> <p><strong><a href="https://docs.anthropic.com/claude/prompt-library">Prompt Library - Anthropic</a></strong></p> <ul> <li>A useful library of prompts designed for use with Anthropic’s Claude model.</li> </ul> <p><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></strong></p> <ul> <li>This post effectively captures the essence of the “bitter lesson” and its implications for the development of large language models. The memes are also quite entertaining.</li> </ul> <p><strong><a href="https://www.palladiummag.com/2024/05/17/my-last-five-years-of-work/">My Last Five Years of Work - Palladium Magazine</a></strong></p> <ul> <li>Written by an Anthropic employee, this article reflects on the changing nature of work in the age of potential superintelligence. It offers a glimpse into the thoughts of AI lab insiders, although I don’t fully agree with the author’s predictions. Nevertheless, it’s a thought-provoking read.</li> </ul> <p><strong><a href="https://www.sequoiacap.com/article/ais-600b-question/">AI’s $600B Question</a></strong></p> <ul> <li>This article presents an interesting perspective: as technology advances, compute power will become cheaper, and GPUs will depreciate faster than anticipated. I believe there’s truth to this.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</a></strong></p> <ul> <li>This provides a detailed explanation of how to preprocess vast amounts of web data for LLM pretraining. It emphasizes the importance of careful deduplication and filtering for educational content using LLM labeling.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge">The Friendship That Made Google Huge</a></strong></p> <ul> <li>Another excellent New Yorker article, this one about Jeff Dean and his colleagues. It highlights Dean’s brilliance and how Google’s ability to scale was its true strength. I wonder if that still holds true today. The close intellectual synchronization between Jeff Dean and Sanjay Ghemawat, developed through years of collaboration, is particularly intriguing.</li> </ul> <p><strong><a href="https://nicholas.carlini.com/writing/2024/how-i-use-ai.html#background">How I Use AI - Nicholas Carlini</a></strong></p> <ul> <li>An insightful piece by a DeepMind researcher on how he utilizes AI. His approach aligns with my own and what I advocate for.</li> </ul>]]></content><author><name></name></author><category term="link"/><category term="AI"/><category term="sci-fi"/><summary type="html"><![CDATA[A collection of articles and videos that I read in 2024]]></summary></entry></feed>