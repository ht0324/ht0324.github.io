<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-26T16:12:02+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Thoughts on GPT-4o Image Generation</title><link href="https://ht0324.github.io/blog/2025/gpt4-image-generation/" rel="alternate" type="text/html" title="Thoughts on GPT-4o Image Generation"/><published>2025-03-26T01:30:00+00:00</published><updated>2025-03-26T01:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/gpt4-image-generation</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/gpt4-image-generation/"><![CDATA[<p>Today, OpenAI revealed GPT-4o’s <a href="https://openai.com/index/introducing-4o-image-generation/">image generation capabilities</a>. While this feature was previewed in the initial 4o announcement about <a href="https://openai.com/index/hello-gpt-4o/">a year ago</a>, the actual results are still surprising. Playing with it triggered some realizations about the future implications of direct image generation that I want to share here.</p> <hr/> <h3 id="the-multimodal-approach">The Multimodal Approach</h3> <p>The concept isn’t entirely new. When OpenAI introduced GPT-4o, they described it as modeling input as text, pixels, and sounds - combining all modalities with one big autoregressive transformer. The output would likewise be text, images, and audio.</p> <p>They presented it as straightforward, though obviously there are complicated architectural decisions behind the scenes. But the impact is clear: the model is much more capable at image generation.</p> <p>In previous systems (diffusion models or other chatbot interfaces), images were generated through a two-step process. The language model would generate a sophisticated prompt and feed that into a separate image model. GPT-4o eliminates this bottleneck by directly generating images.</p> <p>Since the large language model directly generates images, it’s much more intelligent in a way. We can also feed in images, and the results speak for themselves. The images show much more consistency. When previous chatbots wanted to modify an image, they had to create a detailed description of that image and feed it into another diffusion model. This was a bottlenecked process, but with 4o, it can handle images directly, modifying them with surprising consistency.</p> <p>What’s particularly impressive is how refined and enhanced the model’s text generation capabilities are within images. Previous diffusion models really struggled with generating clean, readable text in images - it was often distorted, nonsensical, or limited to just a few words. But GPT-4o can generate very clean text, and lots of it. The text is consistently readable and contextually appropriate.</p> <hr/> <h3 id="spark-of-software-20">Spark of Software 2.0</h3> <p>My most visceral moment came from an example where they showed a cat image being iteratively transformed into a game interface. Through multiple iterations, they turned it into an image of a game with a UI, and all the text was remarkably accurate. The model produced an image with a very nice interface, and everything was consistent.</p> <p>That’s when it hit me: if the model can generate UI and text so accurately, in the future our computer interfaces could be entirely AI-generated in real-time with all the context available from the user. Large language models could generate your computer interface frame by frame based on your input and feedback.</p> <p>Imagine a user interface that actually warps and changes based on user needs. Our current operating systems (macOS, Linux, Windows) are all rule-based with fixed definitions. But what if a large language model generated a new UI that helped the user get things done? It would be totally adaptive and different for every user - changing styles and functionality based on preferences or context.</p> <p>What would a word processor look like in that interface? The possibilities seem endless. Currently, an OS has the lower-level kernel with renderers and shaders that produce pixels. But this would be an end-to-end network - a large language model OS directly generating pixels from our input.</p> <p>The concept of world simulators isn’t entirely new - NVIDIA is already using their <a href="https://blogs.nvidia.com/blog/what-is-robotics-simulation/">Isaac Sim platform</a> to <a href="https://blogs.nvidia.com/blog/openusd-sdg-advance-robot-learning/">generate synthetic data for training robot models</a>, Google DeepMind has developed <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a> that can generate interactive 3D environments, and Microsoft Research recently unveiled <a href="https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/">Muse</a>, their first generative AI model designed for gameplay ideation. But what if instead of simulating physical worlds, we used these capabilities to simulate an operating system? That’s where my realization really hit me.</p> <p>One of the most compelling aspects of this approach would be how it leverages context. Context is incredibly important for large language models - your previous conversations, actions, and preferences inform their responses. Current operating systems gather tons of contextual information about how we use them, but this data isn’t being utilized well. What if an OS could learn and adapt from your previous interactions, inputs, preferences, and habits while you used it? Every aspect of your computing experience could be customized not by explicit settings, but by the system understanding you over time.</p> <p>This is basically <a href="https://karpathy.medium.com/software-2-0-a64152b37c35">Software 2.0</a> and <a href="https://x.com/karpathy/status/1723140519554105733">LLM OS</a> that Andrej Karpathy described. I was aware of this idea before, but never I have felt that it would be feasible in such a short time. I think with some effort, this kind of OS might be technically possible right now with large-scale servers and APIs, but it would be largely impractical. Yet the implications are huge, and I believe this kind of OS is inevitable.</p> <p>Of course, it wouldn’t be a fully end-to-end neural network. There would certainly be some rule-based systems guiding the LLMs. But my point still stands - we might be seeing the first glimpse of a new OS approach.</p> <p>Karpathy mainly discussed Software 2.0 replacing rule-based software stacks, which is already happening. But I think Software 2.0 will also eventually replace the OS stack itself.</p>]]></content><author><name></name></author><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[What GPT-4o's image capabilities tell us about the future of operating systems]]></summary></entry><entry><title type="html">Llama 3 Paper - Review (Part 2)</title><link href="https://ht0324.github.io/blog/2025/Llama3-part2/" rel="alternate" type="text/html" title="Llama 3 Paper - Review (Part 2)"/><published>2025-03-25T21:00:00+00:00</published><updated>2025-03-25T21:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Llama3-part2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Llama3-part2/"><![CDATA[<p>This is part 2 of my review of Meta’s Llama 3 technical paper. In <a href="/blog/2025/Llama3-part1">part 1</a>, I covered the core language model architecture, training methodology, and overall performance. Now I’ll dive into the multimodal aspects of the model (vision, video, and speech), which represent significant additions to the Llama ecosystem.</p> <p>What strikes me about Meta’s approach is their consistent focus on compositionality. Instead of training entirely new models from scratch, they extend the existing Llama 3 language models with specialized adapters. This pragmatic approach allows them to add new capabilities while preserving the existing text understanding abilities.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Compositional Multimodal Architecture</strong><br/> Meta adopts a modular, compositional approach for all multimodal capabilities in Llama 3. Rather than training joint models from scratch, they combine pre-trained language models with modality-specific encoders connected through adapter layers. This architecture has several advantages: it enables parallel development of language and vision/audio capabilities, avoids the complexities of joint training on multiple modalities, preserves text-only performance, and reduces computational overhead during inference by processing input modalities efficiently.</p> <p><strong>Vision Encoder and Adapter</strong><br/> The vision module consists of a pre-trained ViT-H/14 image encoder (modified to include 850M parameters) combined with cross-attention layers that connect the visual representations to the language model. These cross-attention layers are substantial, adding about 100B parameters to the 405B model. To preserve fine-grained visual information, they extract features from multiple intermediate layers of the vision encoder rather than just using the final layer output, which helps with tasks requiring detailed localization.</p> <p><strong>Video Recognition Architecture</strong><br/> The video module builds on the image module by adding two key components: a “temporal aggregator” that merges frames to capture temporal relationships, and dedicated video cross-attention layers. The aggregator uses a perceiver resampler architecture to compress multiple frames into a more compact representation. During pre-training, they start with 16 frames (aggregated to 1) and scale up to 64 frames during fine-tuning to handle longer videos more effectively.</p> <p><strong>Speech Understanding Approach</strong><br/> Unlike the vision module, the speech component doesn’t use cross-attention layers. Instead, it generates embeddings that directly integrate with text tokens in the language model. The speech module consists of a 1B-parameter Conformer encoder followed by a smaller adapter. Interestingly, this direct integration approach allows the speech interface to leverage the language model’s existing capabilities without modifying its parameters, which seems to work remarkably well at larger scales.</p> <p><strong>Speech Generation</strong><br/> For text-to-speech capabilities, Meta takes a different approach. Rather than fine-tuning the language model for speech generation, they implement a streaming text-to-speech system that uses Llama 3 embeddings to enhance text normalization and prosody modeling. The embeddings from Llama 3 are used to improve context-aware text normalization and more natural-sounding prosody.</p> <p><strong>Scaling and Training Challenges</strong><br/> Training these multimodal adapters introduces unique challenges beyond those faced when training the core language model. The model computation becomes heterogeneous (some tokens require more processing than others), data shows high variance in token counts across modalities, and there are numerical instability issues from combining different types of representations. Meta addresses these through clever pipeline design, sequence parallelization, and using higher precision for gradient accumulation.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Compositional Design Makes Technical Sense</strong><br/> Initially, I had concerns about the compositional approach. I wondered if mapping the higher-dimensional image modality into the latent space of a language model might cause significant information loss. While the dimensionality itself is an implementation detail, I believe the inherent modality of text fundamentally contains less information than images. However, when I saw GPT-4o generating remarkably accurate images from text prompts and handling complex visual tasks, it became clear that language model latent spaces are surprisingly robust at encoding visual concepts. This suggests the limitation I was worried about may not be as severe in practice. The cross-attention mechanism with multi-layer feature extraction appears to be particularly effective at preserving the detailed information from higher-dimensional modalities.</p> <p><strong>Multi-layer Feature Extraction Preserves Fine-grained Information</strong><br/> One insight I found particularly interesting was how they addressed the problem of CLIP-like models failing to preserve fine-grained localization information. Instead of relying solely on the final layer output, they extract features from multiple intermediate layers of the vision encoder (specifically the 4th, 8th, 16th, 24th, and 31st layers). This approach makes sense because the lower layers retain more spatial and detailed information before it gets abstracted away in higher layers. I hadn’t previously considered this limitation of contrastive learning approaches, but it explains why models like CLIP might struggle with tasks requiring precise visual details or localization.</p> <p><strong>Handling Many-shot Jailbreaking in Long Context Models</strong><br/> Something that caught my attention was the vulnerability of long-context models to many-shot jailbreaking attacks. It’s fascinating how the longer context window enables a new attack vector - they mentioned specifically that 256-shot attacks become possible. What’s impressive is how they mitigated this by fine-tuning models on datasets that include examples of safe behavior even when unsafe behavior appears in context. The fact that they could achieve this without impacting false refusal rates or helpfulness metrics shows the model’s ability to distinguish between demonstrations in context and actual instructions.</p> <p><strong>Safety Becomes Increasingly Granular</strong><br/> What struck me about Meta’s safety approach is how granular it’s becoming. Rather than having a generic “harmful content” classifier, they’re developing increasingly specialized safety mechanisms for specific types of risks. This categorization of safety concerns (into areas like cybersecurity, coding, spear-phishing, etc.) reveals how the frontier of AI safety is evolving from broad mitigation to very specific risk assessment and targeted interventions. It also suggests that as models get more capable, the attack vectors multiply, requiring more complex safety strategies.</p> <p><strong>Contextual Safety Challenges</strong><br/> The paper mentions that 256-shot attacks become possible with longer context windows, which I found fascinating. It shows that simply extending a model’s capabilities (like context length) can introduce entirely new safety vulnerabilities that weren’t relevant before. This suggests a kind of “safety debt” that comes with each capability enhancement - each new ability potentially opens up novel attack vectors that need additional mitigations.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Meta’s approach to expanding Llama 3’s capabilities into multimodal territory shows a thoughtful balance between pragmatism and innovation. Rather than developing entirely new architectures or joint pre-training approaches, they extend the existing language model through specialized adapters and compositional design. This strategy allows them to leverage the strengths of existing pre-trained components while adding new capabilities incrementally.</p> <p>The paper also highlights the ongoing tension between capability enhancement and safety. As models gain new abilities (like longer context), new vulnerabilities emerge that require additional mitigations. This suggests that safety work isn’t a one-time effort but an ongoing process that must evolve alongside model capabilities.</p> <p>As these models continue to develop, I’m particularly interested in seeing how the compositional approach scales to even more modalities and how it affects overall model capabilities. Does adding more modalities lead to emergent abilities through cross-modal transfer? Do certain modalities complement each other in unexpected ways? The Llama 3 paper doesn’t directly address these questions, but it provides a solid foundation for exploring them in future work.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Continuing my analysis of Meta's Llama 3 technical paper, focusing on multimodal capabilities and performance]]></summary></entry><entry><title type="html">Llama 3 Paper - Review (Part 1)</title><link href="https://ht0324.github.io/blog/2025/Llama3-part1/" rel="alternate" type="text/html" title="Llama 3 Paper - Review (Part 1)"/><published>2025-03-18T20:30:00+00:00</published><updated>2025-03-18T20:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Llama3-part1</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Llama3-part1/"><![CDATA[<p>Today I’m reviewing Meta’s Llama 3 technical paper. Due to the length and depth of the paper, I’ll be splitting this review into two parts - this is part 1, focusing on the pre-training and infrastructure aspects. The Llama 3 family represents a significant step up from Llama 2, with the flagship 405B parameter model performing competitively against leading models like GPT-4. What makes this paper particularly interesting is its comprehensive description of Meta’s approach to scaling, including data preparation, model training, and infrastructure challenges.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Scaling Up Pre-training</strong><br/> Llama 3 scales substantially beyond previous versions, with the flagship model using 405B parameters (compared to Llama 2’s much smaller size) and trained on approximately 15T tokens, versus 1.8T for Llama 2. The compute used for training the flagship model was nearly 50× more than Llama 2’s largest model. Most impressively, Meta made this massive scaling work with a standard dense Transformer rather than a Mixture-of-Experts (MoE) architecture.</p> <p><strong>Data Quality and Processing</strong><br/> Meta placed strong emphasis on data quality rather than just quantity. Their processing pipeline involved removing PII (personally identifiable information), applying multiple levels of deduplication (URL-level, document-level, and line-level), and using both heuristic and model-based filtering. They also used classifiers to identify and upsample high-quality code and reasoning content. For multilingual support, they implemented language-specific processing and quality filtering.</p> <p><strong>Context Length Scaling</strong><br/> Llama 3 was designed to handle context windows up to 128K tokens. Rather than training on long sequences from the beginning (which would be computationally prohibitive due to the quadratic scaling of self-attention), they used a multi-phase approach – first training on 8K contexts and then gradually increasing to 128K tokens in the final stages of pre-training over approximately 800B tokens.</p> <p><strong>Hardware and Infrastructure</strong><br/> Training at this scale required massive hardware resources and sophisticated infrastructure. The 405B model was trained on up to 16K H100 GPUs. They used a combination of tensor parallelism, pipeline parallelism, context parallelism, and data parallelism (what they call “4D parallelism”) to efficiently distribute computation. They report achieving 38-43% Model FLOPs Utilization (MFU), which is impressive at this scale.</p> <p><strong>Post-Training Alignment</strong><br/> After pre-training, the models underwent extensive post-training alignment using a combination of supervised fine-tuning (SFT), rejection sampling, and Direct Preference Optimization (DPO). One interesting note is their deliberate choice to avoid more complex reinforcement learning algorithms like PPO, which they found less stable and harder to scale.</p> <p><strong>Specialized Capabilities</strong><br/> The paper details multiple specialized capabilities added during post-training, including code generation (with execution-based feedback), multilingual performance, reasoning, tool-use, and factuality. For many of these, they developed specialized data generation pipelines, often leveraging earlier iterations of Llama 3 itself to generate training data.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Value of Simple, Stable Architectures</strong><br/> One of the most interesting choices Meta made was sticking with a dense Transformer architecture rather than using a Mixture-of-Experts approach. They explicitly state this was to “maximize training stability,” which signals that at huge scale, reliability and predictability might be more valuable than theoretical efficiency. This matches what DeepSeek researchers have also mentioned about the challenges of scaling MoE models.</p> <p><strong>Data Quality Trumps Architecture Complexity</strong><br/> The paper dedicates substantial space to discussing data curation, which suggests that data quality remains one of the most crucial factors for performance. Even with massive compute resources, Meta still invested heavily in filtering, curation, and quality assessment. Their use of multiple levels of deduplication, model-based quality filtering, and domain-specific pipelines reinforces how important data quality is to the final result.</p> <p><strong>Model-Bootstrapped Data Creation</strong><br/> The way Meta used earlier versions of Llama 3 to generate data for subsequent training iterations is fascinating. For specialized capabilities like code generation, they used a bootstrapping approach where the model itself generated samples, which were then filtered based on execution results. This self-improvement cycle – where models help train their successors – is becoming increasingly common but is still remarkable to see at this scale.</p> <p><strong>Context Parallelism for Long Sequences</strong><br/> The paper’s description of context parallelism (CP) for handling long sequences was particularly interesting. By dividing input sequences into chunks distributed across GPUs and using all-gather operations to collect key-value tensors, they managed to train on 128K context lengths without excessive memory usage. This approach differs from previous techniques I’ve seen and shows how specialized infrastructure is becoming for LLM training.</p> <p><strong>Alignment Complexity and Iteration</strong><br/> The post-training sections reveal how labor-intensive alignment still is. They performed six rounds of alignment, iteratively collecting human preferences, generating synthetic data, and fine-tuning. Each round built on the previous one, using increasingly capable models. The process involves carefully balancing data quality, variety, and complexity – and required extensive human annotation and quality control throughout.</p> <p><strong>Infrastructure Challenges at Scale</strong><br/> The sections on reliability and operational challenges highlight just how difficult training at this scale remains. During a 54-day period, they experienced 419 unexpected interruptions, with GPU issues accounting for nearly 60% of these. They also observed diurnal throughput variations of 1-2% due to environmental temperature fluctuations affecting GPU clocks. These details provide a sobering perspective on the practical challenges of pushing the boundaries of scale.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The Llama 3 paper provides valuable insights into how Meta approached training a competitive frontier model. While OpenAI and Anthropic maintain some lead with their proprietary models, Llama 3 demonstrates that with sufficient scale and careful engineering, it’s possible to build models that approach their capabilities while still releasing weights publicly.</p> <p>What’s most striking is the maturity of Meta’s approach. They’ve clearly learned lessons from previous iterations and focused on reliability, scalability, and maintainability rather than pursuing more exotic architectures. Their decision to use a dense Transformer architecture, combined with a stable and relatively simple alignment procedure, shows a preference for approaches that can be reliably scaled up.</p> <p>The level of infrastructure and pipeline engineering described in the paper is also remarkable. From their custom HTML parser to their extensive parallelism strategies to their reliability engineering, the paper makes clear that training models at this scale requires massive investment not just in raw compute, but in the systems that make that compute usable.</p> <p>Overall, this paper offers a detailed look into what it takes to build competitive models at scale. While the specific approaches may evolve, the general principles – quality data, reliable infrastructure, and careful alignment – are likely to remain important for future models as well.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[My analysis and thoughts on Meta's Llama 3 technical paper (Part 1)]]></summary></entry><entry><title type="html">Scaling Laws Paper - Review</title><link href="https://ht0324.github.io/blog/2025/Scaling-Laws/" rel="alternate" type="text/html" title="Scaling Laws Paper - Review"/><published>2025-03-17T14:00:00+00:00</published><updated>2025-03-17T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Scaling-Laws</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Scaling-Laws/"><![CDATA[<p>I recently reviewed the paper <a href="https://arxiv.org/abs/2001.08361">“Scaling Laws for Neural Language Models”</a>, a foundational work that has significantly shaped how we think about training large language models. This paper is particularly interesting because it’s co-authored by Dario Amodei, who would later leave OpenAI to co-found Anthropic, partly based on insights from this research. While scaling laws are now taken for granted in AI research, this paper represents their origins and formal documentation.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Power-Law Scaling Relationships</strong><br/> The paper demonstrates that language model performance improves predictably as we increase model size, dataset size, and compute used for training. These relationships follow power-law patterns, meaning they show linear improvements on a log-log scale. This clean, consistent pattern holds across many orders of magnitude.</p> <p><strong>Model Size vs. Dataset Size Trade-offs</strong><br/> One of the most interesting findings is the relationship between model size and data requirements. The paper found that performance penalty depends predictably on the ratio N^0.74/D, meaning every time we increase model size by 8x, we only need to increase data by roughly 5x to maintain performance. This sublinear relationship has major implications for efficient resource allocation.</p> <p><strong>Compute-Optimal Training</strong><br/> The paper shows there’s an optimal allocation of compute between model size and training tokens. As available compute increases, the optimal strategy shifts toward training very large models on relatively modest amounts of data, stopping significantly before convergence. This challenges the conventional wisdom that models should be trained until convergence.</p> <p><strong>Sample Efficiency of Large Models</strong><br/> Larger models are significantly more sample-efficient than smaller ones, reaching the same performance levels with fewer optimization steps and data points. This suggests that scaling up model size inherently leads to better generalization and learning capabilities.</p> <p><strong>Architectural Invariance</strong><br/> Surprisingly, the specific architectural details like network width, depth, or attention heads matter much less than simply scaling up the total parameter count. Within a wide range, these details have minimal effects on the final performance compared to the overall model scale.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Compute Allocation Is Critical</strong><br/> What struck me most was how the paper provides concrete guidance on allocating precious compute resources. As AI training demands more and more resources, understanding the optimal balance between model size, data, and training time becomes increasingly important. This paper gives us quantitative relationships to guide those decisions.</p> <p><strong>“Don’t Train to Convergence” Is Surprising</strong><br/> The finding that we can achieve optimal performance by training very large models but stopping significantly short of convergence was unexpected. This challenges the traditional training approach and suggests that rapid training of oversized models might be more compute-efficient than fully training smaller ones.</p> <p><strong>The “Bitter Lesson” Vindicated</strong><br/> This paper strongly aligns with Richard Sutton’s <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">“Bitter Lesson”</a> - the idea that methods leveraging computation tend to outperform human-engineered approaches. The scaling laws empirically validate this perspective, showing that simply scaling up compute and model size leads to predictable improvements without needing clever architecture innovations.</p> <p><strong>Data Requirements Grow Slowly</strong><br/> I was relieved to see that data requirements grow much more slowly than model size in the optimal regime. If this relationship were reversed, we would face much more severe data scarcity problems. This finding suggests that model size, not data, might be the primary bottleneck for future progress.</p> <p><strong>Anthropic’s Research Approach Is Evident</strong><br/> Reading this paper, I could see early signs of what would become Anthropic’s research philosophy. The experimental approach - running many controlled experiments to discover patterns rather than starting from a hypothesis - feels similar to their later work like the Transformer Circuits series. This paper seems to contain some of Anthropic’s research DNA.</p> <p><strong>Variables Lack Inherent Meaning</strong><br/> A limitation worth noting is that the specific coefficients and exponents in the scaling laws don’t have inherent meaning - they’re empirically determined and likely depend on the specific data used. While the general form of the relationship probably generalizes, the exact values might differ across domains.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The “Scaling Laws” paper provides a remarkably clear picture of how language model performance scales with model size, data, and compute. Its findings have shaped how the entire field approaches training large models, suggesting that bigger is not just better but also more efficient.</p> <p>What I appreciate most about this work is how it transforms vague intuitions into precise, quantitative relationships. By establishing these power laws, it gives us a framework for making rational decisions about resource allocation in AI training.</p> <p>The implications continue to reverberate through AI research. As we haven’t yet seen these scaling trends plateau, the guidance from this paper remains highly relevant. In many ways, the current race to build larger and more capable AI systems is a direct result of the insights this paper formalized.</p> <p>This paper truly takes the “Bitter Lesson” to heart - showing that scaling computation provides reliable returns without needing architectural breakthroughs. It’s a perspective that has proven incredibly fruitful for advancing AI capabilities.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing OpenAI's paper on scaling laws and the role of model size, data, and compute]]></summary></entry><entry><title type="html">How to Become Irreplaceable</title><link href="https://ht0324.github.io/blog/2025/Irreplacable/" rel="alternate" type="text/html" title="How to Become Irreplaceable"/><published>2025-03-14T15:00:00+00:00</published><updated>2025-03-14T15:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Irreplacable</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Irreplacable/"><![CDATA[<p>Lately, I’ve been thinking a lot about what it means to be irreplaceable—especially now, when it feels like AI is quickly changing what we consider valuable.</p> <p>Traditionally, intelligence was something people paid a premium for. If you were smart, knowledgeable, or skilled, society rewarded you. Intelligence was scarce and valuable. But today, especially after experiencing tools like ChatGPT, it feels clear that intelligence isn’t going to remain scarce for much longer.</p> <p>Just think about it—ChatGPT alone has made access to a certain level of intelligence cheaper than it’s ever been. Five years ago, you would’ve paid a significant amount to have someone tutor you, answer your questions, or explain concepts clearly. Now, anyone can access that kind of intelligence instantly, at a fraction of the cost.</p> <p>This trend isn’t slowing down. In fact, I’m pretty convinced it will accelerate. It’s <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> all over again. Large language models are becoming cheaper and more capable every year, pushing the cost of intelligence lower and lower. For someone like me, who went to university hoping that my knowledge and technical skills would set me apart, that’s a worrying thought.</p> <p>Honestly, I feel an existential anxiety about this. If intelligence itself becomes cheap, what does it mean to provide something truly valuable to society? How can I ensure my skills or knowledge remain valuable enough to earn a good living?</p> <p>The more I think about it, the clearer it becomes that intelligence alone won’t cut it. What will always be scarce—what people will always crave—isn’t intelligence, but identity. Human experiences, individuality, entertainment, personality—these are things machines can’t easily replace, no matter how intelligent they become.</p> <p>Take Taylor Swift as an example. People aren’t drawn to her simply because she’s intelligent or talented (though she is). They’re drawn to her as a person. Her identity—who she is, how she expresses herself—is something uniquely human and impossible to replicate with technology. Taylor Swift is irreplaceable not because of her skills, but because of her identity and the human connection she creates.</p> <p>That’s the kind of scarcity that holds up even as technology advances.</p> <p>So, where does that leave me? Right now, my skills and identity aren’t particularly unique. I’m not providing anything truly scarce. It’s a sobering realization, but also an important one. Maybe the key to staying valuable in this era isn’t about becoming smarter, but becoming more human—expressing something authentic, relatable, or simply enjoyable.</p> <p>I’m still figuring this out. But one thing seems clear: in a world where intelligence is increasingly abundant, identity might just become the most valuable asset we have.</p>]]></content><author><name></name></author><category term="Thought"/><category term="AI"/><category term="Life"/><summary type="html"><![CDATA[As intelligence becomes cheaper, what does it mean to be valuable?]]></summary></entry><entry><title type="html">Claude Code — Three Days In</title><link href="https://ht0324.github.io/blog/2025/Claude-Code-2/" rel="alternate" type="text/html" title="Claude Code — Three Days In"/><published>2025-03-07T12:00:00+00:00</published><updated>2025-03-07T12:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code-2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code-2/"><![CDATA[<p>It’s been three days since I started using <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>. The initial excitement was so strong that I immediately wrote a <a href="https://ht0324.github.io/blog/2025/Claude-Code/">blog post</a> about it. But after a couple more days, having spent more time exploring its capabilities, I’ve found myself with even more thoughts—so here’s a quick follow-up.</p> <hr/> <h3 id="time-saving-understated-yet-mind-blowing">Time-Saving: Understated Yet Mind-Blowing</h3> <p>I know I’ve already emphasized how much time Claude Code saves, but seriously, I can’t stress it enough. Over the past couple of days, I’ve been completely revamping my personal website from the ground up. Let me preface this by saying: I’m not really a web guy. Before Claude Code, my blog was just a clone of the <a href="https://alshedivat.github.io/al-folio/">Al Folio</a> theme. Whenever I wanted to tweak something, it felt daunting. Each tiny modification usually ended in frustration or compromise because the effort required to change something small just wasn’t worth it.</p> <p>Now, if I want to adjust the tag styling or add new functionality—boom, I just instruct Claude Code to do it. It happily handles the rest. The difference in my blog over just these past couple of days is truly night and day. I’ve implemented numerous nooks and crannies, subtle features, and design tweaks that previously felt impossible.</p> <p>Of course, Claude isn’t perfect, but the sheer amount of time it saves is staggering. To put it into perspective, yesterday I wasn’t satisfied with how the tag feature functioned, so I decided to change it up. Claude handled the entire process for about $1 in API costs. A dollar. Less than a cup of coffee. Without it, the same change would have probably taken me hours of tinkering and frustration.</p> <p>At first, Claude felt like a pair-programming partner, but even after just three days, that feeling is fading. Instead, it’s become something more like a virtual assistant—an assistant that can actually do things, not just talk about them.</p> <hr/> <h3 id="chatbots-vs-agents-what-is-vs-do-it">Chatbots vs. Agents: “What Is” vs. “Do It”</h3> <p>Using traditional chatbots like ChatGPT always felt like carefully constructing queries, specifying every little detail of my intent. Most of my interactions with ChatGPT—and likely yours too—are fundamentally knowledge retrieval: asking “What is this?” or “Tell me about that.” ChatGPT essentially compresses vast internet knowledge and provides succinct, precise answers. It’s exceptional at answering questions and explaining concepts, but it’s mostly passive—focused on understanding and summarizing existing knowledge.</p> <p>But Claude Code changed the game completely. With a truly agentic model like Claude, I don’t have to painstakingly spell out exactly how I want something done. Instead, I just specify <em>what</em> I want done, and Claude figures out the <em>how</em>. It doesn’t just summarize information; it actively plans, makes decisions, and implements changes autonomously. This shift from explicitly defined instructions (“what is”) to implicit, actionable intent (“do it”) feels genuinely profound—a step change in capability that transforms the user experience.</p> <p>Of course, because Claude isn’t perfect yet, the collaboration still exists in the form of verifying the results. But as I’ve <a href="https://medium.com/@FdForThought/framing-rlhf-as-generation-vs-verification-4d9e95b88534">said before</a> <a href="https://medium.com/@FdForThought/generation-vs-verification-epiphany-after-o1-713c6f411206">many times</a>: verification is vastly easier than generation, and right now, I’m comfortably on the verification side. I simply check Claude’s implementation by deploying my blog and testing the functionality. This still requires my input, but the mental load is drastically lighter.</p> <hr/> <h3 id="from-collaboration-to-automation">From Collaboration to Automation</h3> <p>While it still feels somewhat collaborative now, I’m realizing that my role is already shrinking from active collaborator to passive verifier. It’s an assistant relationship—not really a partnership anymore. Verification feels simpler and simpler, and honestly, I can foresee even verification becoming automated soon. At that point, human involvement becomes negligible.</p> <p>Previously, I believed in the paradigm that self-feedback loops (generation versus verification) would incrementally increase intelligence. But intelligence and agency are fundamentally different. Learning how to be smart/knowledgable and learning how to act are not the same, and the latter is far more powerful. I’m now convinced that agentic capability will lead to implications far bigger and faster than I initially anticipated.</p> <p>If (or more realistically, <em>when</em>) verification itself is automated, we won’t be collaborating with AI—we’ll be observing it collaborating with itself. That’s when things will get really interesting (and potentially unsettling).</p> <hr/> <h3 id="toward-agentic-intelligence">Toward Agentic Intelligence?</h3> <p>OpenAI previously described levels of AI intelligence: Level 2 (reasoners), Level 3 (agents), and Level 4 (collaborators or organizations). Claude Code sits loosely at Level 3, and my recent experiences hint strongly that Level 4—agents autonomously collaborating with each other—is closer than we might think.</p> <p>As these agentic models improve and increasingly handle their own verification, we’ll approach a point where human oversight becomes unnecessary. This isn’t speculation anymore; using Claude Code has convinced me that the shift toward fully autonomous agentic systems isn’t just possible—it’s imminent.</p> <p>The impact of that shift can’t be overstated. As impressive as Claude Code is today, it’s the worst this technology will ever be. And that’s both thrilling and deeply concerning.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[Further reflections on Claude Code]]></summary></entry><entry><title type="html">VQVAE – Review</title><link href="https://ht0324.github.io/blog/2025/VQVAE/" rel="alternate" type="text/html" title="VQVAE – Review"/><published>2025-03-05T11:30:00+00:00</published><updated>2025-03-05T11:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/VQVAE</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/VQVAE/"><![CDATA[<p>In this post, I review the paper <a href="https://arxiv.org/abs/1711.00937">“Neural Discrete Representation Learning”</a>, commonly known as VQ-VAE. This paper introduces a generative model that combines vector quantization with Variational Autoencoders (VAEs).</p> <p>Its core idea is replacing the continuous latent space typically used in VAEs with discrete embeddings. This addresses the notorious issue of posterior collapse, ensuring the encoder meaningfully contributes to data reconstruction rather than relying solely on a powerful decoder.</p> <p>The discrete latent space makes this model particularly suited to domains naturally represented by discrete data, like speech, language, and structured visual information.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Discrete Latent Variables</strong><br/> Instead of using continuous distributions, VQ-VAE encodes data into discrete latent variables selected from a predefined embedding dictionary. This discretization helps prevent posterior collapse by forcing the encoder to produce meaningful, constrained representations.</p> <p><strong>Vector Quantization (VQ)</strong><br/> Vector Quantization involves mapping encoder outputs to the nearest embeddings in a learned dictionary. Although there’s no straightforward gradient through this discrete step, the authors use a simple “straight-through” estimator, effectively copying gradients from decoder inputs back to encoder outputs.</p> <p><strong>Commitment Loss</strong><br/> To ensure the encoder doesn’t produce arbitrary embeddings, VQ-VAE introduces a commitment loss. This regularization term encourages encoder outputs to remain close to their assigned embedding vectors, stabilizing training and improving the quality of learned representations.</p> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Posterior Collapse is a Bigger Deal Than I Thought</strong><br/> Initially, I underestimated posterior collapse, thinking that if the decoder reconstructed well, the encoder must be doing its job. But I learned that’s not necessarily true—if the decoder is too powerful, it can bypass the encoder entirely, undermining the entire concept of an autoencoder. VQ-VAE addresses this directly through discretization.</p> <p><strong>Constraining the Latent Space is Helpful</strong><br/> Surprisingly, imposing discrete constraints on latent representations helps the model learn better. Intuitively, I thought constraints might harm performance, but VQ-VAE demonstrates that limiting flexibility actually prevents the model from “getting lost,” ultimately improving representation quality.</p> <p><strong>The Gradient Copying Trick is Surprisingly Effective</strong><br/> VQ-VAE’s training includes copying decoder input gradients directly to encoder outputs—a method that feels very ad-hoc. Despite its simplicity and my initial skepticism, this approach works remarkably well, suggesting that straightforward solutions can sometimes outperform more sophisticated ones.</p> <p><strong>Tokenization of Latent Space Could Lead to New Applications</strong><br/> By tokenizing the latent space, VQ-VAE opens potential avenues for using transformer architectures on latent representations. Given that transformers excel with discrete token sequences, VQ-VAE’s discrete embeddings might unlock new approaches for processing continuous data modalities as if they were language-like sequences.</p> <p><strong>Discrete Representations Naturally Align with Certain Data Types</strong><br/> I heard an hypothesis that VQ-VAE performs particularly well with inherently discrete data like language tokens or audio spectrograms. This makes intuitive sense, yet it remains an open question whether discrete latent spaces universally outperform continuous ones across various data domains.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>VQ-VAE introduces a straightforward yet effective method to discretize latent representations in autoencoders. By embracing discrete embeddings, it elegantly sidesteps posterior collapse and paves the way for new model architectures inspired by token-based learning.</p> <p>While discrete latent spaces offer promising advantages, further exploration is necessary to fully understand their limits and strengths across diverse applications.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[My thoughts on the VQ-VAE paper]]></summary></entry><entry><title type="html">Claude Code</title><link href="https://ht0324.github.io/blog/2025/Claude-Code/" rel="alternate" type="text/html" title="Claude Code"/><published>2025-03-04T10:59:59+00:00</published><updated>2025-03-04T10:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code/"><![CDATA[<p>Today, I used <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>, an experimental tool released by Anthropic a while ago. I really wanted to use it right at the start of the announcement, but I kept putting it off because I hadn’t found a use case for it yet. But today, I had to rapidly prototype an app, so I decided to give it a shot.</p> <hr/> <h3 id="claudes-agentic-capabilities">Claude’s Agentic Capabilities</h3> <p>Claude Code is basically <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which has been updated by Anthropic to have more agentic capabilities. They’ve essentially wrapped it with tools that can run commands and execute terminal operations. Claude can see your repository, view your file directory, edit files, and execute terminal commands. So, yeah, it has much more agency.</p> <p>I needed to quickly prototype an app that hosts images, allows filtering by tags, and enables image similarity search. And lo and behold, after just 2-3 hours, I had a functioning prototype.</p> <p>I haven’t used <a href="https://www.cursor.com/">Cursor</a>, the VS Code variant that gained traction some time ago and was widely praised by people in the Bay Area. But maybe now I’m experiencing the epiphany that Cursor users had. The experience wasn’t just about typing in my query and having Claude do everything from start to finish - it felt more like pair programming.</p> <p>It gave me the sense that there was a person behind the terminal, troubleshooting issues and debugging with me. Of course, Claude wasn’t perfect, but the back-and-forth, iterative process was refreshing and much more engaging than just randomly encountering errors and sifting through Stack Overflow links to fix them.</p> <hr/> <h3 id="a-new-kind-of-collaboration">A New Kind of Collaboration(?)</h3> <p>It was a massive time-saver. I provided high-level directions, Claude executed them, and when problems arose, we solved them together. The biggest surprise was how collaborative the process felt - it was much easier and more efficient than doing everything on my own, or switching back and fourth using a chat interface.</p> <p>If I had done this single-handedly, it would have taken days. But for just $3.50 in API costs (which might seem expensive depending on your threshold), I built a prototype that I’m pretty confident in within 2–3 hours.</p> <p>Claude Code also helped me fix bugs on my self-hosted blog. I’m more of an AI guy, not a web guy, so managing my blog often involved trial and error - throwing things at the wall and seeing what stuck. I had to manually fix bugs and do all sorts of tweaks to get it to look the way I wanted.</p> <p>But with Claude Code, I could simply query my repository - it became conversational. That made it much easier to fix persistent issues in a short amount of time. All in all, it’s an incredibly useful tool.</p> <hr/> <h3 id="collaboration-as-a-mirage">Collaboration as a Mirage</h3> <p>But the reason why it felt like a collaborative process is that Claude still isn’t perfect in its agentic execution. Don’t get me wrong - Claude is very capable, but it still requires human intervention. For complex tasks, it doesn’t always get things right on the first try.</p> <p>If we ever reach a point where AI no longer needs intervention - where it can figure out even highly complex instructions entirely on its own - then it would no longer feel like collaboration; it would be full automation.</p> <p>That said, I structured my app development process in a very step-by-step manner, executing tasks sequentially. Maybe that’s why I didn’t run into any major setbacks. If I had given more abstract instructions, I might have been more surprised by what Claude could do. But the key point remains: it still feels like a collaboration because Claude makes mistakes, and I had to intervene to correct them.</p> <p>Looking at the trajectory of AI progress, though, I don’t think this collaborative phase will last long. Eventually, it won’t be a partnership - it will be a replacement.</p> <hr/> <h3 id="the-inevitable-shift">The Inevitable Shift</h3> <p>I’ve recently seen posts on <a href="https://x.com/David_Kasten/status/1893357776702976286">X</a> of anecdotes that from recruiters at frontier labs who now assume that junior staffs are AI replaceable. I was initially skeptical of this claim, but now I kinda believe it. Also, in Andrej Karpathy’s <a href="https://x.com/karpathy/status/1894099637218545984">post</a>, he argues that agency is more valuable than intelligence. This struck me as profound.</p> <p>In terms of intelligence, today’s top LLMs are already more knowledgeable than humans. But in terms of agency, humans still have the upper hand. However, after my interaction with Claude Code, I caught a glimpse of a future where machines will reach agency supremacy as well. When that happens, it won’t be collaboration - it will be a full <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">Humans Need Not Apply</a>(outdated, but still relevant).</p> <p>I encourage everyone reading this to try it for themselves. Hearing about it isn’t the same as experiencing it firsthand. Maybe I’m exaggerating, but I stand by my belief that this kind of automation is a matter of when, not if. And this is the worst this technology will ever be, it’s only going to get better from here.</p>]]></content><author><name></name></author><category term="Blog"/><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[My experience with Claude Code]]></summary></entry><entry><title type="html">Link Archive - Feb 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/" rel="alternate" type="text/html" title="Link Archive - Feb 2025"/><published>2025-02-28T22:59:59+00:00</published><updated>2025-02-28T22:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/"><![CDATA[<p>Another month, another batch of interesting links I’ve come across. Here’s what caught my attention in February 2025.</p> <hr/> <p><strong><a href="https://youtube.com/watch?v=_1f-o0nqpEI">DeepSeek, China, OpenAI, and AI Megaclusters - Lex Fridman Podcast #459</a></strong></p> <ul> <li>I first encountered Dylan Patel on the Dwarkesh podcast with Asianometry, so I was glad to see him on Lex Fridman’s. He specializes in analyzing AI hyperscalers, so his insights are incredibly in-depth. Basically, highly recommended if you’re interested in the infrastructure side of things.</li> </ul> <p><strong><a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model</a></strong></p> <ul> <li>This is published by a group of researchers at Google DeepMind, particularly Sholto Douglas. It’s a step-by-step guide to scaling up compute using infrastructure, specifically TPUs. I’ve been working my way through it, and it starts with the very basics but gets pretty complicated quickly. It’s refreshing to see closed AI labs offering these “breadcrumbs” to the open-source community, sharing their knowledge.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></strong></p> <ul> <li>Similar in nature to the DeepMind scaling book, but this one’s from the Hugging Face team. It’s also very detailed, with an interactive guide, and, I should say, quite a mouthful.</li> </ul> <p><strong><a href="https://srush.github.io/raspy/">Thinking like Transformer</a></strong></p> <ul> <li>The author created a Python library and a detailed blog post explaining how computation actually works within a transformer network. He also developed an abstract conceptual framework for it. It’s really complicated—I still don’t fully grasp it—but it’s nonetheless fascinating.</li> </ul> <p><strong><a href="https://www.youtube.com/watch?v=a42key59cZQ&amp;list=WL&amp;index=2">Gwern - Anonymous Writer Who Predicted AI Trajectory on $12K/Year Salary</a></strong></p> <ul> <li>This is from the Dwarkesh Podcast. He introduces a blogger named Gwern, and you really need to read his work to understand. He has an extensive blog called <a href="http://gwern.net/">gwern.net</a>. His posts are incredibly in-depth and well-researched, more like separate articles. The sheer depth is staggering, unlike anything I’ve encountered, even in reputable sources like the New York Times. I think this guy is incredibly capable. It also made me think about how the current internet is dominated by big tech. But in the pre-big tech era, I heard there were these niche, individual creators posting unique content. Gwern feels like a holdover from that era.</li> </ul> <p><strong><a href="https://gwern.net/subculture">The Melancholy of Subculture Society - Gwern</a></strong></p> <ul> <li>Gwern’s writing is extensive and very long, so it requires dedicated time. This is one piece I read, and it’s a musing on how the internet, as technology evolves, is kind of segregating society, and the state of things. I think it’s a well-thought-out piece.</li> </ul> <p><strong><a href="https://zhengdongwang.com/2024/12/29/2024-letter.html">2024 Letter - Zhengdong Wang</a></strong></p> <ul> <li>This blog post, written by a researcher at Google DeepMind, makes a bombshell statement. Regarding the current state of their large language model research and machine learning models, he boldly claims that the models will essentially achieve anything if the evaluations are clearly defined. So, if there are evals, <em>any</em> evals, the model will succeed. It’s an audacious statement, but the implications are mind-blowing. I should probably write a dedicated blog post about this.</li> </ul> <p><strong><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">Sesame Research - Crossing the Uncanny Valley of Voice</a></strong></p> <ul> <li>This startup called Sesame revealed a really interesting voice-to-voice language model. They have a demo, and it works quite well. The key difference between Sesame and ChatGPT’s Advanced Voice Mode is that the model produces nuanced tones, like “ums” and “ahs,” very naturally. It feels incredibly realistic. For the first 10 minutes, I was blown away. But as I probed the model further, it became clear it doesn’t possess the same level of intelligence as LLMs. The same goes for ChatGPT’s Advanced Voice Mode; OpenAI seems to have significantly limited the model to comply with their guidelines. In Sesame’s case, I suspect the limitations are purely due to model and computational constraints.</li> </ul> <hr/> <p><br/> Hope you enjoyed this month’s picks—March should bring even more to explore!</p>]]></content><author><name></name></author><category term="Link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos I explored in February 2025]]></summary></entry><entry><title type="html">GAN – Review</title><link href="https://ht0324.github.io/blog/2025/GAN/" rel="alternate" type="text/html" title="GAN – Review"/><published>2025-02-27T18:00:00+00:00</published><updated>2025-02-27T18:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/GAN</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/GAN/"><![CDATA[<p>This time, I’m reviewing the classic paper <a href="https://arxiv.org/abs/1406.2661">“Generative Adversarial Nets”</a>, famously known as GAN. This 2014 paper by Ian Goodfellow and colleagues introduced a revolutionary method of training generative models using an adversarial setup between two neural networks—a generator and a discriminator.</p> <p>The central concept is simple yet powerful: a generator attempts to produce realistic samples to fool a discriminator, while the discriminator tries to distinguish real samples from generated ones. This competition pushes both models to improve continually, ultimately enabling the generator to produce incredibly realistic outputs without directly modeling complex probability distributions.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Generator and Discriminator</strong><br/> GAN consists of two neural networks trained simultaneously. The <strong>Generator (G)</strong> learns to produce data that looks like real samples from random noise, and the <strong>Discriminator (D)</strong> learns to classify whether a given sample is real or fake. This interplay drives continuous improvement.</p> <p><strong>Minimax Game and Value Function</strong><br/> GAN training is framed as a minimax game, where D tries to maximize its accuracy, while G tries to minimize it. Mathematically, this interaction is captured by a value function involving two competing optimization steps, one ascending (for D) and one descending (for G).</p> <p><strong>Noise as Input (Latent Representation)</strong><br/> The generator takes random noise as input (often standard Gaussian noise), which it transforms into realistic data. This noise acts as a latent representation, similar in purpose to latent spaces in other generative models like VAE or diffusion models.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>An Intuitive “Police vs. Counterfeiters” Paradigm</strong><br/> The analogy used by the authors—police versus counterfeiters—made the adversarial training idea very intuitive. The discriminator is essentially a “police officer,” trying to catch counterfeit samples, while the generator, acting as “counterfeiters,” constantly improves to evade detection. This simple analogy clarified the competitive dynamics at the heart of GANs.</p> <p><strong>Why the Order of Optimization Matters</strong><br/> One subtle but critical insight is the optimization order: in theory, the discriminator should reach optimality first before the generator updates. This sequential ordering makes sense because the discriminator provides the target (“answer sheet”) for the generator. Practically, training typically alternates between discriminator and generator updates, often simplifying to a 1:1 step ratio, despite theoretical ideals suggesting otherwise.</p> <p><strong>The “Saturation” Problem</strong><br/> Initially, the idea of the generator’s gradient saturating (becoming ineffective) was unclear. The authors point out that if the discriminator becomes too strong too early, the generator’s gradients become nearly zero because it consistently outputs samples easily identified as fake. Understanding this clarified the importance of balancing the discriminator and generator strengths.</p> <p><strong>Noise as a Form of Latent Space</strong><br/> Initially, calling the generator input “noise” felt unintuitive. Why noise? After considering diffusion models and VAE, I realized noise serves as a random seed or latent code that gets mapped to structured data. Essentially, it’s just a convenient way to introduce randomness into the otherwise deterministic neural network framework, enabling continuous generation and meaningful interpolation between points in this latent space.</p> <p><strong>Interpolation and Connection to VAEs</strong><br/> Interestingly, GANs achieve interpolation naturally despite the absence of a clearly defined encoder (unlike VAEs). VAEs explicitly model latent spaces to allow interpolation, but GANs achieve this indirectly. Because the generator learns from the discriminator’s feedback rather than directly fitting discrete data points, it inherently learns a continuous representation. This indirect approach was fascinating because it avoids explicitly modeling complicated distributions yet still produces meaningful interpolations.</p> <p><strong>Simplicity of Theoretical Results</strong><br/> The theoretical results, particularly the global optimality condition (pg = pdata), were straightforward yet elegant. By defining an optimal discriminator, the proof shows a simple Jensen-Shannon divergence between the true and generated distributions. Unlike VAE’s mathematically heavy derivations, GAN presents a more intuitive theoretical grounding.</p> <p><strong>GANs vs. VAEs – Simpler but Powerful</strong><br/> Reflecting on VAE and GAN together, I realized GAN’s elegance lies in its simplicity. While VAE explicitly approximates intractable distributions with mathematical rigor, GAN sidesteps complexity through the clever adversarial setup. This simplicity initially may have seemed too naive to work, but history proved otherwise, showing that GANs indeed leveraged the powerful discriminative capabilities already established in neural networks.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>“Generative Adversarial Nets” introduced a simple yet groundbreaking framework that pits generators against discriminators to produce realistic data without explicitly modeling complex distributions. Its intuitive adversarial concept, practical simplicity, and powerful theoretical grounding explain why GANs rapidly became foundational in generative modeling.</p> <p>Exploring GAN alongside VAE deepened my understanding of how different approaches tackle generative tasks, showing GAN’s unique combination of simplicity, intuition, and effectiveness.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the original GAN paper]]></summary></entry></feed>