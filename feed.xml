<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-25T15:38:48+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A personal website of Hun Tae Kim
</subtitle><entry><title type="html">My Take on GPT-5</title><link href="https://ht0324.github.io/blog/2025/gpt-5/" rel="alternate" type="text/html" title="My Take on GPT-5" /><published>2025-08-18T04:51:00+00:00</published><updated>2025-08-18T04:51:00+00:00</updated><id>https://ht0324.github.io/blog/2025/gpt-5</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/gpt-5/"><![CDATA[<p>OpenAI recently released GPT-5, with claims of a new state-of-the-art model that tops benchmarks. After spending some time with it, my initial impression is that while it’s a decent model, it doesn’t feel groundbreaking. However, I’ve come to realize that this release wasn’t really intended for power users like me. For the majority of people, this model is a game-changer.</p>

<h3 id="the-divide-between-power-users-and-everyday-users">The Divide Between Power Users and Everyday Users</h3>

<p>Before GPT-5, I used OpenAI’s o3 model almost exclusively since March. As I’ve discussed in a <a href="/blog/2025/o3-o4mini2/">previous post</a>, I have high regard for o3, mainly because of its “agentic” nature—it could actively surf the web to gather context and provide more reliable answers. This ability to use tools like web search and retrieve context on its own, in my opinion, separates a useful AI from a toy.</p>

<p>This is why it sometimes frustrates me to see friends and colleagues, even those with a ChatGPT Plus subscription, stick to the basic GPT-4o model. They often complain that it hallucinates or makes things up, and when I ask which model they’re using, most of the time it’s the 4o model. A model without a dedicated reasoning process and tool usage is going to be less reliable for complex tasks. I’ve made it a personal rule to never trust a non-reasoning model for anything beyond simple tasks like drafting an email or editing my writing.</p>

<p>The value of a “thinking” model comes from test-time compute scaling. When you allow a model to “think harder” about a problem, the quality of the result is much better than what a non-reasoning model can produce. With GPT-5, this capability is now dynamically available to everyone.</p>

<h3 id="the-router-a-cornerstone-for-a-new-business-model">The Router: A Cornerstone for a New Business Model</h3>

<p>The most significant change with GPT-5 isn’t the base model itself, but the introduction of the <strong>router</strong>. This system dynamically decides whether a query requires the deeper “GPT-5 Thinking” model or can be handled by a simpler one.</p>

<p>A recent <a href="https://semianalysis.com/2025/08/13/gpt-5-ad-monetization-and-the-superapp/">article</a> from SemiAnalysis by Dylan Patel and his team really opened my eyes to the business implications of this. They argue that the router is the cornerstone for OpenAI to finally monetize its massive base of free users. The router can distinguish between a trivial query like, “What is the capital of France?” and a commercially valuable one like, “What are the best running shoes I can buy?”</p>

<p>The first query doesn’t require deep reasoning and is cheap to answer. The second, however, has high commercial intent. The router can allocate more resources to it, use web search, and provide a detailed, reasoned recommendation. This creates an opportunity for OpenAI to take a transaction fee or affiliate revenue, turning the chatbot into a monetizable super-app. It’s a way to monetize without resorting to intrusive ads, which Sam Altman has expressed a distaste for.</p>

<p>While I agree that the router enables this, I’d push back slightly and argue that a sufficiently advanced model could theoretically make these decisions on its own. Still, implementing it as a dedicated router is a clear and deliberate step toward this new paradigm.</p>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>My experience with GPT-5 has solidified a key belief: always use a thinking model. Since its release, I’ve used “GPT-5 Thinking” exclusively, and I don’t care about the automatic routing for my own use.</p>

<p>If you’re reading this, the main takeaway I want to leave you with is this: whenever you have the choice, opt for the model that thinks. The difference in quality and reliability is night and day. For the average user, GPT-5’s greatest gift is making that choice for them, seamlessly bringing the power of reasoning to hundreds of millions of users for the first time.</p>]]></content><author><name></name></author><category term="Thoughts" /><category term="AI" /><summary type="html"><![CDATA[First impressions]]></summary></entry><entry><title type="html">The Murmuring Woman</title><link href="https://ht0324.github.io/blog/2025/murmuring/" rel="alternate" type="text/html" title="The Murmuring Woman" /><published>2025-05-10T11:00:00+00:00</published><updated>2025-05-10T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/murmuring</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/murmuring/"><![CDATA[<p>I had a peculiar experience today that I wanted to share. I was at a cafe with my girlfriend, planning our vacation. Nearby, there was a woman, maybe in her mid-40s, wearing a face mask. She was constantly murmuring to herself – not loudly, I couldn’t make out the words, but it was non-stop. She looked really agitated, getting up, sitting down slightly differently, pacing out of the room and then returning to the same seat. This went on and on. As we wrapped up our trip planning, I looked up, and she was just gone. That quick.</p>

<p>Our vacation plans came together well, but the image of that woman kind of lingered in my head. When I saw her, it really reminded me of large language models. I had to admit that my brain is so stuffed with AI these days – for better or worse.</p>

<p>Hear me out.</p>

<h3 id="from-self-talk-to-chain-of-thought">From Self-Talk to Chain-of-Thought</h3>

<p>The woman’s constant self-talking, that murmuring, felt exactly like what chain-of-thought reasoning models are currently doing. It’s a very simple, almost too easy analogy, but if you start to put weight on it, it feels really, really profound. I don’t know why this happens, but I find that anthropomorphizing large language models sometimes helps me see what capabilities they might need or what data we should give them to make them more capable. These kinds of analogies make it easier for me to see things.</p>

<p>There’s a sort of stack here, a progression:</p>
<ol>
  <li><strong>Traditional LLMs:</strong> These models don’t really “think” in a step-by-step way. They just generate, often verbatim, without much pause – a kind of knee-jerk reaction. This is like System 1 thinking.</li>
  <li><strong>Reasoning Models (Chain-of-Thought):</strong> When these came along, they blew the older models out of the water. This introduced a new scaling paradigm: test-time compute. Introspection, or thinking step-by-step, is much better than a knee-jerk response for many tasks. This is System 2, and it’s really good for improving capabilities. <a href="https://www.youtube.com/watch?v=eaAonE58sLU">Noam Brown’s work</a> really pioneered this area.</li>
</ol>

<h3 id="the-limits-of-introspection-and-the-need-for-tools">The Limits of Introspection and the Need for Tools</h3>

<p>Now, what current models are doing is moving towards becoming agents. And here’s where the analogy with the woman (and I want to be clear, I don’t know her or what she was going through, but she really looked like she was having a tough time – this is just an observation for the sake of analogy) becomes even more interesting.</p>

<p>Constant introspection, just talking to oneself, only gets you so far. And that’s exactly the limit I see with first-generation reasoning models, like some of the DeepSeek models or OpenAI’s o1. They can think, they can “talk to themselves” on and on, but they can’t verify their own thoughts quite reliably.</p>

<p>Compare this to how people generally operate. When “normal” people think, they can self-verify using external tools or interactions. They might talk something through with someone else for verification, or rely on external aids like their iPhone, a book, or a quick search. This analogy is simple. And that’s what models like Anthropic’s Claude 3.7 Sonnet and OpenAI’s o3 are doing now. They are good at interacting with the real world via an external pipeline, a bridge we call “tools.”</p>

<h3 id="the-fine-line-of-anthropomorphism">The Fine Line of Anthropomorphism</h3>

<p>When you anthropomorphize a large language model this way, the need for tools and external interaction becomes obvious. But there’s a key caveat: the internal modeling of an LLM is very different from human cognition. I’m anthropomorphizing for the sake of seeing what LLMs might <em>benefit from</em>, what might make them more capable. It’s a fine line to walk.</p>

<h3 id="arent-we-all-just-next-word-predictors">Aren’t We All Just Next-Word Predictors?</h3>

<p>As this kind of anthropomorphization continues, and it’s easy to do because language models can seem persuasive and lifelike, it reminded me of something <a href="https://youtube.com/watch?v=XgCHZ1G93iA&amp;t=438">Scott Aaronson said</a> a year ago. When LLMs first emerged and there were naysayers arguing “it’s just next-word prediction, just statistical modeling,” he’d retort (paraphrasing) “But what about you? Aren’t <em>you</em> just a next-word predictor? What about your mom?”</p>

<p>It really kind of cracked me up at the time. If I’d said that to some of my close friends who looked down upon LLMs, they would have fumed, they’d be outraged! But when ChatGPT came out, I intuitively, wholeheartedly agreed with Aaronson’s point. My mind hasn’t changed on that.</p>

<p>I think as models get more capable, Aaronson’s quip: “aren’t we just the next-word predictor?”, will become true in a functional sense. Recently, LLMs passed the turing test, but society has moved on like nothing happened. Sooner or later, for every verifiable task, model capabilities will likely exceed human capabilities. And still, when that happens, they will be, at their core, next-word predictors. Superhuman next-word predictors, better than us at any given task.</p>

<p>Then what would we become?</p>]]></content><author><name></name></author><category term="Thoughts" /><category term="AI" /><summary type="html"><![CDATA[A Parable for LLM Thinking]]></summary></entry><entry><title type="html">Dropout - Review</title><link href="https://ht0324.github.io/blog/2025/dropout/" rel="alternate" type="text/html" title="Dropout - Review" /><published>2025-04-28T10:00:00+00:00</published><updated>2025-04-28T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/dropout</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/dropout/"><![CDATA[<p>Dropout is one of those techniques in deep learning that feels ubiquitous, yet revisiting the original 2014 paper, “<a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>” by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov, reveals layers of insight beyond just “randomly turn off neurons.” It’s a paper grounded in intuition and addresses a fundamental challenge in training powerful models.</p>

<h3 id="the-evolutionary-analogy-mix-ability-over-brittle-co-adaptation">The Evolutionary Analogy: Mix-ability over Brittle Co-adaptation</h3>

<p>One of the most striking parts of the paper is the motivation drawn from evolutionary biology, specifically the role of sexual reproduction.</p>

<blockquote>
  <p>“One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes.”</p>
</blockquote>

<p>The paper contrasts this with asexual reproduction, where a well-adapted set of genes might be perfectly optimized for a specific environment but could be brittle if conditions change. Sexual reproduction, by constantly shuffling genes, forces individual genes to be effective in collaboration with a <em>random</em> set of other genes. This “mix-ability” fosters robustness.</p>

<p>This analogy maps beautifully onto neural networks. A standard network might develop complex “co-adaptations” between hidden units, perfectly fitting the training data but failing on unseen examples. Dropout, by randomly removing units during training, acts like gene shuffling. It forces each unit to be useful on its own or in conjunction with various randomly chosen subsets of other units. This prevents the network from relying on fragile partnerships that only exist in the training data, promoting robustness. As the paper humorously adds, ten small conspiracies might be more robust than one large one requiring everyone to play their part perfectly.</p>

<h3 id="the-real-goal-generalizing-to-the-test-set">The Real Goal: Generalizing to the Test Set</h3>

<p>This ties into a crucial point, echoing sentiments sometimes expressed by researchers like Ilya Sutskever: the ultimate objective isn’t just fitting the training data, but generalizing to the <strong>test set</strong>. The paper highlights this early on:</p>

<blockquote>
  <p>“With limited training data, however, many of these complicated relationships will be the result of sampling noise, so they will exist in the training set but not in real test data even if it is drawn from the same distribution. This leads to overfitting…”</p>
</blockquote>

<p>Dropout directly attacks this problem. Overfitting often involves learning spurious correlations, patterns that exist purely by chance in the training sample. Standard networks, especially high-capacity ones, have the “luxury” of using their parameters to memorize this noise to minimize training loss.</p>

<h3 id="dropouts-incentive-learning-robust-features-not-noise">Dropout’s Incentive: Learning Robust Features, Not Noise</h3>

<p>Dropout changes the incentive structure during training. By constantly disrupting pathways (randomly dropping units), it makes it significantly harder for the network to rely on specific, complex interactions between neurons that might only capture spurious correlations. The “reward” (gradient signal) for learning these fragile patterns becomes inconsistent.</p>

<p>Conversely, strong, prominent features that reflect the true underlying data structure are likely detectable through multiple, more robust pathways or redundant representations. These features “survive” the dropout process more reliably, receiving more consistent positive reinforcement. Dropout, therefore, incentivizes the network to invest its capacity in learning features that are resilient to this random disruption – precisely the features most likely to generalize.</p>

<h3 id="the-mechanism-approximating-an-exponential-ensemble">The Mechanism: Approximating an Exponential Ensemble</h3>

<p>The core mechanism is elegant:</p>
<ol>
  <li><strong>During Training:</strong> For each training case (or minibatch), randomly “thin” the network by dropping units (setting their output to zero) with a certain probability <code class="language-plaintext highlighter-rouge">1-p</code>. This means training an exponentially large ensemble of networks (potentially 2^N for N units) that all share weights.</li>
  <li><strong>At Test Time:</strong> Explicitly averaging the predictions of all possible thinned networks is intractable. Instead, use the single, full network but scale down the outgoing weights of units by the retention probability <code class="language-plaintext highlighter-rouge">p</code>. This simple scaling provides a good approximation of the average prediction of the ensemble.</li>
</ol>

<p>This allows training what is effectively a huge ensemble but performing inference efficiently with a single model.</p>

<h3 id="synergy-with-max-norm-regularization">Synergy with Max-Norm Regularization</h3>

<p>The paper notes that dropout often works best with high learning rates and momentum. However, this can risk weights growing uncontrollably. They found a specific technique particularly helpful: <strong>Max-Norm Regularization</strong>.</p>

<blockquote>
  <p>“…constraining the norm of the incoming weight vector at each hidden unit to be upper bounded by a fixed constant c. In other words, if w represents the vector of weights incident on any hidden unit, the neural network was optimized under the constraint IIwII₂ ≤ c.”</p>
</blockquote>

<p>This acts as an important stabilizer. By capping the magnitude (L2 norm) of incoming weights to each neuron, it prevents weights from exploding, allowing the use of aggressive learning rates needed to overcome the noise introduced by dropout, without sacrificing stability.</p>

<h3 id="sparsity-as-a-side-effect">Sparsity as a Side-Effect</h3>

<p>Interestingly, the paper demonstrates (Figures 7 &amp; 8) that dropout often leads to sparser activations in hidden units, even without explicit sparsity-inducing penalties. Neurons learn to be more selective, potentially making the learned representations more interpretable or efficient.</p>

<h3 id="final-thoughts-foundational-simplicity">Final Thoughts: Foundational Simplicity</h3>

<p>Dropout shows the power of simple, well-motivated ideas. It provides a computationally feasible way to prevent overfitting by mimicking evolutionary pressure towards robustness and discouraging the memorization of spurious, training-set-specific correlations. While not a silver bullet (factors like training time and interaction with Batch Normalization mean it’s not always the best choice), its impact on deep learning has been large. Reading the original paper clarifies <em>why</em> it works: it’s not just random noise; it’s structured noise that changes the network’s learning incentives towards generalization.</p>]]></content><author><name></name></author><category term="Paper" /><category term="AI" /><summary type="html"><![CDATA[Revisiting the foundational 2014 paper on Dropout]]></summary></entry><entry><title type="html">Rethinking Sequence-to-Sequence - Review</title><link href="https://ht0324.github.io/blog/2025/attention/" rel="alternate" type="text/html" title="Rethinking Sequence-to-Sequence - Review" /><published>2025-04-26T09:00:00+00:00</published><updated>2025-04-26T09:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/attention</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/attention/"><![CDATA[<p>Reading foundational papers often provides a clearer perspective on how current ideas evolved. Recently, I went through the 2015 ICLR paper “<a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>” by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. It tackles a core problem in early sequence-to-sequence models for machine translation.</p>

<p>The main issue they identified was the “bottleneck” inherent in the standard RNN Encoder-Decoder framework popular at the time (like in Cho et al., 2014a or Sutskever et al., 2014). These models tried to compress the entire meaning of a source sentence, regardless of its length, into a single fixed-length vector. As the paper noted, this makes it difficult to handle long sentences well, performance tended to drop off significantly as sentences got longer.</p>

<p>Their proposed solution was to allow the decoder to look back at the source sentence and selectively focus on relevant parts when generating each target word. This avoids forcing all information through one fixed vector.</p>

<h3 id="key-concepts">Key Concepts</h3>

<p>Here’s a breakdown of the core ideas discussed:</p>

<ul>
  <li><strong>The Problem: Fixed-Length Vector Bottleneck:</strong> Standard encoder-decoders map an input sequence <code class="language-plaintext highlighter-rouge">x = (x_1, ..., x_{T_x})</code> to a fixed context vector <code class="language-plaintext highlighter-rouge">c</code>. The decoder then generates the output <code class="language-plaintext highlighter-rouge">y = (y_1, ..., y_{T_y})</code> based solely on <code class="language-plaintext highlighter-rouge">c</code> and previously generated words. This compression limits the model’s capacity, especially for long inputs.</li>
  <li><strong>The Solution: Alignment Mechanism (Decoder Focus):</strong> Instead of one <code class="language-plaintext highlighter-rouge">c</code>, the proposed model computes a <em>distinct</em> context vector <code class="language-plaintext highlighter-rouge">c_i</code> for each target word <code class="language-plaintext highlighter-rouge">y_i</code>. This <code class="language-plaintext highlighter-rouge">c_i</code> is a weighted sum of <em>annotations</em> <code class="language-plaintext highlighter-rouge">(h_1, ..., h_{T_x})</code> from the encoder. Each <code class="language-plaintext highlighter-rouge">h_j</code> corresponds to a source word <code class="language-plaintext highlighter-rouge">x_j</code> (or rather, the hidden state around it).</li>
  <li><strong>How it Works: Alignment Model &amp; Context Vector:</strong>
    <ul>
      <li>The weight <code class="language-plaintext highlighter-rouge">a_{ij}</code> for each annotation <code class="language-plaintext highlighter-rouge">h_j</code> when generating <code class="language-plaintext highlighter-rouge">y_i</code> depends on how well the input around position <code class="language-plaintext highlighter-rouge">j</code> aligns with the output at position <code class="language-plaintext highlighter-rouge">i</code>.</li>
      <li>These weights are calculated using an “alignment model” <code class="language-plaintext highlighter-rouge">a</code>, which takes the previous decoder hidden state <code class="language-plaintext highlighter-rouge">s_{i-1}</code> and the encoder annotation <code class="language-plaintext highlighter-rouge">h_j</code> as input to produce a score <code class="language-plaintext highlighter-rouge">e_{ij}</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">e_{ij} = a(s_{i-1}, h_j)</code></li>
      <li>The weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> are obtained by normalizing these scores with a softmax: <code class="language-plaintext highlighter-rouge">a_{ij} = exp(e_{ij}) / Σ_k exp(e_{ik})</code>.</li>
      <li>The context vector <code class="language-plaintext highlighter-rouge">c_i</code> is then the weighted sum: <code class="language-plaintext highlighter-rouge">c_i = Σ_j a_{ij} h_j</code>.</li>
      <li>Crucially, the alignment model <code class="language-plaintext highlighter-rouge">a</code> (parameterized as a small feedforward network) is trained <em>jointly</em> with the rest of the system.</li>
    </ul>
  </li>
  <li><strong>Soft vs. Hard Alignment:</strong> The paper uses the term “soft alignment.” This contrasts with “hard alignment,” which would involve making a deterministic choice of which single source word aligns with the target word. Soft alignment uses a weighted average over <em>all</em> source annotations. This makes the mechanism differentiable and allows the model to learn alignments implicitly through backpropagation. It also naturally handles situations where a target word might depend on multiple source words, or vice-versa.</li>
  <li><strong>The Encoder: Bidirectional RNN (BiRNN):</strong> To ensure the annotation <code class="language-plaintext highlighter-rouge">h_j</code> captures context from both before and after the source word <code class="language-plaintext highlighter-rouge">x_j</code>, they used a BiRNN. This consists of a forward RNN processing the sequence from <code class="language-plaintext highlighter-rouge">x_1</code> to <code class="language-plaintext highlighter-rouge">x_{T_x}</code> and a backward RNN processing it from <code class="language-plaintext highlighter-rouge">x_{T_x}</code> to <code class="language-plaintext highlighter-rouge">x_1</code>. The annotation <code class="language-plaintext highlighter-rouge">h_j</code> is the concatenation of the forward hidden state <code class="language-plaintext highlighter-rouge">\vec{h}_j</code> and the backward hidden state <code class="language-plaintext highlighter-rouge">\cev{h}_j</code>. While BiRNNs weren’t new, their use here makes sense for creating richer annotations.</li>
</ul>

<h3 id="key-takeaways">Key Takeaways</h3>

<p>Reflecting on the paper, several points stand out:</p>

<ul>
  <li><strong>Performance Improvement (Especially on Long Sentences):</strong> The results (Figure 2, Table 1) clearly show the benefit. The standard RNNencdec model’s performance drops sharply with sentence length, while the proposed RNNsearch model remains much more robust. The BLEU scores confirm a significant improvement, bringing NMT closer to traditional phrase-based systems of the time.</li>
  <li><strong>Interpretability via Alignment:</strong> The alignment weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> can be visualized (Figure 3). This provides insight into what parts of the source sentence the model focuses on when generating a specific target word. The visualizations showed mostly monotonic alignments (as expected between English and French) but also the ability to handle local reordering (like adjective-noun flips) correctly. This interpretability is a nice side effect compared to trying to understand a monolithic RNN.</li>
  <li><strong>Handling Reordering and Length Differences:</strong> The soft alignment naturally deals with source and target phrases having different lengths or requiring non-trivial mappings, without needing explicit mechanisms like NULL tokens used in traditional SMT.</li>
  <li><strong>Evolutionary Link to Transformers:</strong> Reading this <em>after</em> knowing about Transformers makes the connection clear. The core mechanism, scoring source annotations based on the current decoder state, using softmax for weights, and computing a weighted sum, is essentially the attention mechanism. It reads as a precursor; the Transformer built upon this by removing recurrence and adding multi-head attention, positional encodings, etc. It’s like seeing an earlier stage in the “evolution” of sequence models.</li>
</ul>

<h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3>

<p>This paper reads as a pivotal step in NMT. It directly addressed a clear limitation (the fixed-length vector bottleneck) with a straightforward solution: allowing the model to learn where to focus in the source sequence. The “soft alignment” mechanism introduced is, in essence, the attention mechanism that became central to later architectures like the Transformer.</p>

<p>Looking back now, the ideas seem intuitive, but implementing this effectively and showing its benefits in 2014/2015 was a contribution. It’s a clear paper that explains the problem, the proposed solution, and provides evidence. Reading it helps appreciate the progression of ideas leading to the models we use today.</p>]]></content><author><name></name></author><category term="Paper" /><category term="AI" /><summary type="html"><![CDATA[Looking back at the 2015 paper that introduced an attention-like mechanism to NMT.]]></summary></entry><entry><title type="html">Knowledge Distillation - Review</title><link href="https://ht0324.github.io/blog/2025/distillation/" rel="alternate" type="text/html" title="Knowledge Distillation - Review" /><published>2025-04-22T14:00:00+00:00</published><updated>2025-04-22T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/distillation</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/distillation/"><![CDATA[<p>I’ve known about the concept of knowledge distillation for a while – the core idea is simple: soft labels (the full probability distribution from a model) contain richer information about class relationships than hard labels alone. I first encountered it in a lecture by Geoffrey Hinton (<a href="https://www.youtube.com/watch?v=rGgGOccMEiY">like this one discussing paths to intelligence</a>) and decided to read the original 2015 paper, “<a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a>,” co-authored with Oriol Vinyals and Jeff Dean. It’s short, but with clear insight.</p>

<h3 id="the-insect-analogy-training-vs-deployment">The Insect Analogy: Training vs. Deployment</h3>

<p>What struck me immediately was the opening analogy:</p>

<blockquote>
  <p>“Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction.”</p>
</blockquote>

<p>I haven’t seen many ML papers start with a biological analogy like this. I hadn’t thought about insect life stages this way before. The larva is about consumption and growth, slow-moving, maybe not complex, but efficient at extracting resources (like a large training model absorbing information from data). The adult form is optimized for different tasks, lightweight, fast, mobile, focused on specific functions like reproduction (like an efficient deployment model needing low latency and computational cost).</p>

<p>The analogy fits perfectly with the challenge in machine learning:</p>
<ul>
  <li><strong>Training:</strong> We often use huge, “cumbersome” models (or ensembles) that take lots of computation and time but are great at extracting every bit of signal from large datasets.</li>
  <li><strong>Deployment:</strong> We need models that are fast, efficient, and have low latency for real-world use.</li>
</ul>

<p>Distillation, then, is like the <strong>metamorphosis</strong>: transforming the knowledge captured by the cumbersome larva/training model into the efficient adult/deployment model.</p>

<h3 id="knowledge-beyond-weights">Knowledge Beyond Weights</h3>

<p>The paper points out a potential “conceptual block”:</p>

<blockquote>
  <p>“…we tend to identify the knowledge in a trained model with the learned parameter values.”</p>
</blockquote>

<p>This makes it hard to think about transferring knowledge without just copying weights. Prior work like Rich Caruana’s model compression focused on matching the outputs <em>before</em> the final softmax (the logits). Hinton et al.’s approach refines this by using the <em>probabilities</em> from the softmax, arguing that this captures the learned distribution more meaningfully.</p>

<h3 id="the-value-of-wrong-answers">The Value of “Wrong” Answers</h3>

<p>A key insight is how the large, cumbersome model generalizes. It’s not just about getting the right answer.</p>

<blockquote>
  <p>“…a side-effect of the learning is that the trained model assigns probabilities to all of the incorrect answers… The relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize.”</p>
</blockquote>

<p>The example they give is clear: an image of a BMW might have a tiny probability of being mistaken for a garbage truck, but that probability, however small, is likely higher than it being mistaken for a carrot. This network of similarities and differences between classes is knowledge learned by the teacher model. Hard labels (just “BMW”) throw this information away. Soft labels (the full probability distribution) preserve it.</p>

<p>This aligns with the objective: we don’t just want models to perform well on training data, we want them to <em>generalize</em> well to new data. Soft targets directly transfer the <em>generalization behavior</em> of the teacher model to the student.</p>

<h3 id="the-mechanism-temperature-scaling">The Mechanism: Temperature Scaling</h3>

<p>So how do we use these soft labels? If the teacher model is very confident (assigns probability ~1.0 to the correct class), the probabilities for incorrect classes are tiny. Even if their <em>ratios</em> contain information, they have almost no impact on the cross-entropy loss during student training.</p>

<p>The solution is to “raise the temperature” <code class="language-plaintext highlighter-rouge">T</code> of the softmax function:</p>

<p><code class="language-plaintext highlighter-rouge">q_i = exp(z_i / T) / Σ_j exp(z_j / T)</code></p>

<p>where <code class="language-plaintext highlighter-rouge">z_i</code> are the logits. Normally <code class="language-plaintext highlighter-rouge">T=1</code>. Using a higher <code class="language-plaintext highlighter-rouge">T &gt; 1</code> “softens” the probability distribution, increasing the probabilities of incorrect classes and allowing them to contribute more to the loss function. The student model is trained to match this softened distribution, using the same high temperature <code class="language-plaintext highlighter-rouge">T</code>. (After training, the student uses <code class="language-plaintext highlighter-rouge">T=1</code> for inference).</p>

<p>This temperature scaling is the core mechanism. The paper notes that in the high-temperature limit, this method becomes equivalent to matching the logits (Caruana’s approach), but at intermediate temperatures, it focuses more on matching the more probable incorrect classes, potentially ignoring noise from very negative logits.</p>

<h3 id="training-the-student">Training the Student</h3>

<p>The best results often come from combining two objectives:</p>
<ol>
  <li>Matching the soft targets from the teacher (using cross-entropy with high temperature <code class="language-plaintext highlighter-rouge">T</code>).</li>
  <li>Matching the true hard labels (using cross-entropy with <code class="language-plaintext highlighter-rouge">T=1</code>).</li>
</ol>

<p>They found a weighted average works well, often with a lower weight on the hard target loss. As they say: “Typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful.”</p>

<h3 id="proof-of-generalization-the-mnist-experiment">Proof of Generalization: The MNIST Experiment</h3>

<p>A clear experiment highlights the value of this approach. They trained a student model on MNIST, but <em>omitted all examples of the digit ‘3’</em> from the transfer set. From the student’s perspective, ‘3’ was a “mythical digit” it had never directly seen.</p>

<p>Despite this, the distilled model performed well on classifying ‘3’s at test time (with a bias adjustment). It had learned about ‘3’ indirectly, through the soft targets for other digits, for example, by learning which ‘8’s looked a bit like a ‘3’ according to the teacher model. This is evidence that soft targets transfer generalization capabilities, not just labels.</p>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>This paper is a classic example of clear insight. The core claim is simple:</p>

<blockquote>
  <p>“…a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.”</p>
</blockquote>

<p>Knowledge distillation provides a practical way to harness this information, bridging the gap between powerful-but-cumbersome training models and efficient deployment models. While short, the paper’s impact is significant, reflected in its citations. It’s a testament to clear thinking and finding simple solutions to important problems.</p>]]></content><author><name></name></author><category term="Paper" /><category term="AI" /><summary type="html"><![CDATA[Reviewing the elegant 2015 paper by Hinton, Vinyals, and Dean on knowledge distillation.]]></summary></entry><entry><title type="html">Neural Probabilistic Language Model - Review</title><link href="https://ht0324.github.io/blog/2025/neural-language/" rel="alternate" type="text/html" title="Neural Probabilistic Language Model - Review" /><published>2025-04-20T15:00:00+00:00</published><updated>2025-04-20T15:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/neural-language</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/neural-language/"><![CDATA[<p>I recently dove into Yoshua Bengio et al.’s 2003 paper, “<a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>”. Reading such an old paper, foundational work from over two decades ago, is fascinating. What struck me most wasn’t just the specific model (which is simple by today’s standards), but the clarity with which Bengio laid out the core problems and principles of language modeling, principles that are still relevant. I got a respect for his vision; it feels like this paper set the trajectory for much of what followed.</p>

<h3 id="the-problem-the-curse-of-dimensionality">The Problem: The Curse of Dimensionality</h3>

<p>Bengio starts by framing the fundamental challenge: the <strong>curse of dimensionality</strong>. As he puts it,</p>

<blockquote>
  <p>“…a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.”</p>
</blockquote>

<p>This is because the number of possible sentences is essentially infinite, like <a href="https://medium.com/@FdForThought/a-short-story-in-hell-24b02ff4d812">the Library of Babel</a>. Any specific sentence has almost zero probability of occurring randomly.</p>

<p>The “curse” goes deeper than just the sheer number of sequences. As the number of dimensions (e.g., the length of the sequence, or the number of features considered) increases:</p>

<ol>
  <li><strong>Space Expands Exponentially:</strong> The volume of the space grows very fast, making the available data extremely sparse.</li>
  <li><strong>Distance Intuition Breaks:</strong> In high dimensions, points tend to become equidistant from each other, and most of the volume is concentrated far from the center, near the “surface” of the high-dimensional space. Our low-dimensional intuitions about proximity and density fail.</li>
  <li><strong>Spurious Correlations:</strong> With so many dimensions, it becomes easy to find apparent patterns in data that are just noise.</li>
</ol>

<p>This is a core challenge for many real-world problems, especially with rich sensory data spanning many dimensions. How do you find the signal in such a vast, sparse space without getting lost?</p>

<h3 id="the-solution-fighting-fire-with-fire">The Solution: Fighting Fire with Fire</h3>

<p>Bengio and his colleagues proposed a way to fight this curse:</p>

<blockquote>
  <p>“…learning a distributed representation for words…”</p>
</blockquote>

<p>Essentially, they proposed learning dense, low-dimensional feature vectors (embeddings) for each word in the vocabulary. This is like fighting fire with fire: while the <em>vocabulary</em> space is huge and discrete, the learned <em>feature</em> space is much smaller (e.g., 30-100 dimensions in their experiments vs. 17k+ words) but continuous. Because it’s a dense, continuous space, even a relatively low-dimensional one has a large capacity to represent complex relationships. They are mapping the discrete, high-dimensional vocabulary into a structured, continuous latent space.</p>

<h3 id="the-magic-how-generalization-happens">The Magic: How Generalization Happens</h3>

<p>So how does this help? The paper explains:</p>

<blockquote>
  <p>“Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.”</p>
</blockquote>

<p>This, for me, is the crux of it. The model learns which words play similar roles (semantically, syntactically) and places them close together in the embedding space. Because the probability function operates smoothly over this continuous space, seeing “The cat sat on the mat” helps the model assign a higher probability to the <em>unseen</em> sentence “A dog rested on the rug,” because the corresponding words have similar learned representations. It’s this mapping from discrete symbols to a meaningful continuous space that allows generalization beyond simply memorizing n-grams. This is fundamentally how current LLMs achieve their (still limited, but powerful) generalization capabilities.</p>

<h3 id="learning-end-to-end">Learning End-to-End</h3>

<p>A key part of their proposal was point 3:</p>

<blockquote>
  <p>“learn simultaneously the word feature vectors and the parameters of that probability function.”</p>
</blockquote>

<p>They recognized that the embeddings and the prediction mechanism need to learn <em>from each other</em>. You can’t just fix one and train the other; they have to be optimized together, end-to-end, for the embeddings to become useful for the prediction task and vice-versa.</p>

<h3 id="a-historical-aside-parallel-processing-with-cpus">A Historical Aside: Parallel Processing with CPUs</h3>

<p>What also caught my eye was the extensive discussion on parallelizing the training process. Remember, this was 2003 when widespread GPU computing for ML wasn’t a thing yet. They detail their efforts using <strong>parameter-parallel processing</strong> across multiple CPUs (up to 64 Athlon processors in their cluster!). They discuss asynchronous updates and communication overhead (MPI). It feels like they were laying the conceptual groundwork for the kind of massive parallelization (now mostly on GPUs/TPUs) that is essential for training today’s large models.</p>

<h3 id="lasting-impact">Lasting Impact</h3>

<p>While the specific MLP architecture used in the paper is rudimentary now, the core ideas, tackling the curse of dimensionality with learned distributed representations, enabling generalization through semantic similarity in embedding space, and the need for end-to-end training, remain central to modern NLP and deep learning. Reading this paper felt like a clear early articulation that illuminated the path forward for the field. It helped define the paradigm we’re still working within.</p>]]></content><author><name></name></author><category term="Paper" /><category term="AI" /><summary type="html"><![CDATA[Thoughts on the foundational 2003 paper]]></summary></entry><entry><title type="html">Revisiting the 2014 Sequence-to-Sequence Paper</title><link href="https://ht0324.github.io/blog/2025/lstm/" rel="alternate" type="text/html" title="Revisiting the 2014 Sequence-to-Sequence Paper" /><published>2025-04-20T11:00:00+00:00</published><updated>2025-04-20T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/lstm</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/lstm/"><![CDATA[<p>I recently went back to read the 2014 paper “<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>” by Sutskever, Vinyals, and Le. Since it’s such a seminal paper, practically ancient by today’s ML standards, I thought it would be interesting to look back. While the method itself (LSTM encoder-decoder) is relatively simple compared to modern architectures, focusing on their thinking process back when deep learning was still in its infancy was insightful. I found some things they mentioned almost casually that felt non-trivial to me now.</p>

<p>Here are some of my main takeaways:</p>

<h3 id="refreshing-views-on-dnn-power">Refreshing Views on DNN Power</h3>

<p>The paper starts by framing Deep Neural Networks (DNNs) in ways I hadn’t explicitly considered before. They state:</p>

<blockquote>
  <p>“DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps.”</p>
</blockquote>

<p>This sentence, while true and maybe obvious in retrospect, struck me. Of course, we know matrix multiplications are parallelizable and run well on GPUs, but thinking about it from the perspective of <em>individual neurons</em> performing computations in parallel felt like a useful angle on <em>why</em> NNs are suited for this hardware.</p>

<p>They also highlight:</p>

<blockquote>
  <p>“…their ability to sort N N-bit numbers using only 2 hidden layers of quadratic size…”</p>
</blockquote>

<p>Again, a specific example of computational power packed into a relatively simple network that I hadn’t really internalized.</p>

<h3 id="the-core-problem-and-the-lstm-solution">The Core Problem and the LSTM Solution</h3>

<p>The authors clearly state the limitation they were tackling:</p>

<blockquote>
  <p>“Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.”</p>
</blockquote>

<p>This was a major hurdle for many important tasks like machine translation or question answering, where sequence lengths vary. This is where the Long Short-Term Memory (LSTM) network comes in.</p>

<p>For me, the biggest contribution of this paper is that they took the LSTM and <em>successfully trained an encoder-decoder architecture at scale</em> on a difficult task (English-to-French translation). They showed that LSTMs weren’t just a theoretical curiosity; they were <em>practical</em> for real-world, large-scale NLP problems. This was the first paper that demonstrated LSTMs could <em>work</em> in this way, paving the path for much future research.</p>

<h3 id="the-input-reversal-trick">The Input Reversal Trick</h3>

<p>One specific technical detail that stood out was their trick of reversing the input sentence:</p>

<blockquote>
  <p>“…reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.”</p>
</blockquote>

<p>My first reaction was: Okay, reversing the input brings the <em>first</em> source word closer to the <em>first</em> target word, which makes sense for translation since the beginning often sets the context. But what about the <em>last</em> words of the source sentence? Don’t they get pushed really far away from the end of the target sentence?</p>

<p>That’s a valid point, but I think it highlights a trade-off. Getting the beginning of the translation right is often very important; it lays the groundwork. By reversing the input, they made it easier for the optimization process (SGD) to “establish communication” between the early parts of the source and target sequences. The performance gains they reported (perplexity dropping from 5.8 to 4.7, BLEU jumping from 25.9 to 30.6) suggest this was an effective trade-off, making the model learn better, even if it seems counter-intuitive for the tail end of the sequences.</p>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>Reading this paper was a great reminder of how far the field has come, but also of the clear thinking that set the foundations. I admire Ilya Sutskever’s way of thinking; when I listen to him on podcasts, he often speaks with clarity. Looking at this early work reinforces that impression. Maybe I should make a point of reading through more of his papers.</p>]]></content><author><name></name></author><category term="Paper" /><category term="AI" /><summary type="html"><![CDATA[Personal takeaways from the seminal 2014 paper]]></summary></entry><entry><title type="html">AI as Personal Guardians</title><link href="https://ht0324.github.io/blog/2025/guardian/" rel="alternate" type="text/html" title="AI as Personal Guardians" /><published>2025-04-19T11:00:00+00:00</published><updated>2025-04-19T11:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/guardian</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/guardian/"><![CDATA[<p>I’ve been thinking a lot about how LLMs might reshape society, and a thought clicked: AI could become <strong>personal guardians</strong> for each of us.</p>

<p>The background is this: As I’ve <a href="https://ht0324.github.io/blog/2025/sys1/">discussed before</a>, context is important for LLMs. Even now, the insights they provide in split-second decisions can be helpful. They aren’t perfect, but the intelligence they offer brings value. The main thing limiting their ability to help us more consistently is <em>access</em>. If we don’t actively query the model with the right context, it can’t respond to our specific, nuanced needs. Our lives are complex, and the same query can mean different things depending on the web of our individual circumstances.</p>

<p>So, the context an LLM needs to be truly helpful is immense and deeply personal.</p>

<h3 id="beyond-universal-assistants">Beyond Universal Assistants</h3>

<p>Now, the idea of a universal AI assistant isn’t new – think <em>Her</em> or Jarvis. We all nod along, assuming something like that is coming. But I don’t think most people grasp the gravity of this, the potential impact if put into the palm of our hands. It will touch on the nature of our experience.</p>

<p>What I envision is this: Imagine wearing a small device, maybe a pendant, that continuously and passively records context from your daily life – conversations you have, things you hear, places you go, maybe even subtle reactions. Right now, most of this rich contextual data is <a href="https://ht0324.github.io/blog/2025/ephemeral/">ephemeral</a>, lost the moment it happens because we don’t record our everyday lives.</p>

<p>If this data were captured objectively, it could provide the grounding LLMs need to become more helpful.</p>

<h3 id="confronting-our-subjectivity">Confronting Our Subjectivity</h3>

<p>Here’s why I think people underestimate the impact: we don’t fully appreciate how subjective, limited, fragile, and unreliable our own perception and memory are. Psychological literature makes it clear: human memory isn’t a perfect recording device. We reshape memories, constructing narratives to make sense of the world. Our accounts of the same event differ from person to person, filtered through our limited viewpoints and emotional states. We aren’t purely rational decision-makers.</p>

<p>An AI, fed with continuous, objective context, could hold up a mirror to this subjectivity. It could help us see patterns and realities that our own minds obscure.</p>

<h3 id="the-guardians-role">The Guardian’s Role</h3>

<p>Imagine the possibilities:</p>

<ol>
  <li><strong>Objective Recall &amp; Comparison:</strong> The AI could provide an objective summary of your day, week, month, or even year. It could compare your activities, moods, or interactions over time in ways impossible for our biased human memory. “How does my interaction pattern today compare to last month?” is a question we can barely guess at; the AI could answer with data.</li>
  <li><strong>Personalized Planning:</strong> Based on this deep, objective understanding of your past actions, goals, and context, it could suggest optimal plans for tomorrow, complete with relevant reminders grounded in <em>your</em> actual history.</li>
  <li><strong>Social Shield:</strong> For interactions, it could offer insights or warnings. Imagine someone easily manipulated, such as an elderly person. This AI could recognize patterns of deception or fraud that the person might miss, acting as a protective layer by providing information they didn’t previously have.</li>
</ol>

<p>Thinking about this “social shield” aspect, particularly its potential to guardrail individuals away from harmful decisions, is when the core concept clicked for me. Imagine the AI noticing subtle health patterns and suggesting a check-up, or recognizing manipulative language in a conversation. Preventing bad outcomes by providing timely information, nipping problems in the bud, could be one of the most impactful aspects of this technology. That realization solidified the idea of <strong>“A personal guardian for everyone.”</strong></p>

<p>With such guardians, society could evolve. Individuals might become better decision-makers overall. Imagine consulting your guardian in-depth before making major life choices: which university course to take, which habit you didn’t notice is detrimental to your health, which job offer is best aligned with your long-term patterns and goals.</p>

<h3 id="a-guardian-in-the-cloud">A Guardian in the Cloud</h3>

<p>This isn’t about the AI becoming a godlike entity dictating our lives. It’s about having an immensely valuable and practical <em>tool</em> – an intelligent counterpart striving to help <em>us</em> make better decisions, understand ourselves more clearly, and navigate the world more effectively.</p>

<p>I believe this is possible with current technology, perhaps needing refinement and scale. When (not if) this kind of personalized, context-aware AI guardian becomes widespread, the impact on individual productivity, efficiency, and overall well-being could be large. Everyone could be better off <em>with</em> their guardian than without.</p>

<p>It leads directly to the future Yuval Noah Harari described, where algorithms might genuinely know you better than you know yourself in certain aspects. What a fascinating, and perhaps slightly unnerving, time to be alive.</p>]]></content><author><name></name></author><category term="Blog" /><category term="AI" /><summary type="html"><![CDATA[AI acts as a guardian for individuals]]></summary></entry><entry><title type="html">Agency Over Retrieval?</title><link href="https://ht0324.github.io/blog/2025/o3-o4mini2/" rel="alternate" type="text/html" title="Agency Over Retrieval?" /><published>2025-04-18T10:00:00+00:00</published><updated>2025-04-18T10:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/o3-o4mini2</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/o3-o4mini2/"><![CDATA[<p>I’ve been thinking more about OpenAI’s o3 and o4 mini models since my <a href="/blog/2025/o3-o4mini-product-path/">last post</a>, and using them led to some additional insights. Specifically, I looked into the <a href="https://x.com/elder_plinius/status/1912567149991776417">system prompt for o3/o4 mini</a> and noticed something interesting: it strongly encourages the model to surf the web whenever a query is even slightly vague or uncertain. Essentially, web search is almost the default behavior.</p>

<p>Initially, I was puzzled by this choice. Why prompt a <em>reasoning and agentic</em> model to search the web so readily? These aren’t primarily web search models like Perplexity AI. Why not rely more on their internal knowledge and reasoning first?</p>

<p>But as I thought deeper about it, everything kind of clicked.</p>

<h3 id="beyond-simple-hallucination-mitigation">Beyond Simple Hallucination Mitigation</h3>

<p>Of course, one part of the answer relates to model hallucination. Getting large language models to be consistently reliable and avoid making things up is still a hard problem. While improvements in data and algorithms have reduced blatant hallucination, ensuring reliability over 99% of the time requires grounding. Accessing relevant, up-to-date information is important for that.</p>

<p>However, I don’t think that’s the full story here. I think OpenAI is doing something more fundamental by leveraging the <em>agentic</em> capabilities of these models.</p>

<h3 id="letting-the-model-choose-its-context">Letting the Model Choose Its Context</h3>

<p>Think about traditional approaches like Retrieval-Augmented Generation (RAG). In those systems, a separate retrieval mechanism analyzes the user’s query, finds potentially relevant source documents, and then stuffs that information into the language model’s context window. The retrieval system decides what the main LLM sees.</p>

<p>What OpenAI seems to be doing with o3/o4 mini is different. They are offloading the task of finding relevant information <em>to the main model itself</em>. Instead of an external system <em>pushing</em> context, the agentic model is encouraged to <em>pull</em> the context it decides it needs by actively searching the web.</p>

<p>Let me break it down with an analogy. Imagine you need to solve a complex problem.</p>
<ul>
  <li><strong>Option A (RAG-like):</strong> Someone else (a separate system) looks at your problem, finds some books or articles they think are relevant, and hands them to you. You then try to solve the problem using only those materials.</li>
  <li><strong>Option B (o3/o4 mini-like):</strong> You look at the problem, and <em>you</em> decide to proactively search your bookshelf, scour the internet, gather information, and based on what you find, iteratively search for more information until <em>you</em> feel you have what you need.</li>
</ul>

<p>Option B gives you autonomy. You actively choose what information you consume. This feels like a more effective way to tackle complex or nuanced problems.</p>

<p>The key difference is agency. The RAG system (Option A) isn’t necessarily as smart or capable as the main LLM it’s feeding context to. Why let a potentially less sophisticated system pre-filter the information? Why not let the powerful base model decide what information is most relevant or needed for its own reasoning process?</p>

<p>This principle of giving the model agency to select its own relevant context seems to be more general than just text retrieval via web search. It applies across modalities. Look at OpenAI’s recent post on <a href="https://openai.com/index/thinking-with-images/">“Thinking with Images”</a>. They demonstrate how o3/o4 mini can use tools to manipulate images during their chain of thought. For instance, if text in an image is upside down or hard to read, the model can use tools to zoom in or rotate that specific part of the image to better understand it. If an image is complex, it can zoom into the most relevant section. This is effectively visual information retrieval, driven by the model’s ability to choose which visual information to focus on using tools, mirroring how it uses web search to retrieve textual information.</p>

<p>Traditional RAG can sometimes feel like it just dumps context verbatim into the window, regardless of whether the model deems it insightful or sufficient. Giving the model the agency to search, whether the web for text or pixels within an image, means it can dynamically gather precisely what it needs, when it needs it. It’s a recursive, almost meta-cognitive approach: the model decides how to inform itself.</p>

<h3 id="consolidation-and-the-bitter-lesson-again">Consolidation and the Bitter Lesson Again?</h3>

<p>This feels like another step in the consolidation trend we’ve seen in AI. Previously, NLP was fragmented into many specific tasks (sentiment analysis, named entity recognition, etc.). With the rise of powerful transformers, many of these specialized tasks converged into the capabilities of large, general models.</p>

<p>Now, agency might be enabling further consolidation. Tasks like information retrieval (textual or visual) and hallucination mitigation, previously handled by separate scaffolding or techniques like RAG, might increasingly become integrated into the model’s core agentic reasoning loop. As models become more general and capable agents, they can take on more of these sub-tasks themselves.</p>

<p>In a way, it feels like the <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">Bitter Lesson</a> playing out once more. Instead of relying heavily on human-designed scaffolding and rule-based systems (like fixed retrieval strategies), perhaps it’s more effective to give the scaled-up model the agency and tools (like web search or image manipulation) and let it learn the best methods for gathering and utilizing information to solve the task at hand. Don’t inhibit the model with rigid external structures; let its capabilities grow.</p>

<p>It’s a simple shift: prompting the model to search when unsure, or allowing it to manipulate input images, but the underlying principle of empowering the model’s own agency to manage its information needs feels like an important one.</p>]]></content><author><name></name></author><category term="Blog" /><category term="AI" /><summary type="html"><![CDATA[Thinking about why o3/o4 mini are prompted to search the web by default.]]></summary></entry><entry><title type="html">Thoughts on o3, o4 mini</title><link href="https://ht0324.github.io/blog/2025/o3-o4mini/" rel="alternate" type="text/html" title="Thoughts on o3, o4 mini" /><published>2025-04-17T17:00:00+00:00</published><updated>2025-04-17T17:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/o3-o4mini</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/o3-o4mini/"><![CDATA[<p>So, here are my recent thoughts on the release of OpenAI’s <a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3 and o4 mini</a>. In conclusion, it’s a bit of a mixed bag. It’s easy to get caught up in the hype, and there are notable things present. While generally, I think the release and the resulting models are strong, there are subtle nuances that need to be addressed.</p>

<p>For a quick recap for those who haven’t caught up: OpenAI released <strong>o3</strong> and <strong>o4 mini</strong>, new variants of their reasoning models specifically trained to use tools and be agentic. When you see the demos and people’s use cases, it really is fantastic. It has more of a “person feel.” I’ve used it myself, and compared to previous models that primarily did research by fetching and analyzing web info, <strong>o3</strong> and <strong>o4 mini</strong> feel much more agentic. Unlike previous models which ca use function calling but rather in a separate isolated manner, they seem to actively parse information, act based on it, and use various tools such as command‑line interface. In that sense, they are really capable models.</p>

<h3 id="context-and-comparison">Context and Comparison</h3>

<p>But for me, after the initial impression settled, it felt like OpenAI basically released their version of Anthropic’s <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which is good at agentic tasks. Because of its agentic capabilities, 3.7 Sonnet became a go‑to enterprise solution, especially for agent‑coding IDEs. While OpenAI’s new models arrived two or three months later and are arguably better, the fundamental paradigm hasn’t drastically changed.</p>

<p>Another thing I really noted was within the ChatGPT interface itself. When serving <strong>o3</strong> and <strong>o4 mini</strong>, OpenAI enabled function calling and tool use <em>by default</em>. This means the models can readily use capabilities like data analysis, the coding environment, web search, and others that OpenAI has integrated into ChatGPT. All this combined gives the models a vast array of tools they didn’t have access to so easily before, and it does have a combinatorial effect on model capabilities.</p>

<p>Previously, if I wanted to research a topic while studying, I’d usually paste my content into the chat and query the model. Now, since the model can search the web itself, and it feels less like simple RAG and more like it’s actually fitting the fetched info into its context, I don’t have to provide as much relevant context manually. It feels more reliable in that sense, reducing the need for me to spoon‑feed context. I think the capability of the chat interface itself has changed.</p>

<h3 id="openai-as-a-product-company">OpenAI as a Product Company</h3>

<p>This led me to another thought: the overall trajectory of OpenAI as a company.</p>

<p>When I saw how OpenAI neatly packaged their models and tools onto a platter and served it within the chat interface, it clicked for me: as Sam Altman had <a href="https://www.youtube.com/live/5MWT_doo68k?t=653">said</a>, OpenAI is now officially a product company, though in hindsight, given where their revenue comes from, they always were.</p>

<p><a href="https://medium.com/@furqankhaan/how-openai-and-anthropic-are-cashing-in-on-ai-a-look-at-their-revenue-models-d9d9ae79dd28">If you compare their revenues to Anthropic’s</a>, OpenAI is the market leader, partly due to the network effect of being the first mover. But most of OpenAI’s revenue comes from user subscriptions with a smaller fraction from API usage. Anthropic is the polar opposite, most revenue comes from API usage, though even their API revenue lags behind OpenAI’s API revenue. This suggests subscriptions are more popular than API access, at least for OpenAI.</p>

<p>Then it makes sense for OpenAI to prioritize products because intelligence is becoming cheap, almost too cheap to meter. Model API costs are racing to the bottom, leaving very few margins, especially with competitors like Google, Anthropic, xAI, and others. Subscription is a godsend for cheap cash.</p>

<p>So, what do I mean by OpenAI acting as a product company? As I mentioned, <strong>o3/o4 mini</strong> feel like a better version of <strong>Claude 3.7 Sonnet</strong>. There’s a qualitative jump, I’m not denying that, but there’s also an effective way OpenAI executed the <em>delivery</em>.</p>

<p>You see, when <strong>Claude 3.7 Sonnet</strong> launched, Anthropic just launched an enterprise solution, not a product. And make no mistake, <strong>Claude 3.7 Sonnet</strong> is a capable, agentic model, written more about <a href="/blog/2025/Claude-Code/">here</a>. They also released the Model Context Protocol <a href="https://www.anthropic.com/news/model-context-protocol">(MCP)</a>, which I’ve also <a href="/blog/2025/vibe-coding/">written about and presented on</a>. However, when it came to their consumer-facing product, Anthropic essentially just slapped this smart model into a basic chat interface <em>without</em> providing the tools necessary to showcase its agentic power. The end user interacting with Claude AI wouldn’t even realize how smart and agentic the underlying model is.</p>

<p>Looking back, especially after seeing OpenAI’s <strong>o3/o4 mini</strong> launch, I can see how Anthropic could have stolen OpenAI’s thunder. If they had built the necessary scaffolding and provided adequate tools for Claude to use directly within the chat interface, allowing it to surf the web and execute code agentically, the user experience would have been different. Claude <em>can</em> do these things via MCP, but the defalut interface doesn’t allow it. This forces users like me to manually scaffold MCP and handcraft custom environments just to tap into the model’s full potential, which is far from an ideal experience. With <strong>o3</strong>, OpenAI made it frictionless; it just works.</p>

<h3 id="mcp-and-financial-realities">MCP and Financial Realities</h3>

<p>In that sense, I was surprised when OpenAI <a href="https://x.com/sama/status/1904957253456941061">announced</a> they would also support MCP on their models. Initially, I was skeptical they’d adopt it as a standard. First, Anthropic developed it. Second, it seemed counter to OpenAI’s strategy. As you can see, they were prepping <strong>o3/o4 mini</strong> as a <em>product</em>. They serve it via API too, but that doesn’t feel like their main priority. Their strategy seems to be building their own scaffolding and tool integration into the model and selling it as a product. MCP, being an open standard, directly counteracts that by leveraging the open‑source community.</p>

<p>However, I think this kind of standardization is inevitable, so OpenAI likely just followed suit. For MCP proliferation, the open‑weights/source community and Anthropic seem like the main benefactors, not OpenAI. But personally, I think this outcome—broader adoption of open standards—is desirable, even if it wasn’t OpenAI’s first preference.</p>

<p>I believe that to achieve financial independence, OpenAI will aggressively work towards building itself as a product company. I understand the criticisms about OpenAI deviating from its non‑profit roots. But as I’ve <a href="/blog/2025/OpenAI-for-profit/">covered before</a>, the reality seems simple: they need money. They need financial independence to do what they set out to do. Because of scaling laws and everything else, capital is necessary to scale up compute and continue research. I think they feel they have no choice but to pursue this path. I know Sam Altman can sound manipulative, but I think it might be true in a sense that they didn’t fully anticipate this financial necessity early on, and now they feel forced into this position.</p>

<h3 id="the-walled-garden-strategy">The Walled Garden Strategy</h3>

<p>Given this path towards being a product company, OpenAI’s recent moves start to make more sense. Take their <a href="https://x.com/OpenAI/status/1910378768172212636">enhanced memory feature</a>, for example. This is a play for user retention, a step towards building a walled garden. They want users integrated into <em>their</em> ecosystem.</p>

<p>Looking ahead, imagine if OpenAI develops a truly frontier, genius-level model. What if they <em>only</em> offer it through their ChatGPT interface, not the API? Since subscriptions are the real cash cow compared to the low-margin API race, this seems plausible, especially if AGI-level capabilities emerge. A <a href="https://darioamodei.com/machines-of-loving-grace">country of geniuses in a datacenter</a>, only accessible via chatgpt.com. They could make a lot of money this way.</p>

<p>Combine that potential model superiority with features like memory that build up personalized context over time, and the friction for users to switch to a competitor becomes immense. Your interaction history, your personalized AI – it all stays within OpenAI’s walls. Essentially, the memory capability becomes a strategic tool. It makes the platform stickier and much harder to leave. In a way, OpenAI is making our accumulated data a reason to stay, almost holding it hostage to deter us from jumping ship to competitors. Food for thought as these platforms evolve.</p>

<h3 id="conclusion">Conclusion</h3>

<p>So, in conclusion, yes, <strong>o3</strong> and <strong>o4 mini</strong> are qualitatively good models. But I think these kinds of performance improvements were somewhat expected given the trajectory of previous models and the industry. What I think is non‑trivial, and what not enough people seem to be paying attention to, is <em>how</em> OpenAI implemented this. The product integration, the default tool usage in the chat interface, and what this signals about their shift towards being a product‑first company: that’s the bigger story here.</p>

<p>Moreover, although OpenAI’s revenue mix already revealed its product focus, this release cements it. ChatGPT’s first‑mover advantage has generated a network effect: the more users it attracts, the more feedback and data it gathers, which funds better models, which in turn attract even more users. It’s the classic aggregator flywheel. Even though Google currently dazzles with pure‑model wins like Gemini 2.5 Pro, and Anthropic keeps pushing Claude 3.7 Sonnet, neither has managed to match the friction‑free, fully‑integrated product OpenAI now offers. Unless OpenAI makes a catastrophic misstep, I believe that flywheel will accelerate.</p>]]></content><author><name></name></author><category term="Blog" /><category term="AI" /><summary type="html"><![CDATA[Reflections on the new o3 and o4 mini models]]></summary></entry></feed>