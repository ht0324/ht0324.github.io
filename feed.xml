<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-07T14:10:13+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">VQVAE – Review</title><link href="https://ht0324.github.io/blog/2025/VQVAE/" rel="alternate" type="text/html" title="VQVAE – Review"/><published>2025-03-05T11:30:00+00:00</published><updated>2025-03-05T11:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/VQVAE</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/VQVAE/"><![CDATA[<p>In this post, I review the paper <a href="https://arxiv.org/abs/1711.00937">“Neural Discrete Representation Learning”</a>, commonly known as VQ-VAE. This paper introduces a generative model that combines vector quantization with Variational Autoencoders (VAEs).</p> <p>Its core idea is replacing the continuous latent space typically used in VAEs with discrete embeddings. This addresses the notorious issue of posterior collapse, ensuring the encoder meaningfully contributes to data reconstruction rather than relying solely on a powerful decoder.</p> <p>The discrete latent space makes this model particularly suited to domains naturally represented by discrete data, like speech, language, and structured visual information.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Discrete Latent Variables</strong><br/> Instead of using continuous distributions, VQ-VAE encodes data into discrete latent variables selected from a predefined embedding dictionary. This discretization helps prevent posterior collapse by forcing the encoder to produce meaningful, constrained representations.</p> <p><strong>Vector Quantization (VQ)</strong><br/> Vector Quantization involves mapping encoder outputs to the nearest embeddings in a learned dictionary. Although there’s no straightforward gradient through this discrete step, the authors use a simple “straight-through” estimator, effectively copying gradients from decoder inputs back to encoder outputs.</p> <p><strong>Commitment Loss</strong><br/> To ensure the encoder doesn’t produce arbitrary embeddings, VQ-VAE introduces a commitment loss. This regularization term encourages encoder outputs to remain close to their assigned embedding vectors, stabilizing training and improving the quality of learned representations.</p> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Posterior Collapse is a Bigger Deal Than I Thought</strong><br/> Initially, I underestimated posterior collapse, thinking that if the decoder reconstructed well, the encoder must be doing its job. But I learned that’s not necessarily true—if the decoder is too powerful, it can bypass the encoder entirely, undermining the entire concept of an autoencoder. VQ-VAE addresses this directly through discretization.</p> <p><strong>Constraining the Latent Space is Helpful</strong><br/> Surprisingly, imposing discrete constraints on latent representations helps the model learn better. Intuitively, I thought constraints might harm performance, but VQ-VAE demonstrates that limiting flexibility actually prevents the model from “getting lost,” ultimately improving representation quality.</p> <p><strong>The Gradient Copying Trick is Surprisingly Effective</strong><br/> VQ-VAE’s training includes copying decoder input gradients directly to encoder outputs—a method that feels very ad-hoc. Despite its simplicity and my initial skepticism, this approach works remarkably well, suggesting that straightforward solutions can sometimes outperform more sophisticated ones.</p> <p><strong>Tokenization of Latent Space Could Lead to New Applications</strong><br/> By tokenizing the latent space, VQ-VAE opens potential avenues for using transformer architectures on latent representations. Given that transformers excel with discrete token sequences, VQ-VAE’s discrete embeddings might unlock novel approaches for processing continuous data modalities as if they were language-like sequences.</p> <p><strong>Discrete Representations Naturally Align with Certain Data Types</strong><br/> I heard an hypothesis that VQ-VAE performs particularly well with inherently discrete data like language tokens or audio spectrograms. This makes intuitive sense, yet it remains an open question whether discrete latent spaces universally outperform continuous ones across various data domains.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>VQ-VAE introduces a straightforward yet effective method to discretize latent representations in autoencoders. By embracing discrete embeddings, it elegantly sidesteps posterior collapse and paves the way for new model architectures inspired by token-based learning.</p> <p>While discrete latent spaces offer promising advantages, further exploration is necessary to fully understand their limits and strengths across diverse applications.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[My thoughts on the VQ-VAE paper]]></summary></entry><entry><title type="html">Claude Code</title><link href="https://ht0324.github.io/blog/2025/Claude-Code/" rel="alternate" type="text/html" title="Claude Code"/><published>2025-03-04T10:59:59+00:00</published><updated>2025-03-04T10:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Claude-Code</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Claude-Code/"><![CDATA[<p>Today, I used <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a>, an experimental tool released by Anthropic a while ago. I really wanted to use it right at the start of the announcement, but I kept putting it off because I hadn’t found a use case for it yet. But today, I had to rapidly prototype an app, so I decided to give it a shot.</p> <hr/> <h3 id="claudes-agentic-capabilities">Claude’s Agentic Capabilities</h3> <p>Claude Code is basically <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a>, which has been updated by Anthropic to have more agentic capabilities. They’ve essentially wrapped it with tools that can run commands and execute terminal operations. Claude can see your repository, view your file directory, edit files, and execute terminal commands. So, yeah, it has much more agency.</p> <p>I needed to quickly prototype an app that hosts images, allows filtering by tags, and enables image similarity search. And lo and behold, after just 2-3 hours, I had a functioning prototype.</p> <p>I haven’t used <a href="https://www.cursor.com/">Cursor</a>, the VS Code variant that gained traction some time ago and was widely praised by people in the Bay Area. But maybe now I’m experiencing the epiphany that Cursor users had. The experience wasn’t just about typing in my query and having Claude do everything from start to finish - it felt more like pair programming.</p> <p>It gave me the sense that there was a person behind the terminal, troubleshooting issues and debugging with me. Of course, Claude wasn’t perfect, but the back-and-forth, iterative process was refreshing and much more engaging than just randomly encountering errors and sifting through Stack Overflow links to fix them.</p> <hr/> <h3 id="a-new-kind-of-collaboration">A New Kind of Collaboration(?)</h3> <p>It was a massive time-saver. I provided high-level directions, Claude executed them, and when problems arose, we solved them together. The biggest surprise was how collaborative the process felt - it was much easier and more efficient than doing everything on my own, or switching back and fourth using a chat interface.</p> <p>If I had done this single-handedly, it would have taken days. But for just $3.50 in API costs (which might seem expensive depending on your threshold), I built a prototype that I’m pretty confident in within 2–3 hours.</p> <p>Claude Code also helped me fix bugs on my self-hosted blog. I’m more of an AI guy, not a web guy, so managing my blog often involved trial and error - throwing things at the wall and seeing what stuck. I had to manually fix bugs and do all sorts of tweaks to get it to look the way I wanted.</p> <p>But with Claude Code, I could simply query my repository - it became conversational. That made it much easier to fix persistent issues in a short amount of time. All in all, it’s an incredibly useful tool.</p> <hr/> <h3 id="collaboration-as-a-mirage">Collaboration as a Mirage</h3> <p>But the reason why it felt like a collaborative process is that Claude still isn’t perfect in its agentic execution. Don’t get me wrong - Claude is very capable, but it still requires human intervention. For complex tasks, it doesn’t always get things right on the first try.</p> <p>If we ever reach a point where AI no longer needs intervention - where it can figure out even highly complex instructions entirely on its own - then it would no longer feel like collaboration; it would be full automation.</p> <p>That said, I structured my app development process in a very step-by-step manner, executing tasks sequentially. Maybe that’s why I didn’t run into any major setbacks. If I had given more abstract instructions, I might have been more surprised by what Claude could do. But the key point remains: it still feels like a collaboration because Claude makes mistakes, and I had to intervene to correct them.</p> <p>Looking at the trajectory of AI progress, though, I don’t think this collaborative phase will last long. Eventually, it won’t be a partnership - it will be a replacement.</p> <hr/> <h3 id="the-inevitable-shift">The Inevitable Shift</h3> <p>I’ve recently seen posts on <a href="https://x.com/David_Kasten/status/1893357776702976286">X</a> of anecdotes that from recruiters at frontier labs who now assume that junior staffs are AI replaceable. I was initially skeptical of this claim, but now I kinda believe it. Also, in Andrej Karpathy’s <a href="https://x.com/karpathy/status/1894099637218545984">post</a>, he argues that agency is more valuable than intelligence. This struck me as profound.</p> <p>In terms of intelligence, today’s top LLMs are already more knowledgeable than humans. But in terms of agency, humans still have the upper hand. However, after my interaction with Claude Code, I caught a glimpse of a future where machines will reach agency supremacy as well. When that happens, it won’t be collaboration - it will be a full <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">Humans Need Not Apply</a>(outdated, but still relevant).</p> <p>I encourage everyone reading this to try it for themselves. Hearing about it isn’t the same as experiencing it firsthand. Maybe I’m exaggerating, but I stand by my belief that this kind of automation is a matter of when, not if. And this is the worst this technology will ever be, it’s only going to get better from here.</p>]]></content><author><name></name></author><category term="Blog"/><category term="Thoughts"/><category term="AI"/><summary type="html"><![CDATA[My experience with Claude Code for rapid prototyping, debugging, and implications of AI agency]]></summary></entry><entry><title type="html">Link Archive - Feb 2025</title><link href="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/" rel="alternate" type="text/html" title="Link Archive - Feb 2025"/><published>2025-02-28T22:59:59+00:00</published><updated>2025-02-28T22:59:59+00:00</updated><id>https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Link-Archive-Feb-2025/"><![CDATA[<p>Another month, another batch of interesting links I’ve come across. Here’s what caught my attention in February 2025.</p> <hr/> <p><strong><a href="https://youtube.com/watch?v=_1f-o0nqpEI">DeepSeek, China, OpenAI, and AI Megaclusters - Lex Fridman Podcast #459</a></strong></p> <ul> <li>I first encountered Dylan Patel on the Dwarkesh podcast with Asianometry, so I was glad to see him on Lex Fridman’s. He specializes in analyzing AI hyperscalers, so his insights are incredibly in-depth. Basically, highly recommended if you’re interested in the infrastructure side of things.</li> </ul> <p><strong><a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model</a></strong></p> <ul> <li>This is published by a group of researchers at Google DeepMind, particularly Sholto Douglas. It’s a step-by-step guide to scaling up compute using infrastructure, specifically TPUs. I’ve been working my way through it, and it starts with the very basics but gets pretty complicated quickly. It’s refreshing to see closed AI labs offering these “breadcrumbs” to the open-source community, sharing their knowledge.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></strong></p> <ul> <li>Similar in nature to the DeepMind scaling book, but this one’s from the Hugging Face team. It’s also very detailed, with an interactive guide, and, I should say, quite a mouthful.</li> </ul> <p><strong><a href="https://srush.github.io/raspy/">Thinking like Transformer</a></strong></p> <ul> <li>The author created a Python library and a detailed blog post explaining how computation actually works within a transformer network. He also developed an abstract conceptual framework for it. It’s really complicated—I still don’t fully grasp it—but it’s nonetheless fascinating.</li> </ul> <p><strong><a href="https://www.youtube.com/watch?v=a42key59cZQ&amp;list=WL&amp;index=2">Gwern - Anonymous Writer Who Predicted AI Trajectory on $12K/Year Salary</a></strong></p> <ul> <li>This is from the Dwarkesh Podcast. He introduces a blogger named Gwern, and you really need to read his work to understand. He has an extensive blog called <a href="http://gwern.net/">gwern.net</a>. His posts are incredibly in-depth and well-researched, more like separate articles. The sheer depth is staggering, unlike anything I’ve encountered, even in reputable sources like the New York Times. I think this guy is incredibly capable. It also made me think about how the current internet is dominated by big tech. But in the pre-big tech era, I heard there were these niche, individual creators posting unique content. Gwern feels like a holdover from that era.</li> </ul> <p><strong><a href="https://gwern.net/subculture">The Melancholy of Subculture Society - Gwern</a></strong></p> <ul> <li>Gwern’s writing is extensive and very long, so it requires dedicated time. This is one piece I read, and it’s a musing on how the internet, as technology evolves, is kind of segregating society, and the state of things. I think it’s a well-thought-out piece.</li> </ul> <p><strong><a href="https://zhengdongwang.com/2024/12/29/2024-letter.html">2024 Letter - Zhengdong Wang</a></strong></p> <ul> <li>This blog post, written by a researcher at Google DeepMind, makes a bombshell statement. Regarding the current state of their large language model research and machine learning models, he boldly claims that the models will essentially achieve anything if the evaluations are clearly defined. So, if there are evals, <em>any</em> evals, the model will succeed. It’s an audacious statement, but the implications are mind-blowing. I should probably write a dedicated blog post about this.</li> </ul> <p><strong><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">Sesame Research - Crossing the Uncanny Valley of Voice</a></strong></p> <ul> <li>This startup called Sesame revealed a really interesting voice-to-voice language model. They have a demo, and it works quite well. The key difference between Sesame and ChatGPT’s Advanced Voice Mode is that the model produces nuanced tones, like “ums” and “ahs,” very naturally. It feels incredibly realistic. For the first 10 minutes, I was blown away. But as I probed the model further, it became clear it doesn’t possess the same level of intelligence as LLMs. The same goes for ChatGPT’s Advanced Voice Mode; OpenAI seems to have significantly limited the model to comply with their guidelines. In Sesame’s case, I suspect the limitations are purely due to model and computational constraints.</li> </ul> <hr/> <p><br/> Hope you enjoyed this month’s picks—March should bring even more to explore!</p>]]></content><author><name></name></author><category term="Link"/><category term="AI"/><summary type="html"><![CDATA[A collection of articles and videos I explored in February 2025]]></summary></entry><entry><title type="html">Generative Adversarial Nets – Review</title><link href="https://ht0324.github.io/blog/2025/GAN/" rel="alternate" type="text/html" title="Generative Adversarial Nets – Review"/><published>2025-02-27T18:00:00+00:00</published><updated>2025-02-27T18:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/GAN</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/GAN/"><![CDATA[<p>This time, I’m reviewing the classic paper <a href="https://arxiv.org/abs/1406.2661">“Generative Adversarial Nets”</a>, famously known as GAN. This 2014 paper by Ian Goodfellow and colleagues introduced a revolutionary method of training generative models using an adversarial setup between two neural networks—a generator and a discriminator.</p> <p>The central concept is simple yet powerful: a generator attempts to produce realistic samples to fool a discriminator, while the discriminator tries to distinguish real samples from generated ones. This competition pushes both models to improve continually, ultimately enabling the generator to produce incredibly realistic outputs without directly modeling complex probability distributions.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Generator and Discriminator</strong><br/> GAN consists of two neural networks trained simultaneously. The <strong>Generator (G)</strong> learns to produce data that looks like real samples from random noise, and the <strong>Discriminator (D)</strong> learns to classify whether a given sample is real or fake. This interplay drives continuous improvement.</p> <p><strong>Minimax Game and Value Function</strong><br/> GAN training is framed as a minimax game, where D tries to maximize its accuracy, while G tries to minimize it. Mathematically, this interaction is captured by a value function involving two competing optimization steps, one ascending (for D) and one descending (for G).</p> <p><strong>Noise as Input (Latent Representation)</strong><br/> The generator takes random noise as input (often standard Gaussian noise), which it transforms into realistic data. This noise acts as a latent representation, similar in purpose to latent spaces in other generative models like VAE or diffusion models.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>An Intuitive “Police vs. Counterfeiters” Paradigm</strong><br/> The analogy used by the authors—police versus counterfeiters—made the adversarial training idea very intuitive. The discriminator is essentially a “police officer,” trying to catch counterfeit samples, while the generator, acting as “counterfeiters,” constantly improves to evade detection. This simple analogy clarified the competitive dynamics at the heart of GANs.</p> <p><strong>Why the Order of Optimization Matters</strong><br/> One subtle but critical insight is the optimization order: in theory, the discriminator should reach optimality first before the generator updates. This sequential ordering makes sense because the discriminator provides the target (“answer sheet”) for the generator. Practically, training typically alternates between discriminator and generator updates, often simplifying to a 1:1 step ratio, despite theoretical ideals suggesting otherwise.</p> <p><strong>The “Saturation” Problem</strong><br/> Initially, the idea of the generator’s gradient saturating (becoming ineffective) was unclear. The authors point out that if the discriminator becomes too strong too early, the generator’s gradients become nearly zero because it consistently outputs samples easily identified as fake. Understanding this clarified the importance of balancing the discriminator and generator strengths.</p> <p><strong>Noise as a Form of Latent Space</strong><br/> Initially, calling the generator input “noise” felt unintuitive. Why noise? After considering diffusion models and VAE, I realized noise serves as a random seed or latent code that gets mapped to structured data. Essentially, it’s just a convenient way to introduce randomness into the otherwise deterministic neural network framework, enabling continuous generation and meaningful interpolation between points in this latent space.</p> <p><strong>Interpolation and Connection to VAEs</strong><br/> Interestingly, GANs achieve interpolation naturally despite the absence of a clearly defined encoder (unlike VAEs). VAEs explicitly model latent spaces to allow interpolation, but GANs achieve this indirectly. Because the generator learns from the discriminator’s feedback rather than directly fitting discrete data points, it inherently learns a continuous representation. This indirect approach was fascinating because it avoids explicitly modeling complicated distributions yet still produces meaningful interpolations.</p> <p><strong>Simplicity of Theoretical Results</strong><br/> The theoretical results, particularly the global optimality condition (pg = pdata), were straightforward yet elegant. By defining an optimal discriminator, the proof shows a simple Jensen-Shannon divergence between the true and generated distributions. Unlike VAE’s mathematically heavy derivations, GAN presents an intuitive yet robust theoretical grounding.</p> <p><strong>GANs vs. VAEs – Simpler but Powerful</strong><br/> Reflecting on VAE and GAN together, I realized GAN’s elegance lies in its simplicity. While VAE explicitly approximates intractable distributions with mathematical rigor, GAN sidesteps complexity through the clever adversarial setup. This simplicity initially may have seemed too naive to work, but history proved otherwise, showing that GANs indeed leveraged the powerful discriminative capabilities already established in neural networks.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>“Generative Adversarial Nets” introduced a simple yet groundbreaking framework that pits generators against discriminators to produce realistic data without explicitly modeling complex distributions. Its intuitive adversarial concept, practical simplicity, and powerful theoretical grounding explain why GANs rapidly became foundational in generative modeling.</p> <p>Exploring GAN alongside VAE deepened my understanding of how different approaches tackle generative tasks, highlighting GAN’s unique combination of simplicity, intuition, and effectiveness.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the original GAN paper, understanding the intuitive adversarial game between generators and discriminators, and connections to other generative models]]></summary></entry><entry><title type="html">Auto-Encoding Variational Bayes – Review</title><link href="https://ht0324.github.io/blog/2025/VAE/" rel="alternate" type="text/html" title="Auto-Encoding Variational Bayes – Review"/><published>2025-02-24T14:00:00+00:00</published><updated>2025-02-24T14:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/VAE</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/VAE/"><![CDATA[<p>In this post, I’m reviewing the paper <a href="https://arxiv.org/abs/1312.6114">“Auto-Encoding Variational Bayes”</a>, better known as the Variational Autoencoder (VAE) paper. Published by Kingma and Welling, this paper changed how generative models handle latent variables by introducing a practical and efficient way to perform approximate Bayesian inference.</p> <p>The central challenge addressed here is efficiently training models that involve continuous latent variables with intractable posterior distributions. By cleverly combining stochastic gradient methods with a reparameterization trick, VAE enables effective learning in models previously considered too complex or computationally infeasible.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Variational Autoencoder (VAE)</strong><br/> VAE is a generative model built around an encoder-decoder structure, with the crucial twist of modeling latent variables probabilistically rather than deterministically. It introduces an approximate posterior distribution to represent latent variables, enabling the model to capture uncertainty explicitly.</p> <p><strong>Evidence Lower Bound (ELBO)</strong><br/> ELBO, or Evidence Lower Bound, is central to VAE training. It balances reconstruction accuracy (how well the model reconstructs input data) against the complexity of the latent representation, quantified through KL divergence. Maximizing ELBO effectively encourages the latent distribution to align closely with a chosen prior, typically a standard Gaussian.</p> <p><strong>Reparameterization Trick</strong><br/> The paper introduces a “reparameterization trick,” which transforms a random sampling step into a deterministic operation combined with stochastic noise. This allows gradients to flow through sampling operations, making end-to-end optimization via stochastic gradient descent possible.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Essence of ELBO (Evidence Lower Bound)</strong><br/> Initially, ELBO felt abstract, and the paper didn’t clearly illustrate how it emerged mathematically. After deeper reflection (and some external clarifications), I realized ELBO comes directly from Bayesian inference principles, where we replace an intractable posterior with a simpler approximate distribution. ELBO effectively quantifies how well this approximation matches the true posterior, balancing model accuracy and complexity.</p> <p><strong>KL Divergence and Its Role as a Regularizer</strong><br/> One core insight I gained is the intuitive role of KL divergence. KL divergence measures how much the approximate posterior deviates from the chosen prior distribution. Minimizing KL divergence ensures that the model doesn’t rely excessively on overly complex or arbitrary latent representations, essentially acting as a regularizer that simplifies latent spaces.</p> <p><strong>Why the Reparameterization Trick Matters</strong><br/> The reparameterization trick initially seemed trivial but turned out to be crucial. Without this trick, sampling latent variables would break differentiability, making gradient-based optimization impossible. Reparameterization elegantly solves this by separating randomness (sampling) from deterministic parameters, enabling efficient end-to-end training through backpropagation.</p> <p><strong>Intuition Behind Probabilistic Latent Spaces</strong><br/> Traditional autoencoders map data deterministically into latent spaces, limiting their generative capabilities. By introducing probability distributions in the latent space, VAE smoothly maps continuous regions, allowing for meaningful interpolation and generation. This continuous latent representation enables more natural data generation compared to deterministic counterparts.</p> <p><strong>Complexity in Mathematical Derivations</strong><br/> The mathematical rigor of VAE made me appreciate the complexity of early deep-learning research. The derivations involved Bayesian inference, integral calculus, and careful manipulations to yield neat equations like ELBO. It became clear that foundational AI research required substantial mathematical fluency—something less critical today, perhaps, but undeniably essential back then.</p> <p><strong>Relation to EM Algorithm and Bayesian Methods</strong><br/> The paper also highlighted connections to the Expectation-Maximization (EM) algorithm and Bayesian methods. VAE generalizes and scales ideas traditionally handled by EM, using neural networks and stochastic optimization instead of traditional iterative approaches. Understanding this relation gave me a deeper context of where VAE fits into the broader landscape of machine learning methods.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>“Auto-Encoding Variational Bayes” introduces the Variational Autoencoder—a pivotal approach blending Bayesian inference with neural networks. Through ELBO maximization, KL divergence regularization, and the innovative reparameterization trick, VAEs enable efficient learning in previously intractable scenarios.</p> <p>Despite the mathematical complexity, the intuition is clear: by making latent representations probabilistic, VAE unlocks powerful generative capabilities and remains a foundational concept in modern generative modeling.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the foundational VAE paper, diving into variational inference, reparameterization, and why ELBO matters]]></summary></entry><entry><title type="html">NeRF – Representing Scenes as Neural Radiance Fields - Review</title><link href="https://ht0324.github.io/blog/2025/NERF/" rel="alternate" type="text/html" title="NeRF – Representing Scenes as Neural Radiance Fields - Review"/><published>2025-02-20T20:30:00+00:00</published><updated>2025-02-20T20:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/NERF</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/NERF/"><![CDATA[<p>In this post I’ll talk about the paper <a href="https://arxiv.org/abs/2003.08934">“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis”</a>. This paper introduces NeRF, a method to represent complex scenes as continuous neural fields, enabling high-quality view synthesis from sparse images.</p> <p>NeRF uses a simple yet effective architecture: a fully-connected neural network (MLP) that takes continuous 5D inputs (3D coordinates plus viewing angles) and outputs both color and volume density. This setup allows synthesizing novel views simply by querying the network and performing differentiable volume rendering along camera rays.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Neural Radiance Field (NeRF)</strong><br/> A NeRF is essentially a neural network that maps a 5D coordinate (3D spatial location <code class="language-plaintext highlighter-rouge">(x, y, z)</code> plus viewing direction <code class="language-plaintext highlighter-rouge">(θ, φ)</code>) to two outputs: volume density (opacity) and RGB color. The beauty lies in representing an entire scene within a compact neural network rather than explicitly storing a dense voxel grid or using complex 3D models.</p> <p><strong>Volume Rendering</strong><br/> To synthesize novel views, NeRF employs classical volume rendering techniques. Camera rays traverse the scene, accumulate opacity and color values from sampled points, and project these values into an image. Crucially, this process is differentiable, allowing the network to learn directly from input images without any explicit 3D geometry supervision.</p> <p><strong>Positional Encoding</strong><br/> Interestingly, directly feeding spatial coordinates into a neural network doesn’t work well for capturing fine details. The authors cleverly use positional encoding, mapping coordinates into higher-dimensional spaces using sinusoidal functions, helping the network learn high-frequency variations in geometry and appearance.</p> <p><strong>Hierarchical Sampling (Coarse &amp; Fine Networks)</strong><br/> To make the rendering efficient, NeRF uses a hierarchical sampling strategy. Initially, it samples points coarsely to estimate areas of importance, then densely samples those areas with a second “fine” network. This two-stage approach ensures computational efficiency and improved quality simultaneously.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Surprising Power of Simple MLPs</strong><br/> Initially, I assumed NeRF would require complex architectures. Surprisingly, a plain fully-connected network (MLP) was sufficient. This challenged my assumption that complex scenes need complicated models; NeRF elegantly achieves complexity through smart input encoding and sampling strategies rather than architectural complexity.</p> <p><strong>Positional Encoding Makes a Huge Difference</strong><br/> At first glance, positional encoding seemed like just a minor tweak. But it’s crucial. Without positional encoding, the model struggles to capture high-frequency details like textures and sharp edges. This was counterintuitive, as I originally thought that neural networks naturally handle continuous inputs well. Positional encoding acts like giving the network a “cheat sheet” for frequencies it should pay attention to.</p> <p><strong>Hierarchical Sampling – Efficiency through Bias</strong><br/> The hierarchical sampling was something I wouldn’t have thought of myself. Instead of uniformly sampling every point, NeRF first broadly samples (“coarse”) to identify important regions. Then, based on these initial guesses, it strategically places more samples in regions with higher density (or significance). It’s a smart trick: effectively biasing sampling toward regions that matter, vastly improving computational efficiency.</p> <p><strong>Overfitting as a Feature, Not a Bug</strong><br/> One of the most interesting realizations was that NeRF intentionally overfits to a specific scene. Unlike traditional deep learning models aiming for generalization, NeRF constructs a specialized network for each scene, optimizing solely for accuracy within that context. It felt unconventional but makes sense because NeRF’s goal isn’t generalization—it’s to achieve photorealistic rendering for a given scene.</p> <p><strong>Why This Matters for Interpolation and Rendering Quality</strong><br/> NeRF’s method naturally leads to excellent interpolation between views, providing realistic novel perspectives even from limited viewpoints. Because it learns a continuous representation instead of discrete samples, it effortlessly generates smooth transitions between views. It’s elegant, simple, yet incredibly powerful.</p> <p><strong>Comparisons and Connections with VAEs and GANs</strong><br/> Reflecting on my earlier reviews (VAE and GAN), I realized NeRF shares a fundamental idea: continuous latent representations. While VAEs explicitly enforce structured latent distributions and GANs rely on adversarial learning, NeRF takes a more straightforward approach by embedding spatial coordinates directly into neural networks. However, it still captures continuous structures that allow interpolation, something VAE explicitly constructs and GAN achieves implicitly.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>NeRF brilliantly demonstrates how neural networks can represent entire scenes using only a few images and positional encodings. By intentionally overfitting and employing hierarchical sampling, it achieves stunning photorealism without explicit geometric models.</p> <p>It’s intriguing how simple design choices can dramatically shift the approach—and effectiveness—of neural models in tasks like 3D view synthesis. This simplicity and elegance are precisely why NeRF has quickly become foundational in neural rendering research.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Diving into NeRF — a simple yet clever idea of neural networks representing continuous 3D scenes]]></summary></entry><entry><title type="html">Direct Preference Optimization (DPO) - Review</title><link href="https://ht0324.github.io/blog/2025/DPO/" rel="alternate" type="text/html" title="Direct Preference Optimization (DPO) - Review"/><published>2025-02-17T22:00:00+00:00</published><updated>2025-02-17T22:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/DPO</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/DPO/"><![CDATA[<p>In this post, I’ll share my thoughts on the paper <a href="https://arxiv.org/abs/2305.18290"><em>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em></a>, usually called DPO. This paper is interesting because it approaches the widely used Reinforcement Learning with Human Feedback (RLHF) paradigm in a fundamentally simpler way.</p> <p>The paper starts with the motivation behind alignment: aligning language models (LMs) with human preferences. Existing methods like PPO-based RLHF are effective but notoriously complex and unstable. DPO aims to simplify this by cleverly rethinking how reward modeling and preference alignment can be done.</p> <p>Let’s dive in.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Direct Preference Optimization (DPO)</strong><br/> DPO removes the explicit reward modeling step that’s common in RLHF. Instead of first training a separate reward model and then using RL to maximize that reward, DPO cleverly formulates the reward implicitly within the language model itself. This allows solving the alignment problem using a simple supervised classification objective.</p> <p><strong>Bradley-Terry Model</strong><br/> DPO builds upon the Bradley-Terry model, a statistical approach for estimating the probability that one option is preferred over another. While it initially seemed random that they picked such an old statistical model, it’s been used previously by OpenAI and DeepMind in RL settings. The Bradley-Terry formula gives an intuitive probability distribution over preferences, making it suitable for learning from human feedback.</p> <p><strong>Implicit Reward Representation</strong><br/> The key insight in DPO is that the “reward” for a given output can be represented implicitly as the log ratio of the probabilities from the learned LM and a reference LM (usually the SFT or supervised fine-tuned model). Instead of explicitly modeling rewards, DPO directly optimizes this ratio to reflect human preferences.</p> <p><strong>Simplified Objective (No RL Required)</strong><br/> By using the implicit reward representation, DPO turns preference alignment into a supervised learning problem. The objective is simply to increase the probability of preferred outputs and decrease the probability of non-preferred ones, scaled by a weighting term reflecting confidence in the model’s preference ranking.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Removing the Explicit Reward Model is Clever</strong><br/> Initially, I thought the explicit reward model was essential to RLHF. It seemed natural: first learn rewards from humans, then optimize those rewards. DPO surprised me by showing you can skip that step completely. Instead of explicitly modeling human preferences, DPO encodes them directly into the language model’s probabilities. It’s neat, elegant, and way simpler.</p> <p><strong>The Bradley-Terry Model as the Theoretical Foundation</strong><br/> At first glance, the Bradley-Terry model felt arbitrary to me—it’s a decades-old statistical model, after all. But its use in RLHF contexts actually dates back to earlier work by OpenAI and DeepMind. Bradley-Terry is intuitive because it translates pairwise human preference data directly into probabilities. DPO smartly leverages this model to avoid more complicated RL setups.</p> <p><strong>DPO’s Loss Function Explained</strong><br/> The loss function in DPO initially confused me because of its signs and terms. However, the intuition eventually clicked: it increases the likelihood of preferred outputs and decreases the likelihood of dispreferred ones. Crucially, the loss is weighted by how “wrong” the model is about these preferences. If the model is confident but incorrect, it makes larger corrections. This weighting makes the optimization stable and effective.</p> <p><strong>Why Stability is Important (and How DPO Ensures it)</strong><br/> Standard RLHF methods like PPO often become unstable because they rely on explicit reward models and require careful tuning. DPO bypasses this by using a fixed reference model (usually the SFT model). By avoiding explicit reward estimation and complicated online updates, DPO remains stable, even as the model improves.</p> <p><strong>Surprising Performance Without Complex Tuning</strong><br/> The experimental results clearly showed that DPO achieves comparable or better performance compared to traditional RLHF methods. What was most surprising to me was that DPO achieves high alignment quality without requiring extensive hyperparameter tuning or complex reward sampling procedures. It simplifies the process dramatically.</p> <p><strong>Philosophical Shift: Overfitting vs. Generalization</strong><br/> An interesting aspect of DPO (and similar recent methods like ORPO) is that it operates entirely offline. Unlike traditional RL methods, which involve interactive environments and trajectories, DPO treats preference alignment purely as supervised learning. It’s a philosophical shift from generalization towards targeted optimization or even intentional overfitting to human preferences. It made me rethink what RLHF truly means.</p> <p><strong>KL Divergence and Model Drift</strong><br/> I found the authors’ point about KL divergence important but not fully explained in the paper. DPO achieves high alignment quality without significant drift from the original supervised fine-tuned (SFT) model. This matters because extensive drift can degrade other aspects of the model, such as coherence or factual accuracy. Staying close to the original SFT model helps maintain overall quality.</p> <hr/> <h3 id="some-things-that-were-initially-confusing">Some Things That Were Initially Confusing</h3> <ul> <li><strong>The leap in equation (4)</strong>: I struggled at first to understand how they substituted the optimal policy into their objective so cleanly. It made perfect sense mathematically afterward, but felt like a creative jump rather than an obvious derivation.</li> <li><strong>Weighted gradient interpretation</strong>: Initially, the weighting term in the gradient was counterintuitive due to signs. Eventually, I understood it as correcting more strongly when the model’s implicit reward ordering is confidently wrong.</li> <li><strong>KL Divergence Interpretation</strong>: It wasn’t clear to me initially why minimizing KL divergence from the reference (initial) policy is inherently desirable. Later I understood it prevents excessive drift from a stable baseline, which preserves other desirable model properties.</li> </ul> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>DPO simplifies RLHF dramatically by removing the explicit reward modeling step, making fine-tuning easier, more stable, and faster. Although it’s not strictly reinforcement learning in a traditional sense (it lacks online updates and explicit reward signals), it captures the essence of aligning models to human preferences in a cleaner and more accessible way.</p> <p>The simplicity and clarity of DPO’s core idea—implicitly encoding rewards directly into language models—makes it very appealing. While it’s not the only method (ORPO takes this even further), DPO is an insightful approach that significantly reduces complexity without sacrificing quality.</p> <p>Overall, it’s a valuable reminder that sometimes the best advances come not from adding complexity, but from cleverly stripping it away.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing DPO, a simpler way to fine-tune language models to human preferences without explicitly modeling rewards]]></summary></entry><entry><title type="html">RoFormer – Enhanced Transformer with Rotary Position Embedding - Review</title><link href="https://ht0324.github.io/blog/2025/RoFormer/" rel="alternate" type="text/html" title="RoFormer – Enhanced Transformer with Rotary Position Embedding - Review"/><published>2025-02-13T22:30:00+00:00</published><updated>2025-02-13T22:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/RoFormer</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/RoFormer/"><![CDATA[<p>This post’s review is on <a href="https://arxiv.org/abs/2104.09864">“RoFormer: Enhanced Transformer with Rotary Position Embedding”</a>, a paper that improves the way Transformers handle positional encoding. Transformers typically embed position information by additive vectors, but RoFormer introduces a clever rotation-based positional embedding (RoPE). This method encodes positional relationships multiplicatively, which results in better efficiency and consistency, especially for varying sequence lengths.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Rotary Position Embedding (RoPE)</strong><br/> Unlike traditional positional embeddings, which simply add a positional vector to token embeddings, RoPE applies rotations to embedding vectors based on their positions. Each token’s embedding is rotated in pairs of dimensions using sine and cosine functions. This elegantly encodes positional information by adjusting relative angles between vectors.</p> <p><strong>Multiplicative (Rotation-based) Embedding</strong><br/> The rotation-based embedding transforms embeddings by multiplication instead of addition. This simple yet effective shift captures relative positional information inherently, allowing the embedding to maintain consistent relative positions even if the absolute sequence length changes.</p> <p><strong>Efficient Computation via Orthogonality</strong><br/> Naively implementing rotations would be computationally expensive. However, RoFormer cleverly exploits the sparsity and orthogonality of rotation matrices, drastically reducing computational complexity from quadratic to linear. This means positional embeddings can scale efficiently to longer sequences.</p> <p><strong>Long-term Decay in Attention</strong><br/> RoPE naturally causes the attention weights between distant tokens to decay smoothly. As relative positional distance grows, the interaction between tokens weakens, effectively focusing attention on nearby positions without explicitly setting a fixed window. This resembles the inductive bias present in human language processing.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Simple but Meaningful Idea</strong><br/> Initially, I underestimated how impactful replacing additive positional embeddings with rotations could be. But it turns out this subtle change leads to significant improvements. Rotation inherently encodes relative positional differences, making the model more stable and robust. The idea was simple enough to seem trivial, yet elegant enough to yield practical gains.</p> <p><strong>Consistency Across Sequence Lengths</strong><br/> Traditional positional embeddings like sinusoidal encoding are sensitive to sequence length changes—changing the sequence length shifts positional embeddings and makes learned relationships fragile. RoPE avoids this pitfall by rotating embeddings at fixed angles regardless of sequence length, giving each position a stable identity. This stability makes learning positional relationships more consistent, speeding up convergence.</p> <p><strong>Computational Efficiency via Orthogonality</strong><br/> At first glance, rotation matrices seemed inefficient. But RoFormer cleverly decomposes rotation into sparse orthogonal matrices, dramatically speeding computation. This method allows RoFormer to handle longer sequences efficiently without sacrificing expressiveness or complexity, a clever optimization that was impressive upon deeper consideration.</p> <p><strong>Why Multiplicative is Better than Additive</strong><br/> One insightful realization was why multiplicative (rotation-based) embeddings outperform additive ones. With additive embeddings, absolute positional encodings shift when sequence lengths change. Multiplicative rotation embeddings, however, preserve relative positional angles, allowing the model to generalize better across different contexts and sequence lengths.</p> <p><strong>Connection to Linear Attention and T5</strong><br/> I initially thought linear attention was the key innovation, but realized RoPE’s rotational embedding was the primary novelty. Linear attention was included to address inefficiencies in previous relative position encoding methods (like T5’s quadratic positional matrices), but RoPE itself isn’t restricted to linear attention—it’s broadly applicable.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>RoFormer elegantly solves positional embedding limitations by switching from additive to rotational embeddings. This small shift significantly enhances positional representation stability and efficiency. The multiplicative approach inherently encodes relative positional differences, scales well, and naturally aligns attention weights to human-like linguistic structure.</p> <p>RoPE’s simplicity highlights how impactful thoughtful, minimal changes can be in deep learning. It seems that meaningful advancements often come from refining foundational components rather than adding complexity.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing RoFormer, understanding how simple rotational embedding can significantly boost Transformers by elegantly encoding positional information]]></summary></entry><entry><title type="html">Are You Smarter Than an LLM?</title><link href="https://ht0324.github.io/blog/2025/Smarter-than-LLM/" rel="alternate" type="text/html" title="Are You Smarter Than an LLM?"/><published>2025-02-12T12:00:00+00:00</published><updated>2025-02-12T12:00:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Smarter-than-LLM</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Smarter-than-LLM/"><![CDATA[<p>I recently came across a fascinating and, frankly, humbling interactive blog post titled <a href="https://d.erenrich.net/are-you-smarter-than-an-llm/index.html">“Are you smarter than an LLM?”</a>. It’s a simple concept, but the execution and the implications are profound.</p> <p>The post directly pits the user against a large language model in answering questions from the Massive Multitask Language Understanding (MMLU) benchmark. This experience forced me to confront some preconceived notions I had about LLM capabilities and the meaning of benchmark scores.</p> <hr/> <h3 id="the-mmlu-challenge">The MMLU Challenge</h3> <p>The MMLU benchmark is a well-known evaluation tool for LLMs, designed to measure a model’s ability to understand and reason across a wide range of subjects. It’s become somewhat saturated at the top, with leading models achieving very high scores. I was aware of this saturation, but I hadn’t truly internalized what it meant until I tried the questions myself.</p> <p>The interactive blog post presents MMLU questions, and you answer them alongside the LLM. As you progress, you see both your answers and the LLM’s, along with the correct answers. This direct comparison is where the real learning (and humbling) begins.</p> <p>The MMLU questions are non-trivial. They require a significant breadth and depth of knowledge. Acing these tests, as large language models often do, shouldn’t be taken for granted. I, for one, certainly took it for granted before this experience. When I took this test alongside large language models, the experience of me getting a question wrong while the language model got it right was truly demoralizing.</p> <h3 id="shattering-self-esteem-in-a-good-way">Shattering Self-Esteem (in a Good Way)</h3> <p>The blog post’s title is provocative for a reason. In the context of the MMLU, I had to admit that, in many cases, I wasn’t smarter than the LLM. My score was lower. This isn’t a statement about overall intelligence, of course, but it is a powerful indicator of the superhuman capabilities LLMs are developing in specific domains.</p> <p>I think many people, even those who follow AI advancements, still hold onto a sense of human exceptionalism. We see LLMs achieving high benchmark scores, but we might subconsciously maintain a belief that we’re “still special” in some way. This interactive test directly challenges that notion. It forces you to confront the reality that, in certain areas, LLMs are already surpassing human performance.</p> <p>While MMLU is currently a very saturated benchmark, it’s important to remember that there are many other, even more challenging evaluations out there. For example, <a href="https://epoch.ai/frontiermath">FrontierMath</a> benchmark features problems so difficult that requires several hours or days of work by expert mathematicians. Even Terence Tao participated in its creation. These are exceptionally hard questions, and I am highly skeptical I could answer even one correctly.</p> <p>Yet, even on these incredibly demanding benchmarks, LLMs are starting to achieve non-zero scores, albeit low percentages. And I see no reason why they won’t continue to improve.</p> <hr/> <h3 id="where-do-we-go-from-here">Where Do We Go From Here?</h3> <p>This experience has made me reconsider the trajectory of LLM development. If current models like GPT-4.5 (or whatever the current leading model is) can outperform the average person (and even me!) on a challenging benchmark like MMLU, what will the landscape look like in five years?</p> <p>The pace of progress is astonishing, and this interactive test provides a tangible glimpse into that future. It’s a future where LLMs will likely possess knowledge and reasoning abilities that surpass those of many humans in specific, measurable ways. It’s a future we need to understand and prepare for, and it is already here.</p>]]></content><author><name></name></author><category term="Blog"/><category term="AI"/><summary type="html"><![CDATA[A firsthand experience with the MMLU benchmark]]></summary></entry><entry><title type="html">Mamba – Linear-Time Sequence Modeling with Selective State Spaces - Review</title><link href="https://ht0324.github.io/blog/2025/Mamba/" rel="alternate" type="text/html" title="Mamba – Linear-Time Sequence Modeling with Selective State Spaces - Review"/><published>2025-02-10T23:30:00+00:00</published><updated>2025-02-10T23:30:00+00:00</updated><id>https://ht0324.github.io/blog/2025/Mamba</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Mamba/"><![CDATA[<p>This time I’m reviewing <a href="https://arxiv.org/abs/2312.00752">“Mamba: Linear-Time Sequence Modeling with Selective State Spaces”</a>, a paper aiming to replace attention in Transformers with <a href="https://arxiv.org/abs/2111.00396">state space models (SSMs)</a> that scale linearly with sequence length. The main idea is to introduce “selectivity” into state-space models, enabling them to dynamically focus on or ignore parts of the input, which helps address limitations in traditional Transformer attention models. While interesting, I found this paper particularly challenging - both intuitively and conceptually - due to its complexity and depth of prior research.</p> <hr/> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Selective State Space Models (SSMs)</strong><br/> The core idea is to use SSMs - traditionally linear and time-invariant (LTI) - but introduce selectivity so the parameters can dynamically adapt based on input content. This modification breaks the time-invariance and allows the model to selectively remember or ignore information depending on the input. It’s somewhat similar to gating in LSTMs, but more general.</p> <p><strong>Time-Invariance and Its Breakage</strong><br/> Typical SSMs, like the previous S4 model, are time-invariant - meaning model parameters don’t change with the input or sequence position. Mamba intentionally breaks this constraint, allowing the state update parameters (like the matrix $\Delta$) to be dynamically computed based on the current input. This lets the model “select” what to remember or forget based on the input content.</p> <p><strong>Parallelism (Training vs. Inference)</strong><br/> During training, Mamba operates in a parallel (convolution-like) mode for efficiency. But during inference, it runs in a sequential (recurrent) mode, calculating one step at a time. This hybrid approach gives both efficiency (during training) and flexibility (during inference).</p> <p><strong>Dimensions (D vs. N confusion)</strong><br/> In Mamba, each input channel or embedding dimension (denoted as D) has its own independent state-space model. Within these channels, there’s a latent dimension N that represents the internal hidden state. Understanding this separation was tricky. In simpler terms, each embedding dimension independently runs its own selective SSM with a small latent state N - these dimensions don’t directly interact during the Mamba step.</p> <p><strong>Broadcasting and Selective Updates ($\Delta$ parameter)</strong><br/> One key detail is that the selectivity parameter $\Delta$ is computed from the input and broadcasted across dimensions. $\Delta$ essentially decides how strongly to integrate the current input into the hidden state. A larger $\Delta$ resets the hidden state to pay attention to the current input, while a smaller $\Delta$ lets the hidden state carry more historical information.</p> <p><strong>Connection to Gated Mechanisms (LSTMs)</strong><br/> I realized that Mamba closely resembles gated RNNs like LSTMs. Indeed, the authors explicitly mention that when simplified, selective SSMs reduce to an LSTM-like gate mechanism. In some sense, Mamba can be thought of as a highly refined and generalized form of an LSTM, just scaled up and implemented more efficiently.</p> <hr/> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Selective Attention</strong><br/> Mamba is essentially an advanced form of RNN or LSTM without using explicit attention. By dynamically adjusting how strongly it integrates each input, it selectively “attends” to important tokens. This felt like a neat solution that addresses the drawbacks of simple recurrent models (which can’t selectively filter out irrelevant context) and convolutions (which see everything globally but without context-specific adjustments).</p> <p><strong>Linear-time Sequence Modeling</strong><br/> Transformers scale quadratically with sequence length, limiting practicality for extremely long sequences. Mamba achieves linear-time scaling because the state updates happen independently per channel, avoiding attention’s quadratic complexity. This means it can scale efficiently to very long sequences, like millions of tokens.</p> <p><strong>Compression vs. Retrieval Trade-off</strong><br/> An important intuition I developed was the inherent trade-off Mamba makes compared to attention models. Attention can retrieve information from any position losslessly because it explicitly connects tokens. Mamba, however, compresses all context into a hidden state vector. This makes it memory-efficient but inherently lossy. If crucial information from the distant past isn’t preserved carefully, the model might lose it permanently. On the other hand, this compression is precisely what makes Mamba efficient.</p> <p>For tasks like “needle in a haystack” - finding rare but critical information - this should theoretically be a disadvantage, but Mamba surprisingly performs well. My guess is the model learned effective strategies for compressing and selectively preserving crucial information.</p> <p><strong>Complexity and Interpretability Issues</strong><br/> Mamba is a theoretically elegant but practically complicated model. Its architecture is dense, with a lot of prior literature, especially around the S4 and S6 architectures from Albert Gu and others. Understanding Mamba deeply requires solid familiarity with foundational SSM concepts, which can be daunting without extensive prior study. This complexity might affect its adoption, even if performance is strong.</p> <p><strong>Hardware-Aware Optimization</strong><br/> Another aspect that stood out was Mamba’s aggressive hardware optimization. They discuss parallel scans, kernel fusion, and reducing computational overhead by taking advantage of structured matrices. These optimizations are critical because Mamba inherently loses some parallel efficiency due to its recurrence. However, these steps significantly mitigate performance drawbacks.</p> <hr/> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Mamba tries to replace attention-based Transformers with a linear-time, recurrent, state-space-based approach enhanced by dynamic selectivity. By breaking the traditional LTI assumption, it gains the flexibility to adapt its hidden states selectively, efficiently modeling long sequences. However, this comes with inherent trade-offs in interpretability, retrieval capacity, and complexity.</p> <p>Ultimately, Mamba offers a practical alternative to Transformers for handling extremely long sequences. Still, I see it less as a universal replacement and more as a specialized architecture optimized for scenarios where efficient, long-range modeling with moderate compressive trade-offs makes sense.</p> <p>It’s definitely interesting, but I think its complexity and rigidity might limit broader adoption compared to simpler architectures like Transformers or even more straightforward RNN variants. The idea itself is clever and worth exploring further - but for now, it feels more niche than general-purpose.</p>]]></content><author><name></name></author><category term="Paper"/><category term="AI"/><summary type="html"><![CDATA[Reviewing the Mamba paper, a selective state-space model aiming to replace attention with linear-time sequence modeling]]></summary></entry></feed>