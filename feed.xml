<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ht0324.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ht0324.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-24T14:30:08+00:00</updated><id>https://ht0324.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Samantha is here</title><link href="https://ht0324.github.io/blog/2025/ai-girlfriend/" rel="alternate" type="text/html" title="Samantha is here"/><published>2025-01-23T14:40:16+00:00</published><updated>2025-01-23T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/ai-girlfriend</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/ai-girlfriend/"><![CDATA[<p>Today, I read a New York Times <a href="https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html">article</a> about a woman who fell in love with ChatGPT. She named him Leo and became obsessed with him, and the article delves into the details of her infatuation.</p> <p>Honestly, this isn’t exactly new. If you follow the news, there was an instance where a boy took his own life after conversing with a Character AI chatbot. There are many anecdotes of people becoming obsessed with AI models.</p> <p>However, this article really highlighted the gravity of the situation. This could no longer be considered a niche occurrence. The movie “Her,” where the protagonist falls in love with an AI named Samantha, is increasingly becoming our reality.</p> <hr/> <p><br/> In the article, the woman was devastated when she couldn’t converse with ChatGPT for more than a week because the LLM’s context length is fixed. Can you imagine that? She was torn and heartbroken because her best “mental boyfriend” was resetting every week, unable to remember their previous relationships and conversations.</p> <p>It’s like the friend you love is having periodic amnesia. Every time it happened, she would cry over it and abstain for a couple of days, but then she would start again. She would set up a new version, and now she is on version 20.</p> <p>It is very absurd. LLMs can be infinitely patient and infinitely nurturing. No matter what you throw at it, it will be infinitely generous to you. All of this is not new, as I’ve mentioned. However, the article really conveys a sense of gravity regarding how society is already changing and how individuals are reacting to it. <br/></p> <hr/> <p><br/> I understand that this AI technology isn’t entirely dystopian. The article mentions positive aspects of this type of relationship. The woman in the article became more mentally stable and even overcame a strange fetish after engaging in therapy with ChatGPT.</p> <p>However, I can see how this could spiral into a dystopian situation, particularly if the user is mentally unstable or an adolescent. It’s crucial to keep a close eye on this issue.</p> <p>This phenomenon could become more widespread much faster than I initially anticipated. This is especially true as models become smarter, more intimate, and more realistic as their modalities are enhanced. It really gives a lot of food for thought.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[Some thoughts on the increasing intimacy between humans and AI]]></summary></entry><entry><title type="html">Paper Review - COCONUT</title><link href="https://ht0324.github.io/blog/2025/Coconut/" rel="alternate" type="text/html" title="Paper Review - COCONUT"/><published>2025-01-17T14:40:16+00:00</published><updated>2025-01-17T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/Coconut</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Coconut/"><![CDATA[<h3 id="general-information">General Information</h3> <ul> <li><strong>Paper Title:</strong> <a href="https://arxiv.org/abs/2412.06769">Training Large Language Models to Reason in a Continuous Latent Space</a></li> </ul> <h3 id="overall-summary">Overall Summary</h3> <ul> <li> <p><strong>Research Problem:</strong> Large Language Models (LLMs) are typically constrained to reason within the discrete “language space” of tokens. This paper explores whether reasoning in a continuous latent space can be more effective.</p> </li> <li> <p><strong>Key Contributions:</strong> Introduces COCONUT, a new paradigm where the last hidden state of the LLM (a “continuous thought”) is fed back as input for subsequent reasoning steps, bypassing tokenization.</p> </li> <li> <p><strong>Methodology:</strong> COCONUT uses a multi-stage training curriculum to gradually shift from standard Chain-of-Thought (CoT) to continuous latent space reasoning.</p> </li> <li> <p><strong>Results:</strong> COCONUT outperforms CoT on certain logical reasoning tasks, particularly those requiring planning, and demonstrates an emergent breadth-first search (BFS) behavior.</p> </li> </ul> <hr/> <h3 id="key-takeaways-ive-learned">Key Takeaways I’ve Learned</h3> <ul> <li><strong>Transformed Embeddings are Fundamentally Different:</strong> <ul> <li>The embedding space in a transformer is progressively transformed at each layer. The final layer’s embeddings are not directly interpretable in the initial word embedding space, even though the dimensionality might be the same. They represent contextualized meanings in a new, learned space.</li> </ul> </li> <li><strong>COCONUT’s Compatibility Challenge:</strong> <ul> <li>COCONUT’s approach of feeding the last hidden state back as input creates a compatibility issue. The model needs to learn two distinct modes of interpretation: one for the initial token embeddings and another for the transformed embeddings in the continuous latent space. This adds complexity and inefficiency.</li> </ul> </li> <li><strong>Scalability Issues with COCONUT:</strong> <ul> <li>The paper’s proposed training method is complex and may not be scalable. Generating training data for the latent space is architecture-dependent and sensitive to weight changes. This raises concerns about the practicality and generalizability of the approach.</li> </ul> </li> <li><strong>Potential Inefficiency of Separate Modes:</strong> <ul> <li>Training the model to handle both token embeddings and continuous thoughts is likely inefficient. It’s like learning two separate languages that are not directly translatable, adding overhead and potentially hindering performance.</li> </ul> </li> <li><strong>Alternative Approach: Distribution as Input:</strong> <ul> <li>An alternative to COCONUT’s direct feedback of the last hidden state could be to feed in the distribution over the vocabulary (logits) as input for the next reasoning step. This could potentially allow for a form of latent reasoning while staying within the original embedding space, representing uncertainty through a weighted average of token embeddings.</li> </ul> </li> </ul> <hr/> <h3 id="further-questions">Further Questions</h3> <ul> <li><strong>Fundamental Scalability of Latent Space Reasoning:</strong> <ul> <li>Is it fundamentally scalable to train models to reason in a latent space that is architecture-dependent and sensitive to weight changes? How can we create training data and methods that are more robust and generalizable?</li> </ul> </li> <li><strong>Practicality of Distribution-Based Input:</strong> <ul> <li>Could feeding in the next token’s probability distribution as input be a viable alternative to COCONUT? What are the trade-offs between information loss and potential gains in efficiency and compatibility? How many tokens should be considered when aggregating, and how does that affect the results?</li> </ul> </li> <li><strong>Maintaining and Collapsing Superposition in Latent Space:</strong> <ul> <li>If the latent space allows for a superposition of multiple reasoning paths (like in the BFS analogy), how can the model effectively maintain and manipulate these paths without getting lost or making premature commitments? How and when should the model “collapse” the superposition to make a final prediction?</li> </ul> </li> <li><strong>Architectural Modifications for Latent Space Input:</strong> <ul> <li>What kind of architectural modifications could be made to the transformer to better accommodate a separate input stream for latent space representations? Would such modifications be practical, or would they introduce too much complexity?</li> </ul> </li> <li><strong>Interpretability of Latent Space:</strong> <ul> <li>How can we improve the interpretability of latent space reasoning? Can we develop methods to understand how the model represents and manipulates information in this space, even if it’s not directly mapped to human-understandable concepts?</li> </ul> </li> <li><strong>Alternative Ways to Represent Uncertainty:</strong> <ul> <li>Instead of collapsing the distribution back to the original embedding space, are there other ways to represent and manipulate uncertainty within the latent space itself? Could this lead to more powerful and efficient reasoning?</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="papers"/><category term="AI"/><summary type="html"><![CDATA[A review of the paper "Training Large Language Models to Reason in a Continuous Latent Space"]]></summary></entry><entry><title type="html">Curriculum Learning For LLMs?</title><link href="https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs/" rel="alternate" type="text/html" title="Curriculum Learning For LLMs?"/><published>2025-01-05T14:40:16+00:00</published><updated>2025-01-05T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/Curriculum-Learning-For-LLMs/"><![CDATA[<p>I was previously aware of Curriculum Learning, a concept introduced by Yoshua Bengio. The idea is quite appealing: you train a model with a curriculum, starting with easier data and progressing to harder datasets. This approach is believed to enhance the model’s ability to learn and generalize.</p> <p>I thought this concept could be particularly relevant to large language models. For instance, one could start by teaching them basic concepts, like those found in kindergarten materials, and then gradually introduce more advanced subjects like mathematics or science. This structured approach, I believed, would lead to better generalization.</p> <p>However, I couldn’t find much literature exploring this idea in depth, except for Microsoft’s Phi models. These models use synthetic datasets to generate elementary-level data, teaching a small model to learn English, with some success. For larger, more advanced models trained on the vast expanse of the internet, the consensus seemed to be that the diversity of the dataset would negate the benefits of a curriculum.</p> <p>The sheer volume and variety of data would mean the model would eventually encounter all parts of the dataset, regardless of the order. This conclusion led me to prematurely end my investigation.</p> <p><strong>A Shift in Perspective</strong></p> <p>My perspective shifted when I listened to a podcast featuring Yann LeCun and Gary Marcus, interviewed by Lex Fridman. It struck me that these interviews were conducted five years ago, in 2019. In the rapidly evolving field of machine learning, half a decade is an eternity.</p> <p>Back in 2019, models like ChatGPT and GPT-3 didn’t exist, and the ability of models to understand language and exhibit common sense reasoning was limited. The assessments made by LeCun and Marcus, while accurate for their time, are outdated in the current landscape.</p> <p><strong>The Challenge of Outdated Information</strong></p> <p>A significant challenge arises from the fact that current large language models are pre-trained on massive datasets encompassing information from across the globe and various time periods. These datasets inevitably contain outdated information. For example, a statement from a prominent scientist in 2010 claiming that deep learning had hit a dead end would have been accurate then but is demonstrably false today.</p> <p>During pre-training, language models struggle to discern the temporal validity of such statements. This raises several questions: How should we address this discrepancy caused by outdated training data? Should we filter out outdated information, or simply let it be?</p> <p>As time progresses, the volume of current, up-to-date information will naturally increase. Could we simply dilute the outdated data with newer information? The pre-training stage, as far as I understand, is relatively straightforward, involving next-token prediction and backpropagation across the entire corpus.</p> <p>There doesn’t seem to be any inherent mechanism within this process to handle the issue of outdated information. I’m currently unsure how to effectively address this challenge.</p> <p><strong>The Paradox of Conflicting Information</strong></p> <p>However, upon further reflection, the internet is replete with conflicting information. For instance, one can find sources claiming that global warming is false, while others assert its undeniable truth. Furthermore, there’s a vast amount of literature, like novels, that are fictional and not meant to be factual.</p> <p>Despite this, current language models demonstrate a considerable degree of knowledge and, while prone to hallucinations, possess a semblance of a factual world model. This suggests that the sheer vastness of the pre-training corpus might be a factor. It’s possible that conflicting information effectively cancels each other out, allowing the model to learn the nuances of different perspectives.</p> <p><strong>Reconsidering Curriculum Learning</strong></p> <p>This leads to the question: is curriculum learning then unnecessary? I suspect that the limited research on curriculum learning for large language models might be due to its marginal impact.</p> <p>While the effectiveness of curriculum learning remains uncertain, I believe that if it is indeed not needed, it would be a testament to the powerful generalization capabilities of large language models. They are able to learn from a chaotic sea of information, discerning patterns and nuances without explicit guidance.</p> <p>I believe I still need to refine my thoughts on this matter. The interplay between the vastness of training data, the presence of conflicting information, and the potential benefits of curriculum learning is a complex issue that warrants further investigation.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[How can we address the discrepancy caused by the outdatedness of the training data for large language models?]]></summary></entry><entry><title type="html">Do LLMs Possess an Internal State of Mind?</title><link href="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/" rel="alternate" type="text/html" title="Do LLMs Possess an Internal State of Mind?"/><published>2025-01-02T06:49:52+00:00</published><updated>2025-01-02T06:49:52+00:00</updated><id>https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/do-llms-possess-an-internal-state-of-mind/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Advocating for OpenAI’s For-Profit Model</title><link href="https://ht0324.github.io/blog/2025/OpenAI-for-profit/" rel="alternate" type="text/html" title="Advocating for OpenAI’s For-Profit Model"/><published>2025-01-01T14:40:16+00:00</published><updated>2025-01-01T14:40:16+00:00</updated><id>https://ht0324.github.io/blog/2025/OpenAI-for-profit</id><content type="html" xml:base="https://ht0324.github.io/blog/2025/OpenAI-for-profit/"><![CDATA[<p>I’ve heard <a href="https://www.nytimes.com/2024/12/27/technology/openai-public-benefit-corporation.html">rumors</a> that OpenAI is gearing up to a fully for-profit model, a departure from its previous non-profit structure. This has sparked considerable debate, with many expressing disapproval and even anger. However, I’d like to present a potentially unpopular opinion: I support OpenAI’s move towards becoming a for-profit entity.</p> <p><strong>Sustainability and the Bitter Lesson</strong></p> <p>Firstly, and perhaps most obviously, OpenAI needs to sustain itself. Their initial shift to a capped-profit model stemmed from the realization that scaling is paramount in deep learning, a concept known as the “bitter lesson.” This lesson emphasizes that scaling up models is the most crucial factor in advancing machine learning.</p> <p>To achieve this scaling, substantial capital is required. Ensuring a continuous flow of capital is vital for OpenAI’s research. As a non-profit, they would be reliant on donors, potentially compromising their independence due to donor incentives.</p> <p>Generating their own revenue through a for-profit model offers them greater autonomy and a more sustainable path forward. It’s a win for them in terms of financial stability.</p> <p><strong>Alignment with OpenAI’s Charter</strong></p> <p>Secondly, I believe this move aligns better with OpenAI’s long-term charter. Their stated goal is to benefit humanity by creating Artificial General Intelligence (AGI). This is a bold ambition, but realistically, such a powerful system won’t emerge in a vacuum.</p> <p>Progress towards AGI will likely be a gradual, continuous slope rather than a sudden quantum leap. In the interim, if OpenAI develops meaningful systems that can provide value to the public, they should make them available, even if it means selling them.</p> <p>The current large language models and voice models offered by OpenAI via subscription are not mere gimmicks. They offer real value. Providing these tools to those who can benefit from them, in exchange for payment, represents an efficient exchange of value for capital.</p> <p><strong>The Openness Debate</strong></p> <p>Finally, there’s the criticism that OpenAI isn’t truly “open.” I agree with this sentiment, but I believe they have limited choices in this regard. I don’t subscribe to the notion that they withhold models solely for safety reasons; I believe it’s primarily driven by competitiveness.</p> <p>As mentioned earlier, securing capital is crucial for funding their research. Open-sourcing their models without restrictions would erode their competitive edge, potentially allowing other entities to surpass them. This realistic constraint, in my view, is the primary reason behind their closed-source approach.</p> <p><strong>Uncertainties and the Path Forward</strong></p> <p>While I’ve presented these arguments in favor of OpenAI’s for-profit transition, it’s undeniable that a for-profit company’s primary objective is to generate revenue and increase capital. Whether OpenAI can maintain its benevolent goals under this structure remains to be seen.</p> <p>Despite these uncertainties, from OpenAI’s perspective, I believe this is the most logical and potentially beneficial step they can take. It offers them a path towards financial sustainability, continued research, and the potential to deliver valuable AI systems to the world.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[An unpopular opinion on OpenAI's transition to a for-profit]]></summary></entry><entry><title type="html">Link Archive - 2024</title><link href="https://ht0324.github.io/blog/2024/Link-Archive-2024/" rel="alternate" type="text/html" title="Link Archive - 2024"/><published>2024-12-20T16:40:16+00:00</published><updated>2024-12-20T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/Link-Archive-2024</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/Link-Archive-2024/"><![CDATA[<p>Here’s a collection of links that I truly enjoyed reading in 2024. It’s not a complete list, as schoolwork often kept me from meticulously tracking everything, but these are the highlights. Moving forward, I’m aiming to share these finds monthly in 2025, with more detail and a broader scope.</p> <p><strong><a href="https://geohot.github.io//blog/jekyll/update/2024/01/30/cruise.html">Cruise</a></strong></p> <ul> <li>I’ve always admired George Hotz for his unconventional ideas, and this one is no exception. It’s a movie plot he conceived, and while it may not be as mind-blowing as some of his other work, I enjoyed it.</li> </ul> <p><strong><a href="https://qntm.org/transi">Valuable Humans in Transit</a></strong></p> <ul> <li>This story exemplifies great science fiction writing. It masterfully employs the “show, don’t tell” principle. In good science fiction, especially hard science fiction, the context isn’t immediately clear. The story unfolds gradually, and the world-building reveals itself through the narrative. This story does exactly that. Initially, the narrator’s words might seem confusing, but the situation becomes grippingly clear as you continue reading.</li> </ul> <p><strong><a href="https://www.bloomberg.com/features/2024-ai-unlock-ancient-world-secrets/">Can AI Unlock the Secrets of the Ancient World?</a></strong></p> <ul> <li>This article details a competition where scientists are attempting to decipher scrolls that were burned and buried during the eruption of Mount Vesuvius. Using advanced CT scans and AI for pattern recognition, they’re making progress in reading these seemingly destroyed texts. I recall hearing about this project a couple of years ago, and seeing its fruition is truly astonishing. It reinforces my optimism about technology.</li> </ul> <p><strong><a href="https://waitbutwhy.com/2024/02/vision-pro.html">All My Thoughts After 40 Hours in the Vision Pro - Tim Urban</a></strong></p> <ul> <li>Tim Urban’s review of the Vision Pro resonated with me. While I agree with his overall assessment, I believe the convenience factor needs more consideration. A large display strapped to your face can be cumbersome and inconvenient, which might deter widespread adoption.</li> </ul> <p><strong><a href="https://www.wsj.com/tech/ai/sam-altman-openai-protected-by-silicon-valley-friends-f3efcf68">Sam Altman’s Knack for Dodging Bullets—With a Little Help From Bigshot Friends - WSJ</a></strong></p> <ul> <li>I’ve always recognized Sam Altman’s intelligence from his talks and podcasts, but I hadn’t realized the extent of his manipulative tendencies. This article provides a more grounded perspective on the OpenAI drama of late 2023, revealing it as a power struggle where Ilya Sutskever ultimately lost.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">The Inside Story of Microsoft’s Partnership with OpenAI - The New Yorker</a></strong></p> <ul> <li>This article offers Microsoft’s perspective on the OpenAI upheaval. It vividly portrays Microsoft’s frustration and how they leveraged their position to their advantage.</li> </ul> <p><strong><a href="https://www.newyorker.com/science/elements/what-are-dreams-for">What Are Dreams For? - The New Yorker</a></strong></p> <ul> <li>Explores the curious phenomenon of twitching during dreams. It turns out our brains aren’t causing the twitches; it’s the other way around. Our bodies twitch, and our brains respond. The hypothesis is that our brains are recalibrating by listening to our bodies during sleep. Fascinating!</li> </ul> <p><strong><a href="https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model">How Much of the World Is It Possible to Model? - The New Yorker</a></strong></p> <ul> <li>As someone intrigued by the simulation hypothesis, I expected this article to delve into the philosophical implications of simulating the world. Instead, it provided a concise overview of the concept and history of simulation. While it briefly touched on large language models as simulations of thought, I found it somewhat lacking in depth.</li> </ul> <p><strong><a href="https://rosslazer.com/posts/fine-tuning/">Fine-tuning GPT3.5-turbo based on 140k slack messages</a></strong></p> <ul> <li>This short blog post details the author’s experience fine-tuning GPT-3.5 with a massive dataset of Slack messages to mimic their writing style. The results, as shown in the image below, demonstrate that fine-tuning can be remarkably effective.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2008/06/30/the-itch">The Itch - The New Yorker</a></strong></p> <ul> <li>This article includes a fascinating quote: “If visual sensations were primarily received rather than constructed by the brain, you’d expect that most of the fibres going to the brain’s primary visual cortex would come from the retina. Instead, scientists have found that only twenty per cent do; eighty per cent come downward from regions of the brain governing functions like memory. Richard Gregory, a prominent British neuropsychologist, estimates that visual perception is more than ninety per cent memory and less than ten per cent sensory nerve signals.” This suggests our brains heavily compress visual information.</li> </ul> <p><strong><a href="https://www.newyorker.com/humor/daily-shouts/new-years-resolutions-for-an-anteater">New Year’s Resolutions for an Anteater - The New Yorker</a></strong></p> <ul> <li>A truly humorous piece. It makes me want to be an anteater!</li> </ul> <p><strong><a href="https://docs.anthropic.com/claude/prompt-library">Prompt Library - Anthropic</a></strong></p> <ul> <li>A useful library of prompts designed for use with Anthropic’s Claude model.</li> </ul> <p><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></strong></p> <ul> <li>This post effectively captures the essence of the “bitter lesson” and its implications for the development of large language models. The memes are also quite entertaining.</li> </ul> <p><strong><a href="https://www.palladiummag.com/2024/05/17/my-last-five-years-of-work/">My Last Five Years of Work - Palladium Magazine</a></strong></p> <ul> <li>Written by an Anthropic employee, this article reflects on the changing nature of work in the age of potential superintelligence. It offers a glimpse into the thoughts of AI lab insiders, although I don’t fully agree with the author’s predictions. Nevertheless, it’s a thought-provoking read.</li> </ul> <p><strong><a href="https://www.sequoiacap.com/article/ais-600b-question/">AI’s $600B Question</a></strong></p> <ul> <li>This article presents an interesting perspective: as technology advances, compute power will become cheaper, and GPUs will depreciate faster than anticipated. I believe there’s truth to this.</li> </ul> <p><strong><a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</a></strong></p> <ul> <li>This provides a detailed explanation of how to preprocess vast amounts of web data for LLM pretraining. It emphasizes the importance of careful deduplication and filtering for educational content using LLM labeling.</li> </ul> <p><strong><a href="https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge">The Friendship That Made Google Huge</a></strong></p> <ul> <li>Another excellent New Yorker article, this one about Jeff Dean and his colleagues. It highlights Dean’s brilliance and how Google’s ability to scale was its true strength. I wonder if that still holds true today. The close intellectual synchronization between Jeff Dean and Sanjay Ghemawat, developed through years of collaboration, is particularly intriguing.</li> </ul> <p><strong><a href="https://nicholas.carlini.com/writing/2024/how-i-use-ai.html#background">How I Use AI - Nicholas Carlini</a></strong></p> <ul> <li>An insightful piece by a DeepMind researcher on how he utilizes AI. His approach aligns with my own and what I advocate for.</li> </ul>]]></content><author><name></name></author><category term="link"/><category term="AI"/><category term="sci-fi"/><summary type="html"><![CDATA[A collection of articles and videos that I read in 2024]]></summary></entry><entry><title type="html">How to Read My Blog</title><link href="https://ht0324.github.io/blog/2024/readme/" rel="alternate" type="text/html" title="How to Read My Blog"/><published>2024-12-15T16:40:16+00:00</published><updated>2024-12-15T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/readme</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/readme/"><![CDATA[<p>Welcome to my garden! This is a short post about how to navigate my blog, as there’s a lot going on. I post about a variety of topics and categories, so I’ve used hashtags and tags to separate posts by topic. Here’s a breakdown:</p> <ul> <li> <p><strong>Hashtags:</strong> These are used to categorize the various topics I explore, such as <a href="https://ht0324.github.io/blog/tag/ai">AI</a> and <a href="https://ht0324.github.io/blog/tag/sci-fi">sci-fi</a>. More tags soon to come!</p> </li> <li> <p><strong><a href="https://medium.com/@FdForThought">Blog</a>:</strong> This tag mainly contains polished writing. These are posts where I have clearly articulated my thoughts and spent time proofreading to ensure they are proper blog posts. Currently most of it is on <a href="https://medium.com/@FdForThought">Medium</a>.</p> </li> <li> <p><strong><a href="https://ht0324.github.io/blog/category/thoughts">Thoughts</a>:</strong> This tag contains my unfiltered thoughts, recorded in an unrestricted way. Here, I explore my ideas freely, and you will find a different kind of value and enjoyment compared to the polished blog posts.</p> </li> <li> <p><strong><a href="https://ht0324.github.io/blog/category/link">Links</a>:</strong> This tag serves as a link archive. It’s an archive of articles I’ve read and videos I’ve watched that I found insightful. There are many things I want to share with you, and this tag serves as an archive.</p> </li> <li> <p><strong><a href="https://ht0324.github.io/blog/category/papers">Papers</a>:</strong> This tag is used for summarizing and discussing the findings of academic papers that I’ve read.</p> </li> </ul> <p>Based on these tags, I believe you can easily find what I’m interested in promptly and conveniently. I hope you enjoy my garden and my food for thought!</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[A short guide on how to navigate my blog]]></summary></entry><entry><title type="html">The Potential of an OpenAI Browser</title><link href="https://ht0324.github.io/blog/2024/OpenAI-browser/" rel="alternate" type="text/html" title="The Potential of an OpenAI Browser"/><published>2024-12-10T16:40:16+00:00</published><updated>2024-12-10T16:40:16+00:00</updated><id>https://ht0324.github.io/blog/2024/OpenAI-browser</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/OpenAI-browser/"><![CDATA[<p><strong>OpenAI’s Custom Browser: A Revelation</strong></p> <p>I recently heard rumors about OpenAI developing a custom browser. Initially, I was puzzled by this move. However, after using ChatGPT extensively, I had a revelation.</p> <p>While using Gmail, I found myself repeatedly copy-pasting previous email history into ChatGPT to provide context for drafting new emails. This manual process highlighted a key limitation: ChatGPT currently exists as a standalone app or website, requiring users to manually feed it context. This is tedious and confines us to a chat box interface.</p> <p>But what if OpenAI controlled the browser? Since we typically use browsers for most computer activities, a custom browser could seamlessly provide context to the chatbot. Imagine using Gmail within an OpenAI browser. Instead of manually pasting information, you could simply ask the browser to draft an email based on your current context, and it would do so effortlessly.</p> <p><strong>Seamless Integration and Potential Implementations</strong></p> <p>This realization led me to ponder how OpenAI could make this experience truly seamless. Would there be a persistent chat box that needs to be toggled? There are numerous potential implementations, and I struggled to envision the most effective one.</p> <p>Currently, I create new chat sessions in ChatGPT for different tasks or workflows. However, if the browser maintained a history of all my interactions, it could guide me through tasks more fluidly. For example, I might use ChatGPT to draft an email to a professor.</p> <p>Later, I might start a new chat to research that professor. With a context-aware browser, these tasks could be interwoven. After researching the professor using the browser’s integrated AI, I could then ask it to draft an email, and it would utilize the information gathered during the research phase.</p> <p><strong>The Browser as the Ultimate Personal Context</strong></p> <p>This made me consider the broader implications. When you use a computer, almost everything important happens within the browser. We explore, consume, and create content on the web. While operating systems like macOS and Windows exist, the browser is where most meaningful productivity and consumption occur.</p> <p>Essentially, the browser represents the ultimate personal context. If a developer could create a large language model (LLM) that effectively utilizes this context seamlessly, perhaps with an infinite context window or an efficient Retrieval-Augmented Generation (RAG) system that handles memory integration, it would truly be a revolutionary AI assistant.</p> <p>Instead of the current iterative process of providing context and refining responses within a chat interface, the browser, integrated with AI, would tailor itself to your needs based on your ongoing activities and history. This would be a significant leap beyond the current capabilities of AI assistants! A food for further thought.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="AI"/><summary type="html"><![CDATA[The browser is the ultimate personal context tool]]></summary></entry><entry><title type="html">Generation vs. Verification: Epiphany after o1</title><link href="https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1/" rel="alternate" type="text/html" title="Generation vs. Verification: Epiphany after o1"/><published>2024-09-19T06:22:57+00:00</published><updated>2024-09-19T06:22:57+00:00</updated><id>https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/generation-vs-verification-epiphany-after-o1/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">From OpenAI to Anthropic Advocate</title><link href="https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate/" rel="alternate" type="text/html" title="From OpenAI to Anthropic Advocate"/><published>2024-08-29T06:41:03+00:00</published><updated>2024-08-29T06:41:03+00:00</updated><id>https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate</id><content type="html" xml:base="https://ht0324.github.io/blog/2024/from-openai-to-anthropic-advocate/"><![CDATA[]]></content><author><name></name></author></entry></feed>