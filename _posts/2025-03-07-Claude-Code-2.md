---
layout: post
title: Claude Code — Three Days In
date: 2025-03-07 12:00:00
description: Further reflections on Claude Code as a powerful, actionable assistant and its implications for AI collaboration.
tags: AI
categories: Blog
giscus_comments: true
---

It’s been three days since I started using [Claude Code](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview). The initial excitement was so strong that I immediately wrote a [blog post](https://ht0324.github.io/blog/2025/Claude-Code/) about it. But after a couple more days, having spent more time exploring its capabilities, I’ve found myself with even more thoughts—so here’s a quick follow-up.

---

### Time-Saving: Understated Yet Mind-Blowing

I know I've already emphasized how much time Claude Code saves, but seriously, I can't stress it enough. Over the past couple of days, I've been completely revamping my personal website from the ground up. Let me preface this by saying: I’m not really a web guy. Before Claude Code, my blog was just a clone of the [Al Folio](https://alshedivat.github.io/al-folio/) theme. Whenever I wanted to tweak something, it felt daunting. Each tiny modification usually ended in frustration or compromise because the effort required to change something small just wasn't worth it.

Now, if I want to adjust the tag styling or add new functionality—boom, I just instruct Claude Code to do it. It happily handles the rest. The difference in my blog over just these past couple of days is truly night and day. I've implemented numerous nooks and crannies, subtle features, and design tweaks that previously felt impossible.

Of course, Claude isn't perfect, but the sheer amount of time it saves is staggering. To put it into perspective, yesterday I wasn't satisfied with how the tag feature functioned, so I decided to change it up. Claude handled the entire process for about $1 in API costs. A dollar. Less than a cup of coffee. Without it, the same change would have probably taken me hours of tinkering and frustration.

At first, Claude felt like a pair-programming partner, but even after just three days, that feeling is fading. Instead, it's become something more like a virtual assistant—an assistant that can actually do things, not just talk about them.

---

### Chatbots vs. Agents: "What Is" vs. "Do It"

Using traditional chatbots like ChatGPT always felt like carefully constructing queries, specifying every little detail of my intent. Most of my interactions with ChatGPT—and likely yours too—are fundamentally knowledge retrieval: asking "What is this?" or "Tell me about that." ChatGPT essentially compresses vast internet knowledge and provides succinct, precise answers. It's exceptional at answering questions and explaining concepts, but it's mostly passive—focused on understanding and summarizing existing knowledge.

But Claude Code changed the game completely. With a truly agentic model like Claude, I don't have to painstakingly spell out exactly how I want something done. Instead, I just specify *what* I want done, and Claude figures out the *how*. It doesn't just summarize information; it actively plans, makes decisions, and implements changes autonomously. This shift from explicitly defined instructions ("what is") to implicit, actionable intent ("do it") feels genuinely profound—a step change in capability that transforms the user experience.

Of course, because Claude isn't perfect yet, the collaboration still exists in the form of verifying the results. But as I've [said before](https://medium.com/@FdForThought/framing-rlhf-as-generation-vs-verification-4d9e95b88534) [many times](https://medium.com/@FdForThought/generation-vs-verification-epiphany-after-o1-713c6f411206): verification is vastly easier than generation, and right now, I'm comfortably on the verification side. I simply check Claude's implementation by deploying my blog and testing the functionality. This still requires my input, but the mental load is drastically lighter.

---

### From Collaboration to Automation

While it still feels somewhat collaborative now, I'm realizing that my role is already shrinking from active collaborator to passive verifier. It's an assistant relationship—not really a partnership anymore. Verification feels simpler and simpler, and honestly, I can foresee even verification becoming automated soon. At that point, human involvement becomes negligible.

Previously, I believed in the paradigm that self-feedback loops (generation versus verification) would incrementally increase intelligence. But intelligence and agency are fundamentally different. Learning how to be smart/knowledgable and learning how to act are not the same, and the latter is far more powerful. I'm now convinced that agentic capability will lead to implications far bigger and faster than I initially anticipated.

If (or more realistically, *when*) verification itself is automated, we won't be collaborating with AI—we'll be observing it collaborating with itself. That's when things will get really interesting (and potentially unsettling).

---

### Toward Agentic Intelligence?

OpenAI previously described levels of AI intelligence: Level 2 (reasoners), Level 3 (agents), and Level 4 (collaborators or organizations). Claude Code sits loosely at Level 3, and my recent experiences hint strongly that Level 4—agents autonomously collaborating with each other—is closer than we might think.

As these agentic models improve and increasingly handle their own verification, we'll approach a point where human oversight becomes unnecessary. This isn't speculation anymore; using Claude Code has convinced me that the shift toward fully autonomous agentic systems isn't just possible—it's imminent.

The impact of that shift can't be overstated. As impressive as Claude Code is today, it's the worst this technology will ever be. And that's both thrilling and deeply concerning.