<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Thoughts on GPT-4o Image Generation | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="What GPT-4o's image capabilities tell us about the future of operating systems"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/gpt4-image-generation/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Thoughts on GPT-4o Image Generation</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> March 26, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/thoughts" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Thoughts</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>Today, OpenAI revealed GPT-4o’s <a href="https://openai.com/index/introducing-4o-image-generation/" rel="external nofollow noopener" target="_blank">image generation capabilities</a>. While this feature was previewed in the initial 4o announcement about <a href="https://openai.com/index/hello-gpt-4o/" rel="external nofollow noopener" target="_blank">a year ago</a>, the actual results are still surprising. Playing with it triggered some realizations about the future implications of direct image generation that I want to share here.</p> <hr> <h3 id="the-multimodal-approach">The Multimodal Approach</h3> <p>The concept isn’t new. When OpenAI introduced GPT-4o, they described it as modeling input as text, pixels, and sounds, combining all modalities with one big autoregressive transformer. The output would likewise be text, images, and audio.</p> <p>They presented it as straightforward, though there are complicated architectural decisions behind the scenes. But the impact is clear: the model is much more capable at image generation.</p> <p>In previous systems (diffusion models or other chatbot interfaces), images were generated through a two-step process. The language model would generate a sophisticated prompt and feed that into a separate image model. GPT-4o eliminates this by directly generating images.</p> <p>Since the large language model directly generates images, it’s more capable in this context. We can also feed in images, and the results speak for themselves. The images show more consistency. When previous chatbots wanted to modify an image, they had to create a detailed description of that image and feed it into another diffusion model. This was a bottlenecked process, but with 4o, it can handle images directly, modifying them with more consistency.</p> <p>What’s notable is how refined the model’s text generation capabilities are within images. Previous diffusion models struggled with generating clean, readable text in images; it was often distorted, nonsensical, or limited to a few words. GPT-4o can generate clean text, and lots of it. The text is consistently readable and contextually appropriate.</p> <hr> <h3 id="spark-of-software-20">Spark of Software 2.0</h3> <p>My most visceral moment came from an example where they showed a cat image being iteratively transformed into a game interface. Through multiple iterations, they turned it into an image of a game with a UI, and the text was accurate. The model produced an image with a clean interface, and everything was consistent.</p> <p>That’s when it hit me: if the model can generate UI and text so accurately, in the future our computer interfaces could be entirely AI-generated in real-time with all the context available from the user. Large language models could generate your computer interface frame by frame based on your input and feedback.</p> <p>Imagine a user interface that changes based on user needs. Our current operating systems (macOS, Linux, Windows) are rule-based with fixed definitions. But what if a large language model generated a new UI that helped the user get things done? It would be adaptive and different for every user, changing styles and functionality based on preferences or context.</p> <p>What would a word processor look like in that interface? The possibilities seem endless. Currently, an OS has the lower-level kernel with renderers and shaders that produce pixels. But this would be an end-to-end network - a large language model OS directly generating pixels from our input.</p> <p>The concept of world simulators isn’t new: NVIDIA is already using their <a href="https://blogs.nvidia.com/blog/what-is-robotics-simulation/" rel="external nofollow noopener" target="_blank">Isaac Sim platform</a> to <a href="https://blogs.nvidia.com/blog/openusd-sdg-advance-robot-learning/" rel="external nofollow noopener" target="_blank">generate synthetic data for training robot models</a>, Google DeepMind has developed <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie 2</a> that can generate interactive 3D environments, and Microsoft Research recently unveiled <a href="https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/" rel="external nofollow noopener" target="_blank">Muse</a>, their first generative AI model designed for gameplay ideation. But what if instead of simulating physical worlds, we used these capabilities to simulate an operating system? That’s where my realization really hit me.</p> <p>One of the most compelling aspects of this approach would be how it leverages context. Context is incredibly important for large language models. Your previous conversations, actions, and preferences inform their responses. Current operating systems gather tons of contextual information about how we use them, but this data isn’t being utilized well. What if an OS could learn and adapt from your previous interactions, inputs, preferences, and habits while you used it? Every aspect of your computing experience could be customized not by explicit settings, but by the system understanding you over time.</p> <p>This is basically <a href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="external nofollow noopener" target="_blank">Software 2.0</a> and <a href="https://x.com/karpathy/status/1723140519554105733" rel="external nofollow noopener" target="_blank">LLM OS</a> that Andrej Karpathy described. I was aware of this idea before, but I hadn’t felt it would be feasible in such a short time. With some effort, this kind of OS might be technically possible now with large-scale servers and APIs, but it would be largely impractical. Yet the implications are large, and I believe this kind of OS is inevitable.</p> <p>Of course, it wouldn’t be a fully end-to-end neural network. There would be some rule-based systems guiding the LLMs. But my point still stands: we might be seeing the first glimpse of a new OS approach.</p> <p>Karpathy mainly discussed Software 2.0 replacing rule-based software stacks, which is already happening. But I think Software 2.0 will also eventually replace the OS stack itself.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>