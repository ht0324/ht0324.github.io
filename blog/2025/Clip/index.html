<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Learning Transferable Visual Models from Natural Language Supervision (CLIP) – Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="A review of OpenAI's CLIP model - examining how large-scale image-text learning enables zero-shot visual recognition"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Clip/"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning Transferable Visual Models from Natural Language Supervision (CLIP) – Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> February 3, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>This time, I reviewed the <a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">CLIP paper</a>, which investigates whether learning visual concepts from large-scale natural language supervision can yield strong generalization without explicit task-specific fine-tuning.</p> <p>Here’s my breakdown of the important ideas, observations, and what I took away from reading it.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Natural Language Supervision (Not Gold Labels)</strong><br> Traditional image classifiers rely heavily on carefully labeled datasets (like ImageNet), which inherently limits their generalization. CLIP takes a different route - using weak supervision by pairing images from the internet with their associated text descriptions. This approach leverages massive scale (400 million image-text pairs) rather than high-quality labels.</p> <p><strong>Contrastive Learning of Joint Image-Text Embeddings</strong><br> CLIP trains two encoders - one for images (e.g., ResNet or ViT) and one for text (a Transformer) - to produce embeddings in a shared latent space. The training objective encourages embeddings of matching image-text pairs to be closer together (high cosine similarity) and unrelated pairs farther apart. It’s fundamentally a contrastive learning task, not generation or pure classification.</p> <p><strong>Zero-shot Transfer with Prompting</strong><br> After training, CLIP achieves impressive zero-shot performance by converting downstream classification tasks into text prompts. For instance, rather than learning an explicit classification head for “cat” images, CLIP generates embeddings for prompts like “a photo of a cat,” and classifies images by comparing these prompt embeddings to the image embedding. The choice of prompts matters - ensembling multiple prompts significantly boosts performance.</p> <p><strong>Robustness to Distribution Shifts</strong><br> The paper emphasizes CLIP’s impressive robustness under dataset shifts. While supervised models trained specifically on datasets like ImageNet often suffer dramatically on slightly modified datasets (ImageNet-R, ImageNet-A), CLIP maintains strong performance. This suggests its training on diverse, web-scale data enables better out-of-distribution generalization.</p> <p><strong>Simplicity and Scalability</strong><br> Interestingly, despite the paper’s extensive evaluations, the model itself is straightforward - no complex architectural tweaks or custom layers. They found even simple linear projections were sufficient, and end-to-end training from scratch without special initialization performed better, underscoring that simplicity combined with massive scale can outperform intricate architectures.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Scale Beats Label Quality</strong><br> CLIP demonstrates that massive scale can compensate for lower-quality labels. By training on enormous amounts of internet data without careful annotation, CLIP achieves zero-shot results that often match supervised models. This reinforces a now-familiar theme: more diverse data often beats carefully curated labels, especially for generalization.</p> <p><strong>Prompt Engineering and Ensembling are Surprisingly Powerful</strong><br> CLIP converts image classification tasks into natural language prompts. At first glance, this seems simplistic, but the paper convincingly shows that careful prompt design matters significantly. Due to polysemy (words having multiple meanings), using multiple prompts for a single concept and averaging their embeddings dramatically improves accuracy. This underscores the subtle complexity hidden within seemingly straightforward prompting.</p> <p><strong>CLIP’s Robustness Comes from Data Diversity</strong><br> One of CLIP’s most interesting features is its robustness to distribution shifts, something supervised models frequently struggle with. Initially, I thought this robustness came purely from scale (sheer data volume). But the variety and diversity of the data likely play a bigger role - CLIP encounters a wide range of representations of each concept, making it inherently less brittle to new variations.</p> <p><strong>Limitations in Specialized Tasks</strong><br> Despite its impressive generalization, CLIP falls short on highly specialized or unusual tasks (like precise numerical estimation from images or highly specialized domain tasks). These limitations show that massive-scale generalization doesn’t automatically imply competence in specialized or fine-grained tasks. It’s a clear reminder that general-purpose models still benefit from domain-specific fine-tuning when precision is needed.</p> <p><strong>Humans vs. CLIP in Few-shot Learning</strong><br> The paper briefly discusses an intriguing comparison: humans rapidly improve in recognition tasks with just one example (one-shot learning) but plateau quickly after two or three. In contrast, CLIP (or rather its linear probe) requires several examples (4+) to activate fully. Initially, this confused me. Upon reflection, it’s probably because humans have deeply pre-trained cognitive frameworks instantly adaptable, while linear probes initially lack meaningful activation and thus require multiple examples. It shows the fundamental differences in learning dynamics between humans and machines.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>CLIP is essentially applying the familiar GPT-style “scale-and-diversify” playbook from NLP to vision - massive web-scale datasets, transformer-based encoders, minimal architectural complexity, and zero-shot prompting. The concept isn’t overly complicated, yet the extensive experiments showcase nuanced findings about scale, generalization, and robustness.</p> <p>What stuck with me most:</p> <ul> <li> <strong>Simplicity matters</strong>: Even basic encoders, if trained at massive scale, outperform intricate supervised models.</li> <li> <strong>Generalization requires diversity</strong>: CLIP’s robustness is remarkable, not due to sheer scale alone, but primarily due to the variety in its training data.</li> <li> <strong>Prompts are deceptively subtle</strong>: Simple linguistic variations can significantly impact accuracy, highlighting subtlety in zero-shot prompting.</li> </ul> <p>In short, CLIP provides strong evidence that general-purpose visual models can achieve remarkable performance through large-scale, unsupervised natural language supervision.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>