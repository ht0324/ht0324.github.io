<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeepSeek GRM - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Reviewing the DeepSeek paper on Inference-Time Scaling for Generalist Reward Modeling"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/grm/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">DeepSeek GRM - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> April 12, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>This time, I’m looking at the paper <a href="https://arxiv.org/abs/2504.02495" rel="external nofollow noopener" target="_blank">“Inference-Time Scaling for Generalist Reward Modeling”</a> by Liu et al. from DeepSeek-AI. Given DeepSeek’s recent work, I went into this with some anticipation, though my initial feeling was that it might be less of a fundamentally new algorithm and more a detailed concretization of concepts like RLAIF and <a href="https://arxiv.org/abs/2212.08073" rel="external nofollow noopener" target="_blank">Constitutional AI (CAI)</a>. Still, the paper frames the problem clearly and the results are promising.</p> <p>The core challenge addressed is obtaining accurate reward signals for LLMs in general domains, where tasks aren’t easily verifiable like math problems. While methods like RLAIF help scale beyond human feedback, the paper points out issues with existing reward models (RMs): inflexibility to input types, accuracy limitations, and poor scaling with inference-time compute. This work proposes a way to improve generalist RMs, specifically focusing on making them better when given more thinking time (inference compute).</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Pointwise Generative Reward Modeling (GRM)</strong><br> The authors adopt a GRM approach. Instead of just outputting a scalar score or a pairwise preference, the RM generates textual output (critiques based on principles) and assigns individual (pointwise) scores to each response (e.g., 1-10). This architecture provides flexibility in handling single, paired, or multiple responses using the same format.</p> <p><strong>Self-Principled Critique Tuning (SPCT)</strong><br> This is the proposed training methodology. Unlike CAI which uses a fixed, human-defined constitution, SPCT aims to train the GRM to <em>dynamically generate</em> relevant principles and critiques based on the specific input query and responses.</p> <p><strong>SPCT Stage 1: Rejective Fine-Tuning (RFT)</strong><br> A “cold start” phase to get the GRM generating principles and critiques in the correct format. It uses existing RM datasets and samples trajectories (principle + critique + score). Trajectories are rejected if the predicted reward is incorrect (doesn’t match ground truth preference) or if the task is “too easy” (all sampled trajectories for a given input are correct).</p> <p><strong>SPCT Stage 2: Rule-Based Reinforcement Learning</strong><br> An online RL phase (using a GRPO setup) to further improve the GRM. The key here is that the RL optimizes the <em>reward model itself</em>. The reward signal for this RL process is based on simple accuracy rules: does the GRM’s generated critique and score correctly identify the best response according to the ground truth preference label from the dataset? A KL penalty is used to maintain stability.</p> <p><strong>Inference-Time Scaling via Sampling &amp; Voting</strong><br> To leverage more compute at inference, the paper proposes sampling <code class="language-plaintext highlighter-rouge">k</code> times from the trained GRM for the same input. Each sample yields a potentially different set of principles, critiques, and scores. The final score for a response is obtained by summing the scores across all <code class="language-plaintext highlighter-rouge">k</code> samples (“Voting”). This expands the effective reward range, allowing for finer granularity.</p> <p><strong>Meta Reward Model (Meta RM)</strong><br> As an enhancement to voting, a separate, smaller RM is trained to evaluate the quality of the principles and critiques generated by the main GRM in each of the <code class="language-plaintext highlighter-rouge">k</code> samples. During inference, this Meta RM filters the <code class="language-plaintext highlighter-rouge">k</code> samples down to the top <code class="language-plaintext highlighter-rouge">k_meta</code> based on critique quality, and voting is performed only on these higher-quality samples.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Problem Framing: Generalist Rewards &amp; Scaling</strong><br> The paper does a good job setting up the motivation. Getting good rewards for complex, open-ended tasks is hard. While CAI introduced principles, this work focuses on making the principle application dynamic and scaling the RM’s quality with compute. The four challenges identified (flexibility, accuracy, inference scaling, learning scalable behaviors) provide a clear target.</p> <p><strong>Dynamic Principles vs. Fixed Constitution</strong><br> The shift from CAI’s static constitution to SPCT’s dynamically generated principles felt like a notable difference. The idea is that the RM learns to adapt its evaluation criteria (the principles) to the specific context, rather than relying on a predefined, potentially inflexible set. This seems like a natural evolution.</p> <p><strong>Improving the Reward Model, Not Just the Policy</strong><br> A point that required careful distinction during discussion was the target of the RL. While methods like GRPO use RL to improve the <em>policy</em> LLM based on a reward signal, the rule-based RL in SPCT is used to improve the <em>reward model itself</em>, making it better at generating principles and critiques that align with ground truth preferences. It’s a bit “meta”: training the judge to be a better judge.</p> <p><strong>The “Rejecting Too Easy” Strategy</strong><br> The RFT stage’s approach of discarding trajectories where the GRM was correct every time was interesting. There are a few angles: Is it just removing uninformative data where the model already performs perfectly? Or is it actively forcing the model to focus on harder examples where it might struggle, thereby promoting more robust learning? Or perhaps it simplifies downstream ranking if all examples aren’t trivially correct? It seems like a pragmatic way to focus the training signal.</p> <p><strong>How Inference Scaling Works (Summing, Not Averaging)</strong><br> The voting mechanism (Eq. 6) initially seemed odd – why sum scores instead of averaging? But, the thing to remember is that the goal is <em>ranking</em> responses. Summing preserves the relative ranking just as averaging would, but directly reflects the expanded granularity achieved by sampling multiple “perspectives” (principles). Since the absolute value isn’t used directly in a loss function later (unlike scalar value learning), maintaining a fixed 1-10 scale via averaging isn’t strictly necessary.</p> <p><strong>Performance: Scaling Matters</strong><br> The results (Tables 2, 3, 6, Figure 4) show that the SPCT-trained DeepSeek-GRM performs well, beating baselines and competing with strong models. More importantly, the inference-time scaling works. Using more samples (<code class="language-plaintext highlighter-rouge">k=32</code>) or the Meta RM (<code class="language-plaintext highlighter-rouge">k=8</code> or <code class="language-plaintext highlighter-rouge">k=16</code>) allows the 27B GRM to match or exceed the performance of much larger models (like a 671B RFT model) that use less inference compute. This supports the core premise that investing compute at inference time can be effective if the model is trained appropriately (via SPCT).</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This paper presents Self-Principled Critique Tuning (SPCT) as a method to train Pointwise Generative Reward Models (GRMs) that generate dynamic principles and critiques, enabling effective inference-time scaling. By training the RM itself using a combination of rejective fine-tuning and rule-based online RL, the authors create a system (DeepSeek-GRM) that improves its reward signal quality when given more compute via sampling.</p> <p>While building on ideas from RLAIF and CAI, the dynamic principle generation and the explicit focus on training the RM for inference-time scalability (including the Meta RM) feel like distinct contributions. The empirical results suggest that scaling inference compute via sampling, especially when guided by a meta-judge, can be an effective way to boost reward quality, potentially rivaling the gains from simply scaling model size during training. It makes me curious about how these improved reward models will be used to train DeepSeek’s next generation of policy models.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>