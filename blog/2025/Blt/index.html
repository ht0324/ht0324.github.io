<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>BLT – Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="My thoughts on the Meta's Byte Latent Transformer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Blt/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">BLT – Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> January 19, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>This is a review of the paper <a href="https://arxiv.org/abs/2412.09871" rel="external nofollow noopener" target="_blank">“Byte Latent Transformer: Patches Scale Better Than Tokens”</a>. This paper introduces the Byte Latent Transformer (BLT), a language model that directly operates at the byte-level without tokenization. Tokenization has long been considered a “necessary evil,” a heuristic step used for efficiency but fundamentally limiting the flexibility and generalization of models.</p> <p>BLT replaces traditional fixed-vocabulary tokens with dynamically-sized byte patches, segmenting input sequences based on entropy (uncertainty). This approach improves inference efficiency and robustness compared to traditional tokenizer-based models, which allocate equal compute resources to each token, regardless of complexity.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Byte Latent Transformer (BLT)</strong><br> BLT is a Transformer model designed to handle raw byte inputs directly, eliminating tokenization entirely. It processes sequences by dynamically grouping bytes into patches, allowing it to adaptively allocate compute resources depending on data complexity rather than using static token boundaries.</p> <p><strong>Entropy-Based Patching</strong><br> Instead of using heuristics like Byte Pair Encoding (BPE), BLT segments bytes into patches based on entropy. Regions with higher entropy (complexity) are segmented into smaller patches, and simpler regions use larger patches. This dynamic patching is incremental, meaning each decision is made only based on previous bytes.</p> <p><strong>Local Encoder and Dynamic Patching</strong><br> After patch segmentation, a “local encoder,” a lightweight Transformer, converts byte patches into vector representations. Crucially, this encoder uses cross-attention (rather than typical self-attention), aggregating byte-level information within a patch into a single embedding. This approach efficiently encodes byte-level context into a manageable format for the global Transformer layers.</p> <p><strong>Hash n-gram Embeddings</strong><br> BLT enhances byte embeddings by hashing sequences of bytes (n-grams) and mapping them into a learned embedding table. This method captures local byte-level context without storing embeddings for every possible n-gram, improving representational efficiency.</p> <p><strong>Bits-Per-Byte (BPB)</strong><br> Instead of perplexity, BLT uses Bits-Per-Byte (BPB) as a measure to compare models. BPB assesses how effectively a model compresses data at the byte-level, making it suitable for evaluating byte-based and tokenizer-based models fairly.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Tokenizer-Free Modeling Is Possible</strong><br> I’ve always agreed with Andrej Karpathy’s view of tokenization as a “necessary evil,” useful but limiting. This paper shows that tokenizer-free modeling at the byte level is feasible, with performance comparable to token-based models. The shift toward end-to-end byte-level processing looks promising.</p> <p><strong>Dynamic Patching is Smart and Efficient</strong><br> The idea of entropy-based dynamic patching is a neat one. Instead of wasting compute resources uniformly, BLT assigns computational power based on local complexity. If a sequence of bytes is predictable, BLT groups them into larger patches to save computation; if unpredictability rises, patches become smaller. This method aligns computational cost directly with informational complexity, improving efficiency.</p> <p><strong>Local Encoder and Cross-Attention</strong><br> The local encoder works well: it compresses raw byte sequences into meaningful embeddings. Initially, I didn’t fully grasp why cross-attention was chosen over self-attention, but now it’s clear: cross-attention neatly summarizes byte-level details into concise patch representations. It balances complexity and efficiency.</p> <p><strong>Why Hash n-gram Embeddings Work Well</strong><br> Hashing n-grams to enrich byte embeddings was another subtle but useful choice. The hashing approach allows BLT to incorporate extensive byte-level context without becoming computationally overwhelming or needing a massive vocabulary. It was a simple solution to a potentially complicated problem, and a good fit for the architecture.</p> <p><strong>Performance Trade-offs and Practical Challenges</strong><br> Despite its innovations, BLT showed a noticeable performance degradation on some benchmarks compared to the tokenizer-based Llama 3. The paper didn’t fully clarify this performance gap, but it might be due to smaller datasets, less optimized hyperparameters, or inherent trade-offs between efficiency and representational power. Additionally, BLT’s lack of fixed vocabulary and special tokens (like end-of-sequence markers) complicates practical tasks like customization or fine-tuning.</p> <p><strong>Limitations in Scalability and Practicality</strong><br> Although BLT shows strong potential, its practical scalability wasn’t convincingly demonstrated at the 8B parameter scale. Tokenizers, despite their limitations, make model customization straightforward, something BLT inherently sacrifices. Future work should explore how BLT scales and whether it can effectively handle practical issues like special tokens or model fine-tuning.</p> <hr> <h3 id="summary">Summary</h3> <p>The BLT paper was insightful, introducing a tokenizer-free architecture that processes raw bytes directly. Its dynamic patching and local encoder designs improve efficiency and robustness. However, practical issues and scaling limitations indicate there’s more work needed before BLT-like architectures can replace tokenizer-based models in large-scale, real-world applications.</p> <p>It’s good to see steps toward end-to-end language modeling without tokenizers, but achieving both efficiency and practicality at massive scales still presents an open challenge.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>