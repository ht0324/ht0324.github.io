<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Let's Reproduce GPT-2 by Karpathy - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="My notes and takeways on Andrej Karpathy's GPT-2 reproduction video"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Karpathy-gpt2/"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Let's Reproduce GPT-2 by Karpathy - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> January 14, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>I recently watched Andrej Karpathy’s <a href="https://youtube.com/watch?v=l8pRSuU81PU" rel="external nofollow noopener" target="_blank">“Let’s Reproduce GPT-2 (124M)”</a> video. This post covers the core ideas and key insights I learned, especially around positional embeddings, transformer architecture tweaks, and practical considerations when implementing models like GPT-2.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Transformer Modifications: Pre-Layer Normalization (Pre-LN)</strong><br> In the original transformer, Layer Normalization comes after attention and feedforward layers (Post-LN). GPT-2 switched this, applying normalization beforehand (Pre-LN). This seemingly small tweak stabilizes training by improving gradient flow, especially in deeper models.</p> <p><strong>Transformer as MapReduce</strong><br> Karpathy uses an analogy: Transformers operate similarly to a MapReduce process. Self-attention acts like a “Reduce” step, combining information from all tokens, while the subsequent MLP acts like “Map,” independently processing each token’s representation.</p> <p><strong>Optimizing Attention Computation (Tensor Gymnastics)</strong><br> GPT-2’s attention mechanism involves tensor reshaping for efficiency. Input tensors are projected simultaneously into queries, keys, and values across all heads using batch matrix multiplication. These tensors are then reshaped and transposed efficiently for multi-head attention computation.</p> <p><strong>FlashAttention</strong><br> FlashAttention is a recent, efficient algorithm for speeding up attention computations. By breaking attention into smaller “tiles” and combining multiple computations (kernel fusion), it reduces memory usage, allowing transformers to handle longer sequences faster.</p> <p><strong>Weight Sharing Between Input and Output Layers</strong><br> GPT-2 shares weights between the token embedding layer (<code class="language-plaintext highlighter-rouge">wte</code>) and the final linear output layer (<code class="language-plaintext highlighter-rouge">lm_head</code>). This design choice helps the model learn more consistent representations and often results in better generalization, despite initially seeming counterintuitive.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Why Learned Positional Embeddings Work Well</strong><br> Initially, positional embeddings felt abstract. Now I see they’re powerful because each embedding channel independently captures different positional information, like long-range versus short-range patterns. This allows the model to adapt its position representations specifically to the training data, unlike fixed sine-cosine methods.</p> <p><strong>The Impact of Pre-LN on Training Stability</strong><br> Before this, I thought normalization order wouldn’t matter much. But moving LayerNorm before each attention and MLP block stabilizes gradients, helping especially in deeper transformer stacks. This highlights the subtle but meaningful effects minor architectural tweaks can have.</p> <p><strong>Autoregressive Prediction and the “Last Vector” Mystery</strong><br> I always wondered why GPT predicts from only the last token’s embedding, even though the whole sequence is processed. The reason: the last token embedding naturally contains information from the entire context due to causal attention, preserving autoregressiveness efficiently. Using earlier vectors directly would break this causal property.</p> <p><strong>Why Attention Head Dimensionality (<code class="language-plaintext highlighter-rouge">head_size</code>) Matters</strong><br> Each attention head having a smaller, separate dimensionality (<code class="language-plaintext highlighter-rouge">head_size</code>) surprised me at first. But now, it makes sense: smaller head dimensions force each attention head to specialize in capturing distinct relationships, making the model both efficient and expressive.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This GPT-2 deep dive clarified many subtle yet significant details such as positional embeddings, attention optimizations, and training nuances. Small architectural choices, like moving normalization or adjusting head dimensions, can meaningfully impact performance.</p> <p>The MapReduce analogy particularly helped demystify transformers, clarifying the internal data flow. Also, understanding why GPT predicts using only the final token made its autoregressive nature clear.</p> <p>Overall, transformers have subtle complexities beneath their simplicity, and Karpathy’s reproduction video made me appreciate these subtleties more deeply.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>