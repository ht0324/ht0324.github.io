<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Rethinking Sequence-to-Sequence - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Looking back at the 2015 paper that introduced an attention-like mechanism to NMT."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/attention/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Rethinking Sequence-to-Sequence - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> April 26, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>Reading foundational papers often provides a clearer perspective on how current ideas evolved. Recently, I went through the 2015 ICLR paper “<a href="https://arxiv.org/abs/1409.0473" rel="external nofollow noopener" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>” by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. It tackles a core problem in early sequence-to-sequence models for machine translation.</p> <p>The main issue they identified was the “bottleneck” inherent in the standard RNN Encoder-Decoder framework popular at the time (like in Cho et al., 2014a or Sutskever et al., 2014). These models tried to compress the entire meaning of a source sentence, regardless of its length, into a single fixed-length vector. As the paper noted, this makes it difficult to handle long sentences well, performance tended to drop off significantly as sentences got longer.</p> <p>Their proposed solution was to allow the decoder to look back at the source sentence and selectively focus on relevant parts when generating each target word. This avoids forcing all information through one fixed vector.</p> <h3 id="key-concepts">Key Concepts</h3> <p>Here’s a breakdown of the core ideas discussed:</p> <ul> <li> <strong>The Problem: Fixed-Length Vector Bottleneck:</strong> Standard encoder-decoders map an input sequence <code class="language-plaintext highlighter-rouge">x = (x_1, ..., x_{T_x})</code> to a fixed context vector <code class="language-plaintext highlighter-rouge">c</code>. The decoder then generates the output <code class="language-plaintext highlighter-rouge">y = (y_1, ..., y_{T_y})</code> based solely on <code class="language-plaintext highlighter-rouge">c</code> and previously generated words. This compression limits the model’s capacity, especially for long inputs.</li> <li> <strong>The Solution: Alignment Mechanism (Decoder Focus):</strong> Instead of one <code class="language-plaintext highlighter-rouge">c</code>, the proposed model computes a <em>distinct</em> context vector <code class="language-plaintext highlighter-rouge">c_i</code> for each target word <code class="language-plaintext highlighter-rouge">y_i</code>. This <code class="language-plaintext highlighter-rouge">c_i</code> is a weighted sum of <em>annotations</em> <code class="language-plaintext highlighter-rouge">(h_1, ..., h_{T_x})</code> from the encoder. Each <code class="language-plaintext highlighter-rouge">h_j</code> corresponds to a source word <code class="language-plaintext highlighter-rouge">x_j</code> (or rather, the hidden state around it).</li> <li> <strong>How it Works: Alignment Model &amp; Context Vector:</strong> <ul> <li>The weight <code class="language-plaintext highlighter-rouge">a_{ij}</code> for each annotation <code class="language-plaintext highlighter-rouge">h_j</code> when generating <code class="language-plaintext highlighter-rouge">y_i</code> depends on how well the input around position <code class="language-plaintext highlighter-rouge">j</code> aligns with the output at position <code class="language-plaintext highlighter-rouge">i</code>.</li> <li>These weights are calculated using an “alignment model” <code class="language-plaintext highlighter-rouge">a</code>, which takes the previous decoder hidden state <code class="language-plaintext highlighter-rouge">s_{i-1}</code> and the encoder annotation <code class="language-plaintext highlighter-rouge">h_j</code> as input to produce a score <code class="language-plaintext highlighter-rouge">e_{ij}</code>.</li> <li><code class="language-plaintext highlighter-rouge">e_{ij} = a(s_{i-1}, h_j)</code></li> <li>The weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> are obtained by normalizing these scores with a softmax: <code class="language-plaintext highlighter-rouge">a_{ij} = exp(e_{ij}) / Σ_k exp(e_{ik})</code>.</li> <li>The context vector <code class="language-plaintext highlighter-rouge">c_i</code> is then the weighted sum: <code class="language-plaintext highlighter-rouge">c_i = Σ_j a_{ij} h_j</code>.</li> <li>Crucially, the alignment model <code class="language-plaintext highlighter-rouge">a</code> (parameterized as a small feedforward network) is trained <em>jointly</em> with the rest of the system.</li> </ul> </li> <li> <strong>Soft vs. Hard Alignment:</strong> The paper uses the term “soft alignment.” This contrasts with “hard alignment,” which would involve making a deterministic choice of which single source word aligns with the target word. Soft alignment uses a weighted average over <em>all</em> source annotations. This makes the mechanism differentiable and allows the model to learn alignments implicitly through backpropagation. It also naturally handles situations where a target word might depend on multiple source words, or vice-versa.</li> <li> <strong>The Encoder: Bidirectional RNN (BiRNN):</strong> To ensure the annotation <code class="language-plaintext highlighter-rouge">h_j</code> captures context from both before and after the source word <code class="language-plaintext highlighter-rouge">x_j</code>, they used a BiRNN. This consists of a forward RNN processing the sequence from <code class="language-plaintext highlighter-rouge">x_1</code> to <code class="language-plaintext highlighter-rouge">x_{T_x}</code> and a backward RNN processing it from <code class="language-plaintext highlighter-rouge">x_{T_x}</code> to <code class="language-plaintext highlighter-rouge">x_1</code>. The annotation <code class="language-plaintext highlighter-rouge">h_j</code> is the concatenation of the forward hidden state <code class="language-plaintext highlighter-rouge">\vec{h}_j</code> and the backward hidden state <code class="language-plaintext highlighter-rouge">\cev{h}_j</code>. While BiRNNs weren’t new, their use here makes sense for creating richer annotations.</li> </ul> <h3 id="key-takeaways">Key Takeaways</h3> <p>Reflecting on the paper, several points stand out:</p> <ul> <li> <strong>Performance Improvement (Especially on Long Sentences):</strong> The results (Figure 2, Table 1) clearly show the benefit. The standard RNNencdec model’s performance drops sharply with sentence length, while the proposed RNNsearch model remains much more robust. The BLEU scores confirm a significant improvement, bringing NMT closer to traditional phrase-based systems of the time.</li> <li> <strong>Interpretability via Alignment:</strong> The alignment weights <code class="language-plaintext highlighter-rouge">a_{ij}</code> can be visualized (Figure 3). This provides insight into what parts of the source sentence the model focuses on when generating a specific target word. The visualizations showed mostly monotonic alignments (as expected between English and French) but also the ability to handle local reordering (like adjective-noun flips) correctly. This interpretability is a nice side effect compared to trying to understand a monolithic RNN.</li> <li> <strong>Handling Reordering and Length Differences:</strong> The soft alignment naturally deals with source and target phrases having different lengths or requiring non-trivial mappings, without needing explicit mechanisms like NULL tokens used in traditional SMT.</li> <li> <strong>Evolutionary Link to Transformers:</strong> Reading this <em>after</em> knowing about Transformers makes the connection clear. The core mechanism, scoring source annotations based on the current decoder state, using softmax for weights, and computing a weighted sum, is essentially the attention mechanism. It reads as a precursor; the Transformer built upon this by removing recurrence and adding multi-head attention, positional encodings, etc. It’s like seeing an earlier stage in the “evolution” of sequence models.</li> </ul> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>This paper reads as a pivotal step in NMT. It directly addressed a clear limitation (the fixed-length vector bottleneck) with a straightforward solution: allowing the model to learn where to focus in the source sequence. The “soft alignment” mechanism introduced is, in essence, the attention mechanism that became central to later architectures like the Transformer.</p> <p>Looking back now, the ideas seem intuitive, but implementing this effectively and showing its benefits in 2014/2015 was a contribution. It’s a clear paper that explains the problem, the proposed solution, and provides evidence. Reading it helps appreciate the progression of ideas leading to the models we use today.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>