<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mamba - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Reviewing the Mamba paper, replacing attention with linear-time sequence modeling"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Mamba/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mamba - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> February 10, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>This time I’m reviewing <a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">“Mamba: Linear-Time Sequence Modeling with Selective State Spaces”</a>, a paper aiming to replace attention in Transformers with <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">state space models (SSMs)</a> which I covered <a href="/blog/2025/S4">here</a> that scale linearly with sequence length. The main idea is to introduce “selectivity” into state-space models, enabling them to dynamically focus on or ignore parts of the input, which helps address limitations in traditional Transformer attention models. While interesting, I found this paper particularly challenging, both intuitively and conceptually, due to its complexity and depth of prior research.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Selective State Space Models (SSMs)</strong><br> The core idea is to use SSMs, traditionally linear and time-invariant (LTI), but introduce selectivity so the parameters can dynamically adapt based on input content. This modification breaks the time-invariance and allows the model to selectively remember or ignore information depending on the input. It’s somewhat similar to gating in LSTMs, but more general.</p> <p><strong>Time-Invariance and Its Breakage</strong><br> Typical SSMs, like the previous S4 model, are time-invariant, meaning model parameters don’t change with the input or sequence position. Mamba intentionally breaks this constraint, allowing the state update parameters (like the matrix $\Delta$) to be dynamically computed based on the current input. This lets the model “select” what to remember or forget based on the input content.</p> <p><strong>Parallelism (Training vs. Inference)</strong><br> During training, Mamba operates in a parallel (convolution-like) mode for efficiency. During inference, it runs in a sequential (recurrent) mode, calculating one step at a time. This hybrid approach gives both efficiency (during training) and flexibility (during inference).</p> <p><strong>Dimensions (D vs. N confusion)</strong><br> In Mamba, each input channel or embedding dimension (denoted as D) has its own independent state-space model. Within these channels, there’s a latent dimension N that represents the internal hidden state. This separation was tricky to grasp. In simpler terms, each embedding dimension independently runs its own selective SSM with a small latent state N; these dimensions don’t directly interact during the Mamba step.</p> <p><strong>Broadcasting and Selective Updates ($\Delta$ parameter)</strong><br> A key detail is that the selectivity parameter $\Delta$ is computed from the input and broadcasted across dimensions. $\Delta$ essentially decides how strongly to integrate the current input into the hidden state. A larger $\Delta$ resets the hidden state to pay attention to the current input, while a smaller $\Delta$ lets the hidden state carry more historical information.</p> <p><strong>Connection to Gated Mechanisms (LSTMs)</strong><br> I realized that Mamba closely resembles gated RNNs like LSTMs. Indeed, the authors explicitly mention that when simplified, selective SSMs reduce to an LSTM-like gate mechanism. Mamba can be thought of as a refined and generalized form of an LSTM, just scaled up and implemented more efficiently.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Selective Attention</strong><br> Mamba is an advanced form of RNN or LSTM without using explicit attention. By dynamically adjusting how strongly it integrates each input, it selectively “attends” to important tokens. This felt like a neat solution that addresses the drawbacks of simple recurrent models (which can’t selectively filter out irrelevant context) and convolutions (which see everything globally but without context-specific adjustments).</p> <p><strong>Linear-time Sequence Modeling</strong><br> Transformers scale quadratically with sequence length, limiting practicality for very long sequences. Mamba achieves linear-time scaling because the state updates happen independently per channel, avoiding attention’s quadratic complexity. This means it can scale efficiently to very long sequences, like millions of tokens.</p> <p><strong>Compression vs. Retrieval Trade-off</strong><br> An important trade-off is what Mamba makes compared to attention models. Attention can retrieve information from any position losslessly because it explicitly connects tokens. Mamba, however, compresses all context into a hidden state vector. This is memory-efficient but lossy. If crucial information from the distant past isn’t preserved carefully, the model might lose it permanently. On the other hand, this compression is precisely what makes Mamba efficient.</p> <p>For tasks like “needle in a haystack”—finding rare but critical information—this should be a disadvantage, but Mamba performs well. My guess is the model learned effective strategies for compressing and selectively preserving crucial information.</p> <p><strong>Complexity and Interpretability Issues</strong><br> Mamba is a theoretically elegant but practically complex model. Its architecture is dense, with a lot of prior literature, especially around the S4 and S6 architectures from Albert Gu and others. Understanding Mamba deeply requires solid familiarity with foundational SSM concepts, which can be daunting without prior study. This complexity might affect its adoption, even if performance is strong.</p> <p><strong>Hardware-Aware Optimization</strong><br> Another aspect that stood out was Mamba’s hardware optimization. They discuss parallel scans, kernel fusion, and reducing computational overhead by taking advantage of structured matrices. These optimizations are critical because Mamba inherently loses some parallel efficiency due to its recurrence. However, these steps mitigate performance drawbacks.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Mamba tries to replace attention-based Transformers with a linear-time, recurrent, state-space-based approach enhanced by dynamic selectivity. By breaking the traditional LTI assumption, it gains the flexibility to adapt its hidden states selectively, efficiently modeling long sequences. However, this brings trade-offs in interpretability, retrieval capacity, and complexity.</p> <p>Ultimately, Mamba offers a practical alternative to Transformers for handling very long sequences. I see it less as a universal replacement and more as a specialized architecture optimized for scenarios where efficient, long-range modeling with moderate compressive trade-offs makes sense.</p> <p>It’s interesting, but I think its complexity and rigidity might limit broader adoption compared to simpler architectures like Transformers or even more straightforward RNN variants. The idea itself is clever and worth exploring further, but for now it feels more niche than general-purpose.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>