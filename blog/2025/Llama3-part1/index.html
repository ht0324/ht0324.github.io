<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Llama 3 Paper - Review (Part 1) | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="My analysis and thoughts on Llama 3"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Llama3-part1/"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Llama 3 Paper - Review (Part 1)</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> March 18, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>Today I’m reviewing Meta’s Llama 3 technical paper. Due to the length and depth of the paper, I’ll be splitting this review into two parts; this is part 1, focusing on the pre-training and infrastructure aspects. The Llama 3 family represents a significant step up from Llama 2, with the flagship 405B parameter model performing competitively against leading models like GPT-4. What makes this paper interesting is its comprehensive description of Meta’s approach to scaling, including data preparation, model training, and infrastructure challenges.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Scaling Up Pre-training</strong><br> Llama 3 scales substantially beyond previous versions, with the flagship model using 405B parameters (compared to Llama 2’s smaller size) and trained on approximately 15T tokens, versus 1.8T for Llama 2. The compute used for training the flagship model was nearly 50× more than Llama 2’s largest model. Meta made this scaling work with a standard dense Transformer rather than a Mixture-of-Experts (MoE) architecture.</p> <p><strong>Data Quality and Processing</strong><br> Meta placed strong emphasis on data quality rather than just quantity. Their processing pipeline involved removing PII (personally identifiable information), applying multiple levels of deduplication (URL-level, document-level, and line-level), and using both heuristic and model-based filtering. They also used classifiers to identify and upsample high-quality code and reasoning content. For multilingual support, they implemented language-specific processing and quality filtering.</p> <p><strong>Context Length Scaling</strong><br> Llama 3 was designed to handle context windows up to 128K tokens. Rather than training on long sequences from the beginning (which would be computationally prohibitive due to the quadratic scaling of self-attention), they used a multi-phase approach: first training on 8K contexts, then gradually increasing to 128K tokens in the final stages of pre-training over approximately 800B tokens.</p> <p><strong>Hardware and Infrastructure</strong><br> Training at this scale required large hardware resources and sophisticated infrastructure. The 405B model was trained on up to 16K H100 GPUs. They used a combination of tensor parallelism, pipeline parallelism, context parallelism, and data parallelism (what they call “4D parallelism”) to distribute computation. They report achieving 38-43% Model FLOPs Utilization (MFU) at this scale.</p> <p><strong>Post-Training Alignment</strong><br> After pre-training, the models underwent extensive post-training alignment using a combination of supervised fine-tuning (SFT), rejection sampling, and Direct Preference Optimization (DPO). One interesting note is their deliberate choice to avoid more complex reinforcement learning algorithms like PPO, which they found less stable and harder to scale.</p> <p><strong>Specialized Capabilities</strong><br> The paper details multiple specialized capabilities added during post-training, including code generation (with execution-based feedback), multilingual performance, reasoning, tool-use, and factuality. For many of these, they developed specialized data generation pipelines, often leveraging earlier iterations of Llama 3 itself to generate training data.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>The Value of Simple, Stable Architectures</strong><br> One of the most interesting choices Meta made was sticking with a dense Transformer architecture rather than using a Mixture-of-Experts approach. They explicitly state this was to “maximize training stability,” which signals that at huge scale, reliability and predictability might be more valuable than theoretical efficiency. This matches what DeepSeek researchers have also mentioned about the challenges of scaling MoE models.</p> <p><strong>Data Quality Trumps Architecture Complexity</strong><br> The paper dedicates substantial space to discussing data curation, which suggests that data quality remains one of the most crucial factors for performance. Even with massive compute resources, Meta still invested heavily in filtering, curation, and quality assessment. Their use of multiple levels of deduplication, model-based quality filtering, and domain-specific pipelines reinforces how important data quality is to the final result.</p> <p><strong>Model-Bootstrapped Data Creation</strong><br> The way Meta used earlier versions of Llama 3 to generate data for subsequent training iterations is notable. For specialized capabilities like code generation, they used a bootstrapping approach where the model itself generated samples, which were then filtered based on execution results. This self-improvement cycle, where models help train their successors, is becoming more common and is notable at this scale.</p> <p><strong>Context Parallelism for Long Sequences</strong><br> The paper’s description of context parallelism (CP) for handling long sequences was useful. By dividing input sequences into chunks distributed across GPUs and using all-gather operations to collect key-value tensors, they managed to train on 128K context lengths without excessive memory usage. This approach differs from previous techniques I’ve seen and shows how specialized infrastructure is becoming for LLM training.</p> <p><strong>Alignment Complexity and Iteration</strong><br> The post-training sections reveal how labor-intensive alignment still is. They performed six rounds of alignment, iteratively collecting human preferences, generating synthetic data, and fine-tuning. Each round built on the previous one, using increasingly capable models. The process involves carefully balancing data quality, variety, and complexity, and required human annotation and quality control throughout.</p> <p><strong>Infrastructure Challenges at Scale</strong><br> The sections on reliability and operational challenges highlight how difficult training at this scale remains. During a 54-day period, they experienced 419 unexpected interruptions, with GPU issues accounting for nearly 60% of these. They also observed diurnal throughput variations of 1-2% due to environmental temperature fluctuations affecting GPU clocks. These details provide a perspective on the practical challenges of pushing the boundaries of scale.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>The Llama 3 paper provides valuable insights into how Meta approached training a competitive frontier model. While OpenAI and Anthropic maintain some lead with their proprietary models, Llama 3 demonstrates that with sufficient scale and careful engineering, it’s possible to build models that approach their capabilities while still releasing weights publicly.</p> <p>What’s notable is the maturity of Meta’s approach. They’ve drawn lessons from previous iterations and focused on reliability, scalability, and maintainability rather than pursuing more exotic architectures. Their decision to use a dense Transformer architecture, combined with a stable and relatively simple alignment procedure, shows a preference for approaches that can be reliably scaled up.</p> <p>The level of infrastructure and pipeline engineering described in the paper stands out. From their custom HTML parser to their extensive parallelism strategies to their reliability engineering, the paper makes clear that training models at this scale requires investment in raw compute and in the systems that make that compute usable.</p> <p>Overall, this paper offers a detailed look into what it takes to build competitive models at scale. While the specific approaches may evolve, the general principles: quality data, reliable infrastructure, and careful alignment, are likely to remain important for future models as well.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>