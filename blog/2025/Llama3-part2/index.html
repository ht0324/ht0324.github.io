<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Llama 3 Paper - Review (Part 2) | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Continuing my analysis of Llama 3"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Llama3-part2/"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Llama 3 Paper - Review (Part 2)</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> March 25, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>This is part 2 of my review of Meta’s Llama 3 technical paper. In <a href="/blog/2025/Llama3-part1">part 1</a>, I covered the core language model architecture, training methodology, and overall performance. Now I’ll dive into the multimodal aspects of the model (vision, video, and speech), which represent significant additions to the Llama ecosystem.</p> <p>What strikes me about Meta’s approach is their consistent focus on compositionality. Instead of training entirely new models from scratch, they extend the existing Llama 3 language models with specialized adapters. This pragmatic approach allows them to add new capabilities while preserving the existing text understanding abilities.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Compositional Multimodal Architecture</strong><br> Meta adopts a modular, compositional approach for all multimodal capabilities in Llama 3. Rather than training joint models from scratch, they combine pre-trained language models with modality-specific encoders connected through adapter layers. This architecture has several advantages: it enables parallel development of language and vision/audio capabilities, avoids the complexities of joint training on multiple modalities, preserves text-only performance, and reduces computational overhead during inference by processing input modalities efficiently.</p> <p><strong>Vision Encoder and Adapter</strong><br> The vision module consists of a pre-trained ViT-H/14 image encoder (modified to include 850M parameters) combined with cross-attention layers that connect the visual representations to the language model. These cross-attention layers are substantial, adding about 100B parameters to the 405B model. To preserve fine-grained visual information, they extract features from multiple intermediate layers of the vision encoder rather than just using the final layer output, which helps with tasks requiring detailed localization.</p> <p><strong>Video Recognition Architecture</strong><br> The video module builds on the image module by adding two key components: a “temporal aggregator” that merges frames to capture temporal relationships, and dedicated video cross-attention layers. The aggregator uses a perceiver resampler architecture to compress multiple frames into a more compact representation. During pre-training, they start with 16 frames (aggregated to 1) and scale up to 64 frames during fine-tuning to handle longer videos more effectively.</p> <p><strong>Speech Understanding Approach</strong><br> Unlike the vision module, the speech component doesn’t use cross-attention layers. Instead, it generates embeddings that directly integrate with text tokens in the language model. The speech module consists of a 1B-parameter Conformer encoder followed by a smaller adapter. This direct integration approach allows the speech interface to leverage the language model’s existing capabilities without modifying its parameters, which works well at larger scales.</p> <p><strong>Speech Generation</strong><br> For text-to-speech capabilities, Meta takes a different approach. Rather than fine-tuning the language model for speech generation, they implement a streaming text-to-speech system that uses Llama 3 embeddings to enhance text normalization and prosody modeling. The embeddings from Llama 3 are used to improve context-aware text normalization and more natural-sounding prosody.</p> <p><strong>Scaling and Training Challenges</strong><br> Training these multimodal adapters introduces unique challenges beyond those faced when training the core language model. The model computation becomes heterogeneous (some tokens require more processing than others), data shows high variance in token counts across modalities, and there are numerical instability issues from combining different types of representations. Meta addresses these through pipeline design, sequence parallelization, and using higher precision for gradient accumulation.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>Compositional Design Makes Technical Sense</strong><br> Initially, I had concerns about the compositional approach. I wondered if mapping the higher-dimensional image modality into the latent space of a language model might cause significant information loss. While the dimensionality itself is an implementation detail, I believe the inherent modality of text fundamentally contains less information than images. However, when I saw GPT-4o generating accurate images from text prompts and handling complex visual tasks, it became clear that language model latent spaces are robust at encoding visual concepts. This suggests the limitation I was worried about may not be as severe in practice. The cross-attention mechanism with multi-layer feature extraction appears to be effective at preserving the detailed information from higher-dimensional modalities.</p> <p><strong>Multi-layer Feature Extraction Preserves Fine-grained Information</strong><br> One insight I found particularly interesting was how they addressed the problem of CLIP-like models failing to preserve fine-grained localization information. Instead of relying solely on the final layer output, they extract features from multiple intermediate layers of the vision encoder (specifically the 4th, 8th, 16th, 24th, and 31st layers). This approach makes sense because the lower layers retain more spatial and detailed information before it gets abstracted away in higher layers. I hadn’t previously considered this limitation of contrastive learning approaches, but it explains why models like CLIP might struggle with tasks requiring precise visual details or localization.</p> <p><strong>Handling Many-shot Jailbreaking in Long Context Models</strong><br> Something that caught my attention was the vulnerability of long-context models to many-shot jailbreaking attacks. The longer context window enables a new attack vector: they mentioned specifically that 256-shot attacks become possible. They mitigated this by fine-tuning models on datasets that include examples of safe behavior even when unsafe behavior appears in context. The fact that they could achieve this without impacting false refusal rates or helpfulness metrics shows the model’s ability to distinguish between demonstrations in context and actual instructions.</p> <p><strong>Safety Becomes Increasingly Granular</strong><br> What struck me about Meta’s safety approach is how granular it’s becoming. Rather than having a generic “harmful content” classifier, they’re developing increasingly specialized safety mechanisms for specific types of risks. This categorization of safety concerns (into areas like cybersecurity, coding, spear-phishing, etc.) reveals how the frontier of AI safety is evolving from broad mitigation to very specific risk assessment and targeted interventions. It also suggests that as models get more capable, the attack vectors multiply, requiring more complex safety strategies.</p> <p><strong>Contextual Safety Challenges</strong><br> The paper mentions that 256-shot attacks become possible with longer context windows. It shows that extending a model’s capabilities (like context length) can introduce new safety vulnerabilities that weren’t relevant before. This suggests a kind of “safety debt” that comes with each capability enhancement: each new ability potentially opens up novel attack vectors that need additional mitigations.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>Meta’s approach to expanding Llama 3’s capabilities into multimodal territory shows a thoughtful balance between pragmatism and innovation. Rather than developing entirely new architectures or joint pre-training approaches, they extend the existing language model through specialized adapters and compositional design. This strategy allows them to leverage the strengths of existing pre-trained components while adding new capabilities incrementally.</p> <p>The paper also highlights the ongoing tension between capability enhancement and safety. As models gain new abilities (like longer context), new vulnerabilities emerge that require additional mitigations. This suggests that safety work isn’t a one-time effort but an ongoing process that must evolve alongside model capabilities.</p> <p>As these models continue to develop, I’m particularly interested in seeing how the compositional approach scales to even more modalities and how it affects overall model capabilities. Does adding more modalities lead to emergent abilities through cross-modal transfer? Do certain modalities complement each other in unexpected ways? The Llama 3 paper doesn’t directly address these questions, but it provides a solid foundation for exploring them in future work.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>