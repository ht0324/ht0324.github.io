# Are You Smarter Than an LLM?
_Published: 2025-02-12_
_Tags: AI_
_Categories: Blog_
_Original: https://ht0324.github.io/blog/2025/Smarter-than-LLM/_

<p>I recently came across an interesting and humbling interactive blog post titled <a href="https://d.erenrich.net/are-you-smarter-than-an-llm/index.html">“Are you smarter than an LLM?”</a>. The concept is simple, but the execution and implications are striking.</p>

<p>The post directly pits the user against a large language model in answering questions from the Massive Multitask Language Understanding (MMLU) benchmark. This experience forced me to confront some preconceived notions I had about LLM capabilities and the meaning of benchmark scores.</p>

<hr />

<h3 id="the-mmlu-challenge">The MMLU Challenge</h3>

<p>The MMLU benchmark is a well-known evaluation tool for LLMs, designed to measure a model’s ability to understand and reason across a wide range of subjects. It’s become somewhat saturated at the top, with leading models achieving very high scores. I was aware of this saturation, but I hadn’t truly internalized what it meant until I tried the questions myself.</p>

<p>The interactive blog post presents MMLU questions, and you answer them alongside the LLM. As you progress, you see both your answers and the LLM’s, along with the correct answers. This direct comparison is where the real learning begins.</p>

<p>The MMLU questions are non-trivial. They require broad and deep knowledge. Acing these tests, as large language models often do, shouldn’t be taken for granted. I, for one, certainly took it for granted before this experience. When I took this test alongside large language models, the experience of me getting a question wrong while the language model got it right was demoralizing.</p>

<h3 id="a-humbling-check">A Humbling Check</h3>

<p>The blog post’s title is provocative for a reason. In the context of the MMLU, I had to admit that, in many cases, I wasn’t smarter than the LLM. My score was lower. This isn’t a statement about overall intelligence, of course, but it is a sign of the strong capabilities LLMs are developing in specific domains.</p>

<p>I think many people, even those who follow AI advancements, still hold onto a sense of human exceptionalism. We see LLMs achieving high benchmark scores, but we might subconsciously maintain a belief that we’re “still special” in some way. This interactive test directly challenges that notion. It forces you to confront the reality that, in certain areas, LLMs are already surpassing human performance.</p>

<p>While MMLU is currently a very saturated benchmark, it’s important to remember that there are many other, even more challenging evaluations out there. For example, <a href="https://epoch.ai/frontiermath">FrontierMath</a> benchmark features problems so difficult that they require several hours or days of work by expert mathematicians. Even Terence Tao participated in its creation. These are exceptionally hard questions, and I doubt I could answer even one correctly.</p>

<p>Yet, even on these very demanding benchmarks, LLMs are starting to achieve non-zero scores, though at low percentages. And I see no reason why they won’t continue to improve.</p>

<hr />

<h3 id="where-do-we-go-from-here">Where Do We Go From Here?</h3>

<p>This experience has made me reconsider the trajectory of LLM development. If current models like GPT-4.5 (or whatever the current leading model is) can outperform the average person (and even me) on a challenging benchmark like MMLU, what will the landscape look like in five years?</p>

<p>The pace of progress is astonishing, and this interactive test provides a tangible glimpse into that future. It’s a future where LLMs will likely possess knowledge and reasoning abilities that surpass those of many humans in specific, measurable ways. It’s a future we need to understand and prepare for, and it is already here.</p>
