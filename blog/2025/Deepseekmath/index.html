<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DeepSeekMath - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Reviewing DeepSeekMath paper"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/Deepseekmath/"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">DeepSeekMath - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> January 27, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post, I’m reviewing <a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”</a>, which introduces DeepSeekMath 7B, a math-specialized language model.</p> <p>What stands out about this paper isn’t just the performance of DeepSeekMath but the philosophy behind its design. DeepSeek consistently focuses on first-principles thinking—stripping down unnecessary complexity, focusing on what’s truly important, and optimizing aggressively for efficiency. Given that DeepSeek operates under serious GPU constraints due to US export regulations, this emphasis on efficiency isn’t just a preference but a necessity, shaping their entire research trajectory.</p> <p>The key technical contribution of this paper is <strong>Group Relative Policy Optimization (GRPO)</strong>—a modification of Proximal Policy Optimization (PPO) that eliminates the need for a separate value model, making reinforcement learning (RL) significantly more efficient.</p> <hr> <h3 id="key-concepts">Key Concepts</h3> <p><strong>Mathematical Reasoning in LLMs</strong><br> Mathematical reasoning is tough for language models. Unlike general NLP tasks, where fluency and coherence are enough, math requires structured problem-solving. DeepSeekMath is specifically trained with 120B math-related tokens and fine-tuned with reinforcement learning to improve its reasoning ability.</p> <p><strong>Reinforcement Learning (RL) for Mathematical Reasoning</strong><br> DeepSeekMath uses RL to refine its outputs by training with a reward model. Instead of just relying on standard instruction tuning, RL helps align model responses towards more precise and logically structured solutions.</p> <p><strong>Group Relative Policy Optimization (GRPO)</strong><br> GRPO is the highlight of the paper. It’s a <strong>compute-efficient</strong> alternative to standard PPO. Traditional PPO requires a separate value model to estimate advantage, which is computationally expensive. Instead, GRPO ranks multiple sampled responses and calculates rewards based on their relative ranking, removing the need for an explicit value function. This aligns well with the original spirit of PPO while making it much more efficient.</p> <p><strong>Why Efficiency Matters Here</strong><br> DeepSeek, as a company, has repeatedly <a href="https://www.thefai.org/posts/deepseek-s-success-reinforces-the-case-for-export-controls" rel="external nofollow noopener" target="_blank">emphasized</a> that their biggest bottleneck is compute, not talent. This constraint forces them to innovate in ways that maximize training efficiency. By making reinforcement learning more efficient, GRPO allows them to run <strong>more iterations</strong> of training within the same compute budget. Given that experience (the number of training iterations) is a critical factor in model performance, this approach compounds over time.</p> <hr> <h3 id="key-takeaways-what-i-learned">Key Takeaways (What I Learned)</h3> <p><strong>First-Principles Thinking in Model Design</strong><br> One of the biggest takeaways from this paper is how DeepSeek consistently strips things down to what actually matters. The removal of the value model in PPO is a prime example of this. Instead of treating it as an unavoidable cost, they found a way to compute advantage <strong>without it</strong>, saving compute while still aligning with the core principle of PPO.</p> <p><strong>Reinforcement Learning Efficiency Can Directly Impact Model Performance</strong><br> A lot of research in RL for LLMs focuses on making models “better,” but this paper indirectly makes another point—<strong>efficiency itself is a performance factor</strong>. If you can run 2x or 3x more RL iterations for the same compute budget, you might end up with a better-trained model even if the method itself isn’t inherently more powerful. This is a perspective that isn’t always emphasized but makes a lot of sense in practice.</p> <p><strong>GRPO is an Elegant Fix to a Known Problem</strong><br> PPO’s requirement for a separate value model has long been a pain point due to its added complexity and compute cost. GRPO solves this by leveraging relative ranking instead of absolute value estimation. What I like about this approach is that it’s not just a random heuristic—it actually aligns <strong>better</strong> with the fundamental idea of PPO while also making it more practical. This kind of solution—where something becomes both <em>simpler</em> and <em>better</em>—is rare and always worth paying attention to.</p> <p><strong>DeepSeek’s Strategy of Leaning into Efficiency is Paying Off</strong><br> The efficiency-first approach DeepSeek is taking is starting to show real results. It’s easy to get caught up in scaling laws and assume more compute is always better, but DeepSeek is proving that <strong>how</strong> you use that compute is just as important. By optimizing RL efficiency, they can squeeze more training out of their limited resources, which in turn compounds into better models. This approach is clearly working, as seen later in DeepSeek R1, which made a significant impact.</p> <hr> <h3 id="summary--final-thoughts">Summary &amp; Final Thoughts</h3> <p>DeepSeekMath is a great example of how efficiency-driven research can lead to meaningful improvements. GRPO streamlines PPO by removing the need for a value model, reducing computational cost while staying true to the original PPO formulation. This enables DeepSeek to run more RL iterations, which in turn leads to better-trained models.</p> <p>This paper reinforces the idea that <strong>efficiency isn’t just a secondary concern—it’s a core factor that directly influences model performance</strong>. It also shows how taking a first-principles approach to ML research can lead to both simpler and more effective solutions. While DeepSeekMath itself is a step forward in mathematical reasoning, I think the bigger impact of this work is in how it shapes reinforcement learning efficiency going forward.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>