<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Knowledge Distillation - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Reviewing the elegant 2015 paper by Hinton, Vinyals, and Dean on knowledge distillation."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/distillation/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Knowledge Distillation - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> April 22, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>I’ve known about the concept of knowledge distillation for a while – the core idea is simple: soft labels (the full probability distribution from a model) contain richer information about class relationships than hard labels alone. I first encountered it in a lecture by Geoffrey Hinton (<a href="https://www.youtube.com/watch?v=rGgGOccMEiY" rel="external nofollow noopener" target="_blank">like this one discussing paths to intelligence</a>) and decided to read the original 2015 paper, “<a href="https://arxiv.org/abs/1503.02531" rel="external nofollow noopener" target="_blank">Distilling the Knowledge in a Neural Network</a>,” co-authored with Oriol Vinyals and Jeff Dean. It’s short, but with clear insight.</p> <h3 id="the-insect-analogy-training-vs-deployment">The Insect Analogy: Training vs. Deployment</h3> <p>What struck me immediately was the opening analogy:</p> <blockquote> <p>“Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction.”</p> </blockquote> <p>I haven’t seen many ML papers start with a biological analogy like this. I hadn’t thought about insect life stages this way before. The larva is about consumption and growth, slow-moving, maybe not complex, but efficient at extracting resources (like a large training model absorbing information from data). The adult form is optimized for different tasks, lightweight, fast, mobile, focused on specific functions like reproduction (like an efficient deployment model needing low latency and computational cost).</p> <p>The analogy fits perfectly with the challenge in machine learning:</p> <ul> <li> <strong>Training:</strong> We often use huge, “cumbersome” models (or ensembles) that take lots of computation and time but are great at extracting every bit of signal from large datasets.</li> <li> <strong>Deployment:</strong> We need models that are fast, efficient, and have low latency for real-world use.</li> </ul> <p>Distillation, then, is like the <strong>metamorphosis</strong>: transforming the knowledge captured by the cumbersome larva/training model into the efficient adult/deployment model.</p> <h3 id="knowledge-beyond-weights">Knowledge Beyond Weights</h3> <p>The paper points out a potential “conceptual block”:</p> <blockquote> <p>“…we tend to identify the knowledge in a trained model with the learned parameter values.”</p> </blockquote> <p>This makes it hard to think about transferring knowledge without just copying weights. Prior work like Rich Caruana’s model compression focused on matching the outputs <em>before</em> the final softmax (the logits). Hinton et al.’s approach refines this by using the <em>probabilities</em> from the softmax, arguing that this captures the learned distribution more meaningfully.</p> <h3 id="the-value-of-wrong-answers">The Value of “Wrong” Answers</h3> <p>A key insight is how the large, cumbersome model generalizes. It’s not just about getting the right answer.</p> <blockquote> <p>“…a side-effect of the learning is that the trained model assigns probabilities to all of the incorrect answers… The relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize.”</p> </blockquote> <p>The example they give is clear: an image of a BMW might have a tiny probability of being mistaken for a garbage truck, but that probability, however small, is likely higher than it being mistaken for a carrot. This network of similarities and differences between classes is knowledge learned by the teacher model. Hard labels (just “BMW”) throw this information away. Soft labels (the full probability distribution) preserve it.</p> <p>This aligns with the objective: we don’t just want models to perform well on training data, we want them to <em>generalize</em> well to new data. Soft targets directly transfer the <em>generalization behavior</em> of the teacher model to the student.</p> <h3 id="the-mechanism-temperature-scaling">The Mechanism: Temperature Scaling</h3> <p>So how do we use these soft labels? If the teacher model is very confident (assigns probability ~1.0 to the correct class), the probabilities for incorrect classes are tiny. Even if their <em>ratios</em> contain information, they have almost no impact on the cross-entropy loss during student training.</p> <p>The solution is to “raise the temperature” <code class="language-plaintext highlighter-rouge">T</code> of the softmax function:</p> <p><code class="language-plaintext highlighter-rouge">q_i = exp(z_i / T) / Σ_j exp(z_j / T)</code></p> <p>where <code class="language-plaintext highlighter-rouge">z_i</code> are the logits. Normally <code class="language-plaintext highlighter-rouge">T=1</code>. Using a higher <code class="language-plaintext highlighter-rouge">T &gt; 1</code> “softens” the probability distribution, increasing the probabilities of incorrect classes and allowing them to contribute more to the loss function. The student model is trained to match this softened distribution, using the same high temperature <code class="language-plaintext highlighter-rouge">T</code>. (After training, the student uses <code class="language-plaintext highlighter-rouge">T=1</code> for inference).</p> <p>This temperature scaling is the core mechanism. The paper notes that in the high-temperature limit, this method becomes equivalent to matching the logits (Caruana’s approach), but at intermediate temperatures, it focuses more on matching the more probable incorrect classes, potentially ignoring noise from very negative logits.</p> <h3 id="training-the-student">Training the Student</h3> <p>The best results often come from combining two objectives:</p> <ol> <li>Matching the soft targets from the teacher (using cross-entropy with high temperature <code class="language-plaintext highlighter-rouge">T</code>).</li> <li>Matching the true hard labels (using cross-entropy with <code class="language-plaintext highlighter-rouge">T=1</code>).</li> </ol> <p>They found a weighted average works well, often with a lower weight on the hard target loss. As they say: “Typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful.”</p> <h3 id="proof-of-generalization-the-mnist-experiment">Proof of Generalization: The MNIST Experiment</h3> <p>A clear experiment highlights the value of this approach. They trained a student model on MNIST, but <em>omitted all examples of the digit ‘3’</em> from the transfer set. From the student’s perspective, ‘3’ was a “mythical digit” it had never directly seen.</p> <p>Despite this, the distilled model performed well on classifying ‘3’s at test time (with a bias adjustment). It had learned about ‘3’ indirectly, through the soft targets for other digits, for example, by learning which ‘8’s looked a bit like a ‘3’ according to the teacher model. This is evidence that soft targets transfer generalization capabilities, not just labels.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>This paper is a classic example of clear insight. The core claim is simple:</p> <blockquote> <p>“…a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target.”</p> </blockquote> <p>Knowledge distillation provides a practical way to harness this information, bridging the gap between powerful-but-cumbersome training models and efficient deployment models. While short, the paper’s impact is significant, reflected in its citations. It’s a testament to clear thinking and finding simple solutions to important problems.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>