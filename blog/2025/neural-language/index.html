<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Neural Probabilistic Language Model - Review | Hun Tae Kim</title> <meta name="author" content="Hun Tae Kim"> <meta name="description" content="Thoughts on the foundational 2003 paper"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,600&amp;family=Source+Serif+4:ital,opsz,wght@0,8..60,400;0,8..60,500;0,8..60,600;0,8..60,700;1,8..60,400;1,8..60,600&amp;display=swap"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ht0324.github.io/blog/2025/neural-language/"> <link type="application/atom+xml" rel="alternate" href="https://ht0324.github.io/feed.xml" title="blank"> </head> <body class="fixed-top-nav " style="overflow-x: hidden; max-width: 100vw;"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/">Hun Tae Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links</a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="overflow-x: hidden;"> <div class="post"> <header class="post-header"> <h1 class="post-title">Neural Probabilistic Language Model - Review</h1> <div class="post-meta-wrapper"> <span> <i class="fas fa-calendar fa-sm"></i> April 20, 2025 </span> </div> <div class="post-tag-wrapper"> <a href="/blog/tag/ai" class="tag-pill"> <i class="fas fa-hashtag fa-sm"></i> AI</a> <a href="/blog/category/paper" class="tag-pill"> <i class="fas fa-folder fa-sm"></i> Paper</a> </div> </header> <article class="post-content"> <div id="markdown-content"> <p>I recently dove into Yoshua Bengio et al.’s 2003 paper, “<a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">A Neural Probabilistic Language Model</a>”. Reading such an old paper, foundational work from over two decades ago, is fascinating. What struck me most wasn’t just the specific model (which is simple by today’s standards), but the clarity with which Bengio laid out the core problems and principles of language modeling, principles that are still relevant. I got a respect for his vision; it feels like this paper set the trajectory for much of what followed.</p> <h3 id="the-problem-the-curse-of-dimensionality">The Problem: The Curse of Dimensionality</h3> <p>Bengio starts by framing the fundamental challenge: the <strong>curse of dimensionality</strong>. As he puts it,</p> <blockquote> <p>“…a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.”</p> </blockquote> <p>This is because the number of possible sentences is essentially infinite, like <a href="https://medium.com/@FdForThought/a-short-story-in-hell-24b02ff4d812" rel="external nofollow noopener" target="_blank">the Library of Babel</a>. Any specific sentence has almost zero probability of occurring randomly.</p> <p>The “curse” goes deeper than just the sheer number of sequences. As the number of dimensions (e.g., the length of the sequence, or the number of features considered) increases:</p> <ol> <li> <strong>Space Expands Exponentially:</strong> The volume of the space grows very fast, making the available data extremely sparse.</li> <li> <strong>Distance Intuition Breaks:</strong> In high dimensions, points tend to become equidistant from each other, and most of the volume is concentrated far from the center, near the “surface” of the high-dimensional space. Our low-dimensional intuitions about proximity and density fail.</li> <li> <strong>Spurious Correlations:</strong> With so many dimensions, it becomes easy to find apparent patterns in data that are just noise.</li> </ol> <p>This is a core challenge for many real-world problems, especially with rich sensory data spanning many dimensions. How do you find the signal in such a vast, sparse space without getting lost?</p> <h3 id="the-solution-fighting-fire-with-fire">The Solution: Fighting Fire with Fire</h3> <p>Bengio and his colleagues proposed a way to fight this curse:</p> <blockquote> <p>“…learning a distributed representation for words…”</p> </blockquote> <p>Essentially, they proposed learning dense, low-dimensional feature vectors (embeddings) for each word in the vocabulary. This is like fighting fire with fire: while the <em>vocabulary</em> space is huge and discrete, the learned <em>feature</em> space is much smaller (e.g., 30-100 dimensions in their experiments vs. 17k+ words) but continuous. Because it’s a dense, continuous space, even a relatively low-dimensional one has a large capacity to represent complex relationships. They are mapping the discrete, high-dimensional vocabulary into a structured, continuous latent space.</p> <h3 id="the-magic-how-generalization-happens">The Magic: How Generalization Happens</h3> <p>So how does this help? The paper explains:</p> <blockquote> <p>“Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.”</p> </blockquote> <p>This, for me, is the crux of it. The model learns which words play similar roles (semantically, syntactically) and places them close together in the embedding space. Because the probability function operates smoothly over this continuous space, seeing “The cat sat on the mat” helps the model assign a higher probability to the <em>unseen</em> sentence “A dog rested on the rug,” because the corresponding words have similar learned representations. It’s this mapping from discrete symbols to a meaningful continuous space that allows generalization beyond simply memorizing n-grams. This is fundamentally how current LLMs achieve their (still limited, but powerful) generalization capabilities.</p> <h3 id="learning-end-to-end">Learning End-to-End</h3> <p>A key part of their proposal was point 3:</p> <blockquote> <p>“learn simultaneously the word feature vectors and the parameters of that probability function.”</p> </blockquote> <p>They recognized that the embeddings and the prediction mechanism need to learn <em>from each other</em>. You can’t just fix one and train the other; they have to be optimized together, end-to-end, for the embeddings to become useful for the prediction task and vice-versa.</p> <h3 id="a-historical-aside-parallel-processing-with-cpus">A Historical Aside: Parallel Processing with CPUs</h3> <p>What also caught my eye was the extensive discussion on parallelizing the training process. Remember, this was 2003 when widespread GPU computing for ML wasn’t a thing yet. They detail their efforts using <strong>parameter-parallel processing</strong> across multiple CPUs (up to 64 Athlon processors in their cluster!). They discuss asynchronous updates and communication overhead (MPI). It feels like they were laying the conceptual groundwork for the kind of massive parallelization (now mostly on GPUs/TPUs) that is essential for training today’s large models.</p> <h3 id="lasting-impact">Lasting Impact</h3> <p>While the specific MLP architecture used in the paper is rudimentary now, the core ideas, tackling the curse of dimensionality with learned distributed representations, enabling generalization through semantic similarity in embedding space, and the need for end-to-end training, remain central to modern NLP and deep learning. Reading this paper felt like a clear early articulation that illuminated the path forward for the field. It helped define the paradigm we’re still working within.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ht0324/ht0324.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hun Tae Kim. All Rights Reserved. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-7ZMBS6JQKV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7ZMBS6JQKV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>